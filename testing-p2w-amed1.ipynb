{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:35:41.196244Z","iopub.execute_input":"2025-03-27T04:35:41.196682Z","iopub.status.idle":"2025-03-27T04:35:41.515218Z","shell.execute_reply.started":"2025-03-27T04:35:41.196646Z","shell.execute_reply":"2025-03-27T04:35:41.514553Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%capture\n!rm -rf /kaggle/working/miniconda\n!wget -q https://repo.anaconda.com/miniconda/Miniconda3-py39_23.5.2-0-Linux-x86_64.sh -O miniconda.sh\n!chmod +x miniconda.sh\n!bash ./miniconda.sh -b -f -p /kaggle/working/miniconda\n!rm miniconda.sh\n\n# Verify Miniconda installation (check for python binary)\n!ls -l /kaggle/working/miniconda/bin/python || echo \"Python binary missing!\"\n\n# Ensure Conda binaries are executable\n!chmod +x /kaggle/working/miniconda/bin/conda\n!chmod +x /kaggle/working/miniconda/bin/python  # Explicitly make python executable\n!chmod +x /kaggle/working/miniconda/etc/profile.d/conda.sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:35:41.516190Z","iopub.execute_input":"2025-03-27T04:35:41.516618Z","iopub.status.idle":"2025-03-27T04:35:50.482209Z","shell.execute_reply.started":"2025-03-27T04:35:41.516588Z","shell.execute_reply":"2025-03-27T04:35:50.481265Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Add Conda to PATH\nimport os\nos.environ['PATH'] = \"/kaggle/working/miniconda/bin:\" + os.environ['PATH']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:35:50.484247Z","iopub.execute_input":"2025-03-27T04:35:50.484531Z","iopub.status.idle":"2025-03-27T04:35:50.488087Z","shell.execute_reply.started":"2025-03-27T04:35:50.484493Z","shell.execute_reply":"2025-03-27T04:35:50.487301Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"%%capture\n!conda install conda=25.1.1 -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:35:50.489135Z","iopub.execute_input":"2025-03-27T04:35:50.489398Z","iopub.status.idle":"2025-03-27T04:36:44.315966Z","shell.execute_reply.started":"2025-03-27T04:35:50.489377Z","shell.execute_reply":"2025-03-27T04:36:44.314723Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"%%capture\n# Initialize Conda\n!conda init bash\n\n# Create Conda environment with Python 3.9\n!conda create -n p2w python=3.9 -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:36:44.317310Z","iopub.execute_input":"2025-03-27T04:36:44.317683Z","iopub.status.idle":"2025-03-27T04:36:53.012369Z","shell.execute_reply.started":"2025-03-27T04:36:44.317643Z","shell.execute_reply":"2025-03-27T04:36:53.011364Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!source /kaggle/working/miniconda/etc/profile.d/conda.sh && conda activate p2w","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:36:53.013253Z","iopub.execute_input":"2025-03-27T04:36:53.013534Z","iopub.status.idle":"2025-03-27T04:36:53.418625Z","shell.execute_reply.started":"2025-03-27T04:36:53.013494Z","shell.execute_reply":"2025-03-27T04:36:53.417436Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"%%capture\n!conda install -n p2w mpi4py openmpi -y\n!conda install -n p2w git -y\n!pip install gdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:36:53.419829Z","iopub.execute_input":"2025-03-27T04:36:53.420160Z","iopub.status.idle":"2025-03-27T04:37:03.042847Z","shell.execute_reply.started":"2025-03-27T04:36:53.420134Z","shell.execute_reply":"2025-03-27T04:37:03.041854Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"%%capture\n!rm -rf /kaggle/working/p2w\n# Git clone your repository\n!git clone https://github.com/jychoi118/P2-weighting.git /kaggle/working/p2w","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:37:03.045884Z","iopub.execute_input":"2025-03-27T04:37:03.046126Z","iopub.status.idle":"2025-03-27T04:37:03.888914Z","shell.execute_reply.started":"2025-03-27T04:37:03.046105Z","shell.execute_reply":"2025-03-27T04:37:03.887940Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!conda info --envs\n!python --version ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:37:03.890675Z","iopub.execute_input":"2025-03-27T04:37:03.890957Z","iopub.status.idle":"2025-03-27T04:37:04.843170Z","shell.execute_reply.started":"2025-03-27T04:37:03.890934Z","shell.execute_reply":"2025-03-27T04:37:04.841954Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/miniconda/lib/python3.9/site-packages/conda/base/context.py:202: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n\nTo remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n\n  conda config --add channels defaults\n\nFor more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n\n  deprecated.topic(\n\n# conda environments:\n#\nbase                   /kaggle/working/miniconda\np2w                    /kaggle/working/miniconda/envs/p2w\n\nPython 3.9.17\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"test_install_code = '''import subprocess\nimport sys\n\ndef install(package_args):\n    \"\"\"Run a pip install command with support for extra arguments.\"\"\"\n    # Base command\n    command = [sys.executable, \"-m\", \"pip\", \"install\"]\n    # Split package_args into a list if it contains spaces (e.g., for --extra-index-url)\n    args = package_args.split()\n    # Extend command with all arguments\n    command.extend(args)\n    try:\n        subprocess.check_call(command)\n        print(f\"Successfully installed: {' '.join(args)}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error installing {' '.join(args)}: {e}\")\n        sys.exit(1)\n# Install PyTorch and its ecosystem together to enforce consistency\n#install(\"torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\")\ninstall(\"torch==2.1.0+cu121 torchvision==0.16.0+cu121 torchaudio==2.1.0+cu121 --extra-index-url https://download.pytorch.org/whl/cu121\")\n# Verify installation\n# Install other packages (avoid upgrading torch)\npackages = [\n    \"numpy==1.24.4\",\n    \"pillow\",\n    \"lpips\",\n    \"torchmetrics==0.11.4\",  # Pin to a version compatible with PyTorch 1.13.1\n    \"click\",\n    \"scipy\",\n    \"psutil\",\n    \"requests\",\n    \"tqdm\",\n    \"blobfile\",\n    \"imageio\",\n    \"imageio-ffmpeg\",\n    \"pyspng\",\n    \"omegaconf\",\n    \"pytorch_lightning==1.9.5\",  # Pin to a version compatible with PyTorch 1.13.1\n    \"einops\",\n    \"taming-transformers\",\n    \"transformers\"\n]\n\nfor package in packages:\n    install(package)\n\n# Verify key installations\nprint(\"Verifying installations...\")\ntry:\n    import torch\n    import numpy as np\n    import torchvision\n    import torchaudio\n    import torchmetrics\n    import pytorch_lightning\n    print(f\"Python version: {sys.version}\")\n    print(f\"PyTorch version: {torch.__version__}\")\n    print(f\"CUDA available: {torch.cuda.is_available()}\")\n    print(f\"NCCL available: {torch.distributed.is_nccl_available()}\")\n    print(f\"NumPy version: {np.__version__}\")\n    print(f\"Torchvision version: {torchvision.__version__}\")\n    print(f\"Torchaudio version: {torchaudio.__version__}\")\n    print(f\"Torchmetrics version: {torchmetrics.__version__}\")\n    print(f\"Pytorch Lightning version: {pytorch_lightning.__version__}\")\nexcept ImportError as e:\n    print(f\"Verification failed: {e}\")\n    sys.exit(1)'''\n# Write the updated test_install.py to the cloned repo directory\nwith open('/kaggle/working/p2w/test_install.py', 'w') as f:\n    f.write(test_install_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:37:04.844291Z","iopub.execute_input":"2025-03-27T04:37:04.844611Z","iopub.status.idle":"2025-03-27T04:37:04.849950Z","shell.execute_reply.started":"2025-03-27T04:37:04.844577Z","shell.execute_reply":"2025-03-27T04:37:04.849034Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"%%capture\n!source /kaggle/working/miniconda/etc/profile.d/conda.sh && conda activate p2w && \\\ncd /kaggle/working/p2w && python test_install.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:37:04.850814Z","iopub.execute_input":"2025-03-27T04:37:04.851122Z","iopub.status.idle":"2025-03-27T04:39:33.486486Z","shell.execute_reply.started":"2025-03-27T04:37:04.851092Z","shell.execute_reply":"2025-03-27T04:39:33.485486Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!mkdir -p /kaggle/working/p2w/logs\n!mkdir -p /kaggle/working/p2w/checkpoints\n!mkdir -p /kaggle/working/p2w/training_amed\n!mkdir -p /kaggle/working/p2w/exps\n!mkdir -p /kaggle/working/p2w/src\n!mkdir -p /kaggle/working/p2w/torch_utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:39:33.487549Z","iopub.execute_input":"2025-03-27T04:39:33.487788Z","iopub.status.idle":"2025-03-27T04:39:34.179878Z","shell.execute_reply.started":"2025-03-27T04:39:33.487766Z","shell.execute_reply":"2025-03-27T04:39:34.178862Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!mkdir -p /kaggle/working/p2w/dnnlib\n!mkdir -p /kaggle/working/p2w/ldm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:39:34.180867Z","iopub.execute_input":"2025-03-27T04:39:34.181134Z","iopub.status.idle":"2025-03-27T04:39:34.415064Z","shell.execute_reply.started":"2025-03-27T04:39:34.181111Z","shell.execute_reply":"2025-03-27T04:39:34.414014Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"init='''\n# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\nfrom .util import EasyDict, make_cache_dir_path\n'''\nwith open('/kaggle/working/p2w/dnnlib/__init__.py', 'w') as f:\n    f.write(init)\nprint(\"Successfully wrote init.py to /kaggle/working/p2w/dnnlib/__init__.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:39:34.416093Z","iopub.execute_input":"2025-03-27T04:39:34.416350Z","iopub.status.idle":"2025-03-27T04:39:34.421658Z","shell.execute_reply.started":"2025-03-27T04:39:34.416316Z","shell.execute_reply":"2025-03-27T04:39:34.420749Z"}},"outputs":[{"name":"stdout","text":"Successfully wrote init.py to /kaggle/working/p2w/dnnlib/__init__.py\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"our ema 0.9999hceckpoints use_fp16=True","metadata":{}},{"cell_type":"code","source":"dnnlib='''\n# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Miscellaneous utility classes and functions.\"\"\"\n\nimport ctypes\nimport fnmatch\nimport importlib\nimport inspect\nimport numpy as np\nimport os\nimport shutil\nimport sys\nimport types\nimport io\nimport pickle\nimport re\nimport requests\nimport html\nimport hashlib\nimport glob\nimport tempfile\nimport urllib\nimport urllib.request\nimport uuid\n\nfrom distutils.util import strtobool\nfrom typing import Any, List, Tuple, Union, Optional\n\n\n# Util classes\n# ------------------------------------------------------------------------------------------\n\n\nclass EasyDict(dict):\n    \"\"\"Convenience class that behaves like a dict but allows access with the attribute syntax.\"\"\"\n\n    def __getattr__(self, name: str) -> Any:\n        try:\n            return self[name]\n        except KeyError:\n            raise AttributeError(name)\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        self[name] = value\n\n    def __delattr__(self, name: str) -> None:\n        del self[name]\n\n\nclass Logger(object):\n    \"\"\"Redirect stderr to stdout, optionally print stdout to a file, and optionally force flushing on both stdout and the file.\"\"\"\n\n    def __init__(self, file_name: Optional[str] = None, file_mode: str = \"w\", should_flush: bool = True):\n        self.file = None\n\n        if file_name is not None:\n            self.file = open(file_name, file_mode)\n\n        self.should_flush = should_flush\n        self.stdout = sys.stdout\n        self.stderr = sys.stderr\n\n        sys.stdout = self\n        sys.stderr = self\n\n    def __enter__(self) -> \"Logger\":\n        return self\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        self.close()\n\n    def write(self, text: Union[str, bytes]) -> None:\n        \"\"\"Write text to stdout (and a file) and optionally flush.\"\"\"\n        if isinstance(text, bytes):\n            text = text.decode()\n        if len(text) == 0: # workaround for a bug in VSCode debugger: sys.stdout.write(''); sys.stdout.flush() => crash\n            return\n\n        if self.file is not None:\n            self.file.write(text)\n\n        self.stdout.write(text)\n\n        if self.should_flush:\n            self.flush()\n\n    def flush(self) -> None:\n        \"\"\"Flush written text to both stdout and a file, if open.\"\"\"\n        if self.file is not None:\n            self.file.flush()\n\n        self.stdout.flush()\n\n    def close(self) -> None:\n        \"\"\"Flush, close possible files, and remove stdout/stderr mirroring.\"\"\"\n        self.flush()\n\n        # if using multiple loggers, prevent closing in wrong order\n        if sys.stdout is self:\n            sys.stdout = self.stdout\n        if sys.stderr is self:\n            sys.stderr = self.stderr\n\n        if self.file is not None:\n            self.file.close()\n            self.file = None\n\n\n# Cache directories\n# ------------------------------------------------------------------------------------------\n\n_dnnlib_cache_dir = None\n\ndef set_cache_dir(path: str) -> None:\n    global _dnnlib_cache_dir\n    _dnnlib_cache_dir = path\n\ndef make_cache_dir_path(*paths: str) -> str:\n    if _dnnlib_cache_dir is not None:\n        return os.path.join(_dnnlib_cache_dir, *paths)\n    if 'DNNLIB_CACHE_DIR' in os.environ:\n        return os.path.join(os.environ['DNNLIB_CACHE_DIR'], *paths)\n    if 'HOME' in os.environ:\n        return os.path.join(os.environ['HOME'], '.cache', 'dnnlib', *paths)\n    if 'USERPROFILE' in os.environ:\n        return os.path.join(os.environ['USERPROFILE'], '.cache', 'dnnlib', *paths)\n    return os.path.join(tempfile.gettempdir(), '.cache', 'dnnlib', *paths)\n\n# Small util functions\n# ------------------------------------------------------------------------------------------\n\n\ndef format_time(seconds: Union[int, float]) -> str:\n    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n    s = int(np.rint(seconds))\n\n    if s < 60:\n        return \"{0}s\".format(s)\n    elif s < 60 * 60:\n        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n    elif s < 24 * 60 * 60:\n        return \"{0}h {1:02}m {2:02}s\".format(s // (60 * 60), (s // 60) % 60, s % 60)\n    else:\n        return \"{0}d {1:02}h {2:02}m\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24, (s // 60) % 60)\n\n\ndef format_time_brief(seconds: Union[int, float]) -> str:\n    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n    s = int(np.rint(seconds))\n\n    if s < 60:\n        return \"{0}s\".format(s)\n    elif s < 60 * 60:\n        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n    elif s < 24 * 60 * 60:\n        return \"{0}h {1:02}m\".format(s // (60 * 60), (s // 60) % 60)\n    else:\n        return \"{0}d {1:02}h\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24)\n\n\ndef ask_yes_no(question: str) -> bool:\n    \"\"\"Ask the user the question until the user inputs a valid answer.\"\"\"\n    while True:\n        try:\n            print(\"{0} [y/n]\".format(question))\n            return strtobool(input().lower())\n        except ValueError:\n            pass\n\n\ndef tuple_product(t: Tuple) -> Any:\n    \"\"\"Calculate the product of the tuple elements.\"\"\"\n    result = 1\n\n    for v in t:\n        result *= v\n\n    return result\n\n\n_str_to_ctype = {\n    \"uint8\": ctypes.c_ubyte,\n    \"uint16\": ctypes.c_uint16,\n    \"uint32\": ctypes.c_uint32,\n    \"uint64\": ctypes.c_uint64,\n    \"int8\": ctypes.c_byte,\n    \"int16\": ctypes.c_int16,\n    \"int32\": ctypes.c_int32,\n    \"int64\": ctypes.c_int64,\n    \"float32\": ctypes.c_float,\n    \"float64\": ctypes.c_double\n}\n\n\ndef get_dtype_and_ctype(type_obj: Any) -> Tuple[np.dtype, Any]:\n    \"\"\"Given a type name string (or an object having a __name__ attribute), return matching Numpy and ctypes types that have the same size in bytes.\"\"\"\n    type_str = None\n\n    if isinstance(type_obj, str):\n        type_str = type_obj\n    elif hasattr(type_obj, \"__name__\"):\n        type_str = type_obj.__name__\n    elif hasattr(type_obj, \"name\"):\n        type_str = type_obj.name\n    else:\n        raise RuntimeError(\"Cannot infer type name from input\")\n\n    assert type_str in _str_to_ctype.keys()\n\n    my_dtype = np.dtype(type_str)\n    my_ctype = _str_to_ctype[type_str]\n\n    assert my_dtype.itemsize == ctypes.sizeof(my_ctype)\n\n    return my_dtype, my_ctype\n\n\ndef is_pickleable(obj: Any) -> bool:\n    try:\n        with io.BytesIO() as stream:\n            pickle.dump(obj, stream)\n        return True\n    except:\n        return False\n\n\n# Functionality to import modules/objects by name, and call functions by name\n# ------------------------------------------------------------------------------------------\n\ndef get_module_from_obj_name(obj_name: str) -> Tuple[types.ModuleType, str]:\n    \"\"\"Searches for the underlying module behind the name to some python object.\n    Returns the module and the object name (original name with module part removed).\"\"\"\n\n    # allow convenience shorthands, substitute them by full names\n    obj_name = re.sub(\"^np.\", \"numpy.\", obj_name)\n    obj_name = re.sub(\"^tf.\", \"tensorflow.\", obj_name)\n\n    # list alternatives for (module_name, local_obj_name)\n    parts = obj_name.split(\".\")\n    name_pairs = [(\".\".join(parts[:i]), \".\".join(parts[i:])) for i in range(len(parts), 0, -1)]\n\n    # try each alternative in turn\n    for module_name, local_obj_name in name_pairs:\n        try:\n            module = importlib.import_module(module_name) # may raise ImportError\n            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n            return module, local_obj_name\n        except:\n            pass\n\n    # maybe some of the modules themselves contain errors?\n    for module_name, _local_obj_name in name_pairs:\n        try:\n            importlib.import_module(module_name) # may raise ImportError\n        except ImportError:\n            if not str(sys.exc_info()[1]).startswith(\"No module named '\" + module_name + \"'\"):\n                raise\n\n    # maybe the requested attribute is missing?\n    for module_name, local_obj_name in name_pairs:\n        try:\n            module = importlib.import_module(module_name) # may raise ImportError\n            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n        except ImportError:\n            pass\n\n    # we are out of luck, but we have no idea why\n    raise ImportError(obj_name)\n\n\ndef get_obj_from_module(module: types.ModuleType, obj_name: str) -> Any:\n    \"\"\"Traverses the object name and returns the last (rightmost) python object.\"\"\"\n    if obj_name == '':\n        return module\n    obj = module\n    for part in obj_name.split(\".\"):\n        obj = getattr(obj, part)\n    return obj\n\n\ndef get_obj_by_name(name: str) -> Any:\n    \"\"\"Finds the python object with the given name.\"\"\"\n    module, obj_name = get_module_from_obj_name(name)\n    return get_obj_from_module(module, obj_name)\n\n\ndef call_func_by_name(*args, func_name: str = None, **kwargs) -> Any:\n    \"\"\"Finds the python object with the given name and calls it as a function.\"\"\"\n    assert func_name is not None\n    func_obj = get_obj_by_name(func_name)\n    assert callable(func_obj)\n    return func_obj(*args, **kwargs)\n\n\ndef construct_class_by_name(*args, class_name: str = None, **kwargs) -> Any:\n    \"\"\"Finds the python class with the given name and constructs it with the given arguments.\"\"\"\n    return call_func_by_name(*args, func_name=class_name, **kwargs)\n\n\ndef get_module_dir_by_obj_name(obj_name: str) -> str:\n    \"\"\"Get the directory path of the module containing the given object name.\"\"\"\n    module, _ = get_module_from_obj_name(obj_name)\n    return os.path.dirname(inspect.getfile(module))\n\n\ndef is_top_level_function(obj: Any) -> bool:\n    \"\"\"Determine whether the given object is a top-level function, i.e., defined at module scope using 'def'.\"\"\"\n    return callable(obj) and obj.__name__ in sys.modules[obj.__module__].__dict__\n\n\ndef get_top_level_function_name(obj: Any) -> str:\n    \"\"\"Return the fully-qualified name of a top-level function.\"\"\"\n    assert is_top_level_function(obj)\n    module = obj.__module__\n    if module == '__main__':\n        module = os.path.splitext(os.path.basename(sys.modules[module].__file__))[0]\n    return module + \".\" + obj.__name__\n\n\n# File system helpers\n# ------------------------------------------------------------------------------------------\n\ndef list_dir_recursively_with_ignore(dir_path: str, ignores: List[str] = None, add_base_to_relative: bool = False) -> List[Tuple[str, str]]:\n    \"\"\"List all files recursively in a given directory while ignoring given file and directory names.\n    Returns list of tuples containing both absolute and relative paths.\"\"\"\n    assert os.path.isdir(dir_path)\n    base_name = os.path.basename(os.path.normpath(dir_path))\n\n    if ignores is None:\n        ignores = []\n\n    result = []\n\n    for root, dirs, files in os.walk(dir_path, topdown=True):\n        for ignore_ in ignores:\n            dirs_to_remove = [d for d in dirs if fnmatch.fnmatch(d, ignore_)]\n\n            # dirs need to be edited in-place\n            for d in dirs_to_remove:\n                dirs.remove(d)\n\n            files = [f for f in files if not fnmatch.fnmatch(f, ignore_)]\n\n        absolute_paths = [os.path.join(root, f) for f in files]\n        relative_paths = [os.path.relpath(p, dir_path) for p in absolute_paths]\n\n        if add_base_to_relative:\n            relative_paths = [os.path.join(base_name, p) for p in relative_paths]\n\n        assert len(absolute_paths) == len(relative_paths)\n        result += zip(absolute_paths, relative_paths)\n\n    return result\n\n\ndef copy_files_and_create_dirs(files: List[Tuple[str, str]]) -> None:\n    \"\"\"Takes in a list of tuples of (src, dst) paths and copies files.\n    Will create all necessary directories.\"\"\"\n    for file in files:\n        target_dir_name = os.path.dirname(file[1])\n\n        # will create all intermediate-level directories\n        if not os.path.exists(target_dir_name):\n            os.makedirs(target_dir_name)\n\n        shutil.copyfile(file[0], file[1])\n\n\n# URL helpers\n# ------------------------------------------------------------------------------------------\n\ndef is_url(obj: Any, allow_file_urls: bool = False) -> bool:\n    \"\"\"Determine whether the given object is a valid URL string.\"\"\"\n    if not isinstance(obj, str) or not \"://\" in obj:\n        return False\n    if allow_file_urls and obj.startswith('file://'):\n        return True\n    try:\n        res = requests.compat.urlparse(obj)\n        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n            return False\n        res = requests.compat.urlparse(requests.compat.urljoin(obj, \"/\"))\n        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n            return False\n    except:\n        return False\n    return True\n\n\ndef open_url(url: str, cache_dir: str = None, num_attempts: int = 10, verbose: bool = True, return_filename: bool = False, cache: bool = True) -> Any:\n    \"\"\"Download the given URL and return a binary-mode file object to access the data.\"\"\"\n    assert num_attempts >= 1\n    assert not (return_filename and (not cache))\n\n    # Doesn't look like an URL scheme so interpret it as a local filename.\n    if not re.match('^[a-z]+://', url):\n        return url if return_filename else open(url, \"rb\")\n\n    # Handle file URLs.  This code handles unusual file:// patterns that\n    # arise on Windows:\n    #\n    # file:///c:/foo.txt\n    #\n    # which would translate to a local '/c:/foo.txt' filename that's\n    # invalid.  Drop the forward slash for such pathnames.\n    #\n    # If you touch this code path, you should test it on both Linux and\n    # Windows.\n    #\n    # Some internet resources suggest using urllib.request.url2pathname() but\n    # but that converts forward slashes to backslashes and this causes\n    # its own set of problems.\n    if url.startswith('file://'):\n        filename = urllib.parse.urlparse(url).path\n        if re.match(r'^/[a-zA-Z]:', filename):\n            filename = filename[1:]\n        return filename if return_filename else open(filename, \"rb\")\n\n    assert is_url(url)\n\n    # Lookup from cache.\n    if cache_dir is None:\n        cache_dir = make_cache_dir_path('downloads')\n\n    url_md5 = hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n    if cache:\n        cache_files = glob.glob(os.path.join(cache_dir, url_md5 + \"_*\"))\n        if len(cache_files) == 1:\n            filename = cache_files[0]\n            return filename if return_filename else open(filename, \"rb\")\n\n    # Download.\n    url_name = None\n    url_data = None\n    with requests.Session() as session:\n        if verbose:\n            print(\"Downloading %s ...\" % url, end=\"\", flush=True)\n        for attempts_left in reversed(range(num_attempts)):\n            try:\n                with session.get(url) as res:\n                    res.raise_for_status()\n                    if len(res.content) == 0:\n                        raise IOError(\"No data received\")\n\n                    if len(res.content) < 8192:\n                        content_str = res.content.decode(\"utf-8\")\n                        if \"download_warning\" in res.headers.get(\"Set-Cookie\", \"\"):\n                            links = [html.unescape(link) for link in content_str.split('\"') if \"export=download\" in link]\n                            if len(links) == 1:\n                                url = requests.compat.urljoin(url, links[0])\n                                raise IOError(\"Google Drive virus checker nag\")\n                        if \"Google Drive - Quota exceeded\" in content_str:\n                            raise IOError(\"Google Drive download quota exceeded -- please try again later\")\n\n                    match = re.search(r'filename=\"([^\"]*)\"', res.headers.get(\"Content-Disposition\", \"\"))\n                    url_name = match[1] if match else url\n                    url_data = res.content\n                    if verbose:\n                        print(\" done\")\n                    break\n            except KeyboardInterrupt:\n                raise\n            except:\n                if not attempts_left:\n                    if verbose:\n                        print(\" failed\")\n                    raise\n                if verbose:\n                    print(\".\", end=\"\", flush=True)\n\n    # Save to cache.\n    if cache:\n        safe_name = re.sub(r\"[^0-9a-zA-Z-._]\", \"_\", url_name)\n        safe_name = safe_name[:min(len(safe_name), 128)]\n        cache_file = os.path.join(cache_dir, url_md5 + \"_\" + safe_name)\n        temp_file = os.path.join(cache_dir, \"tmp_\" + uuid.uuid4().hex + \"_\" + url_md5 + \"_\" + safe_name)\n        os.makedirs(cache_dir, exist_ok=True)\n        with open(temp_file, \"wb\") as f:\n            f.write(url_data)\n        os.replace(temp_file, cache_file) # atomic\n        if return_filename:\n            return cache_file\n\n    # Return data as file object.\n    assert not return_filename\n    return io.BytesIO(url_data)\n'''\nwith open('/kaggle/working/p2w/dnnlib/util.py', 'w') as f:\n    f.write(dnnlib)\nprint(\"Successfully wrote init.py to /kaggle/working/p2w/dnnlib/util.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:39:34.422425Z","iopub.execute_input":"2025-03-27T04:39:34.422659Z","iopub.status.idle":"2025-03-27T04:39:34.439193Z","shell.execute_reply.started":"2025-03-27T04:39:34.422638Z","shell.execute_reply":"2025-03-27T04:39:34.438101Z"}},"outputs":[{"name":"stdout","text":"Successfully wrote init.py to /kaggle/working/p2w/dnnlib/util.py\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"our checkpoints(ema)","metadata":{}},{"cell_type":"code","source":"# # Download checkpoints from Google Drive (replace with your file IDs)\n# import gdown\n# # Replace these with your actual Google Drive file IDs (from shareable links)\n# file_id_64 = '1YphSQBwEPmpp9SBhfKweUM0r-Oxv4ac4'  \n# file_id_256 = '1VWPhWWRz34jS5Nuz03lyTF9f5iX6PTCe' \n# gdown.download(f'https://drive.google.com/uc?id={file_id_64}', '/kaggle/working/p2w/checkpoints/64x64.pt', quiet=False)\n# gdown.download(f'https://drive.google.com/uc?id={file_id_256}', '/kaggle/working/p2w/checkpoints/256x256.pt', quiet=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:39:34.440167Z","iopub.execute_input":"2025-03-27T04:39:34.440504Z","iopub.status.idle":"2025-03-27T04:39:34.456230Z","shell.execute_reply.started":"2025-03-27T04:39:34.440474Z","shell.execute_reply":"2025-03-27T04:39:34.455490Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"author checkpoints(use fp16=False)","metadata":{}},{"cell_type":"code","source":"# Download checkpoints from Google Drive (replace with your file IDs)\nimport gdown\n# Replace these with your actual Google Drive file IDs (from shareable links)\nfile_id_64 = '1rN_YYKz01ED7T5uxSyHnqvCm2LTgaEC7'  \nfile_id_256 = '1Oq7tLMZn-faFFLHMVkLCe7E3m_vUZgol'\ngdown.download(f'https://drive.google.com/uc?id={file_id_64}', '/kaggle/working/p2w/checkpoints/64x64.pt', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id={file_id_256}', '/kaggle/working/p2w/checkpoints/256x256.pt', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:39:34.457034Z","iopub.execute_input":"2025-03-27T04:39:34.457238Z","iopub.status.idle":"2025-03-27T04:39:53.820229Z","shell.execute_reply.started":"2025-03-27T04:39:34.457214Z","shell.execute_reply":"2025-03-27T04:39:53.819460Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1rN_YYKz01ED7T5uxSyHnqvCm2LTgaEC7\nFrom (redirected): https://drive.google.com/uc?id=1rN_YYKz01ED7T5uxSyHnqvCm2LTgaEC7&confirm=t&uuid=31ec0f58-5755-48a7-aaca-43a22b019bec\nTo: /kaggle/working/p2w/checkpoints/64x64.pt\n100%|██████████| 273M/273M [00:05<00:00, 50.7MB/s] \nDownloading...\nFrom (original): https://drive.google.com/uc?id=1Oq7tLMZn-faFFLHMVkLCe7E3m_vUZgol\nFrom (redirected): https://drive.google.com/uc?id=1Oq7tLMZn-faFFLHMVkLCe7E3m_vUZgol&confirm=t&uuid=f16cbeec-25ef-48ab-80c9-cf7adb6717a2\nTo: /kaggle/working/p2w/checkpoints/256x256.pt\n100%|██████████| 374M/374M [00:05<00:00, 73.4MB/s] \n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/p2w/checkpoints/256x256.pt'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"import gdown\ngdown.download(f'https://drive.google.com/uc?id=14e8xccYuLSu0rWPT9LvgWOh6LBCEJf1m', '/kaggle/working/p2w/torch_utils/init.py', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=1k0LR7-6jRHnQrf22lGvieVLK62t2ywXO', '/kaggle/working/p2w/torch_utils/distributed.py', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=1OWe7gRTFKF233mWIkShRwQ03GQryI_tp', '/kaggle/working/p2w/torch_utils/download_util.py', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=1uzdxdkYzczjRuZ_w76MKwgRgI4eugdPb', '/kaggle/working/p2w/torch_utils/misc.py', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=1rsI21_KkmeYROA1nEplZfiHQleQDywfs', '/kaggle/working/p2w/torch_utils/persistence.py', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=1NyBwHRxBPb0oPJDJ9O4cewdoPrXebOB0', '/kaggle/working/p2w/torch_utils/training_stats.py', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=1QkxOC0Md06uAzQwfdRpNzSh2e8UNIgH8', '/kaggle/working/p2w/ldm/util.py', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:39:53.821023Z","iopub.execute_input":"2025-03-27T04:39:53.821470Z","iopub.status.idle":"2025-03-27T04:40:22.287758Z","shell.execute_reply.started":"2025-03-27T04:39:53.821445Z","shell.execute_reply":"2025-03-27T04:40:22.286945Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=14e8xccYuLSu0rWPT9LvgWOh6LBCEJf1m\nFrom (redirected): https://drive.google.com/uc?id=14e8xccYuLSu0rWPT9LvgWOh6LBCEJf1m&confirm=t&uuid=dd5bbe42-41e9-470e-8808-e71f30e8098b\nTo: /kaggle/working/p2w/torch_utils/init.py\n100%|██████████| 337/337 [00:00<00:00, 1.48MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1k0LR7-6jRHnQrf22lGvieVLK62t2ywXO\nFrom (redirected): https://drive.google.com/uc?id=1k0LR7-6jRHnQrf22lGvieVLK62t2ywXO&confirm=t&uuid=2a1008da-0acb-4fbd-b77d-52b083de946e\nTo: /kaggle/working/p2w/torch_utils/distributed.py\n100%|██████████| 2.09k/2.09k [00:00<00:00, 2.99MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1OWe7gRTFKF233mWIkShRwQ03GQryI_tp\nFrom (redirected): https://drive.google.com/uc?id=1OWe7gRTFKF233mWIkShRwQ03GQryI_tp&confirm=t&uuid=c217fc52-a858-4d6b-a8d3-d6ef7c26066e\nTo: /kaggle/working/p2w/torch_utils/download_util.py\n100%|██████████| 5.59k/5.59k [00:00<00:00, 8.48MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1uzdxdkYzczjRuZ_w76MKwgRgI4eugdPb\nFrom (redirected): https://drive.google.com/uc?id=1uzdxdkYzczjRuZ_w76MKwgRgI4eugdPb&confirm=t&uuid=a15469d9-81b2-4d5a-a3a2-701fe3e581f0\nTo: /kaggle/working/p2w/torch_utils/misc.py\n100%|██████████| 11.0k/11.0k [00:00<00:00, 13.8MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1rsI21_KkmeYROA1nEplZfiHQleQDywfs\nFrom (redirected): https://drive.google.com/uc?id=1rsI21_KkmeYROA1nEplZfiHQleQDywfs&confirm=t&uuid=40e5e705-1de5-4bba-9f8a-aa4513186d35\nTo: /kaggle/working/p2w/torch_utils/persistence.py\n100%|██████████| 10.0k/10.0k [00:00<00:00, 10.3MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1NyBwHRxBPb0oPJDJ9O4cewdoPrXebOB0\nFrom (redirected): https://drive.google.com/uc?id=1NyBwHRxBPb0oPJDJ9O4cewdoPrXebOB0&confirm=t&uuid=c2bb6ef3-c125-49b9-8271-42f65d327649\nTo: /kaggle/working/p2w/torch_utils/training_stats.py\n100%|██████████| 10.7k/10.7k [00:00<00:00, 13.1MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1QkxOC0Md06uAzQwfdRpNzSh2e8UNIgH8\nFrom (redirected): https://drive.google.com/uc?id=1QkxOC0Md06uAzQwfdRpNzSh2e8UNIgH8&confirm=t&uuid=7e5e55cb-453f-417e-86c4-f68309c73e1d\nTo: /kaggle/working/p2w/ldm/util.py\n100%|██████████| 5.86k/5.86k [00:00<00:00, 5.41MB/s]\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/p2w/ldm/util.py'"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"**RUN UNTIL THIS FOR SAPLING FROM PKL+SOLVER_AMEDS +SOLVER_UTILS+SAMPLE_AMED**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"networks_code = '''import numpy as np\nimport torch\nfrom torch_utils import persistence\nfrom torch.nn.functional import silu\n\n#----------------------------------------------------------------------------\n# Unified routine for initializing weights and biases.\n\ndef weight_init(shape, mode, fan_in, fan_out):\n    if mode == 'xavier_uniform': return np.sqrt(6 / (fan_in + fan_out)) * (torch.rand(*shape) * 2 - 1)\n    if mode == 'xavier_normal':  return np.sqrt(2 / (fan_in + fan_out)) * torch.randn(*shape)\n    if mode == 'kaiming_uniform': return np.sqrt(3 / fan_in) * (torch.rand(*shape) * 2 - 1)\n    if mode == 'kaiming_normal':  return np.sqrt(1 / fan_in) * torch.randn(*shape)\n    raise ValueError(f'Invalid init mode \"{mode}\"')\n\n#----------------------------------------------------------------------------\n# Fully-connected layer.\n\n@persistence.persistent_class\nclass Linear(torch.nn.Module):\n    def __init__(self, in_features, out_features, bias=True, init_mode='kaiming_normal', init_weight=1, init_bias=0):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        init_kwargs = dict(mode=init_mode, fan_in=in_features, fan_out=out_features)\n        self.weight = torch.nn.Parameter(weight_init([out_features, in_features], **init_kwargs) * init_weight)\n        self.bias = torch.nn.Parameter(weight_init([out_features], **init_kwargs) * init_bias) if bias else None\n\n    def forward(self, x):\n        x = x @ self.weight.to(x.dtype).t()\n        if self.bias is not None:\n            x = x.add_(self.bias.to(x.dtype))\n        return x\n\n#----------------------------------------------------------------------------\n# Timestep embedding used in the DDPM++ and ADM architectures.\n\n@persistence.persistent_class\nclass PositionalEmbedding(torch.nn.Module):\n    def __init__(self, num_channels, max_positions=10000, endpoint=False):\n        super().__init__()\n        self.num_channels = num_channels\n        self.max_positions = max_positions\n        self.endpoint = endpoint\n\n    def forward(self, x):\n        freqs = torch.arange(start=0, end=self.num_channels//2, dtype=torch.float32, device=x.device)\n        freqs = freqs / (self.num_channels // 2 - (1 if self.endpoint else 0))\n        freqs = (1 / self.max_positions) ** freqs\n        x = x.ger(freqs.to(x.dtype))\n        x = torch.cat([x.cos(), x.sin()], dim=1)\n        return x\n\n#----------------------------------------------------------------------------\n\n@persistence.persistent_class\nclass AMED_predictor(torch.nn.Module):\n    def __init__(\n        self,\n        hidden_dim              = 128,\n        output_dim              = 1,\n        bottleneck_input_dim    = 64,  \n        bottleneck_output_dim   = 4,\n        noise_channels          = 8,\n        embedding_type          = 'positional',\n        dataset_name            = None,\n        img_resolution          = None,\n        num_steps               = None,\n        sampler_tea             = None,\n        sampler_stu             = None,\n        M                       = None,\n        guidance_type           = None,\n        guidance_rate           = None,\n        schedule_type           = None,\n        schedule_rho            = None,\n        afs                     = False,\n        scale_dir               = 0,\n        scale_time              = 0,\n        max_order               = None,\n        predict_x0              = True,\n        lower_order_final       = True,\n    ):\n        super().__init__()\n        assert sampler_stu in ['amed', 'dpm', 'dpmpp', 'euler', 'ipndm']\n        assert sampler_tea in ['heun', 'dpm', 'dpmpp', 'euler', 'ipndm']\n        assert scale_dir >= 0\n        assert scale_time >= 0\n        self.dataset_name = dataset_name\n        self.img_resolution = img_resolution\n        self.num_steps = num_steps\n        self.sampler_stu = sampler_stu\n        self.sampler_tea = sampler_tea\n        self.M = M\n        self.guidance_type = guidance_type\n        self.guidance_rate = guidance_rate\n        self.schedule_type = schedule_type\n        self.schedule_rho = schedule_rho\n        self.afs = afs\n        self.scale_dir = scale_dir\n        self.scale_time = scale_time\n        self.max_order = max_order\n        self.predict_x0 = predict_x0\n        self.lower_order_final = lower_order_final\n        \n        init = dict(init_mode='xavier_uniform')\n        \n        self.map_noise = PositionalEmbedding(num_channels=noise_channels, endpoint=True)\n        self.map_layer0 = Linear(in_features=noise_channels, out_features=noise_channels, **init)\n        \n        self.enc_layer0 = Linear(bottleneck_input_dim, hidden_dim)\n        self.enc_layer1 = Linear(hidden_dim, bottleneck_output_dim)\n        \n        self.fc_r = Linear(2 * noise_channels + bottleneck_output_dim, output_dim)\n        if self.scale_dir:\n            self.fc_scale_dir = Linear(2 * noise_channels + bottleneck_output_dim, output_dim)\n        if self.scale_time:\n            self.fc_scale_time = Linear(2 * noise_channels + bottleneck_output_dim, output_dim)\n\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, unet_bottleneck, t_cur, t_next, class_labels=None):\n        # emb = self.map_noise(t_cur.reshape(1,))\n        # emb = emb.reshape(emb.shape[0], 2, -1).flip(1).reshape(*emb.shape) # swap sin/cos\n        # emb = silu(self.map_layer0(emb)).repeat(unet_bottleneck.shape[0], 1)\n        # emb1 = self.map_noise(t_next.reshape(1,))\n        # emb1 = emb1.reshape(emb1.shape[0], 2, -1).flip(1).reshape(*emb1.shape) # swap sin/cos\n        # emb1 = silu(self.map_layer0(emb1)).repeat(unet_bottleneck.shape[0], 1)\n        # emb = torch.cat((emb, emb1), dim=1)\n        batch_size = t_cur.size(0)\n        t_cur = t_cur.view(batch_size)  # [8]\n        t_next = t_next.view(batch_size)  # [8]\n        emb = self.map_noise(t_cur)  # [8, 8]\n        emb = emb.reshape(emb.shape[0], 2, -1).flip(1).reshape(*emb.shape)\n        emb = silu(self.map_layer0(emb))  # [8, 8]\n        emb1 = self.map_noise(t_next)  # [8, 8]\n        emb1 = emb1.reshape(emb1.shape[0], 2, -1).flip(1).reshape(*emb1.shape)\n        emb1 = silu(self.map_layer0(emb1))  # [8, 8]\n        emb = torch.cat((emb, emb1), dim=1)  # [8, 16]\n        \n        unet_bottleneck = unet_bottleneck.reshape(unet_bottleneck.shape[0], -1)\n        unet_bottleneck = self.enc_layer0(unet_bottleneck)\n        unet_bottleneck = silu(unet_bottleneck)\n        unet_bottleneck = self.enc_layer1(unet_bottleneck)\n        out = torch.cat((unet_bottleneck, emb), dim=1)\n\n        r = self.fc_r(out)\n        r = self.sigmoid(r)\n\n        if self.scale_dir:\n            scale_dir = self.fc_scale_dir(out)\n            scale_dir = self.sigmoid(scale_dir) / (1 / (2 * self.scale_dir)) + (1 - self.scale_dir)\n            if not self.scale_time:\n                return r, scale_dir\n\n        if self.scale_time:\n            scale_time = self.fc_scale_time(out)\n            scale_time = self.sigmoid(scale_time) / (1 / (2 * self.scale_time)) + (1 - self.scale_time)\n            if not self.scale_dir:\n                return r, scale_time\n            else:\n                return r, scale_dir, scale_time\n\n        return r\n'''\nwith open('/kaggle/working/p2w/training_amed/networks.py','w') as f:\n    f.write(networks_code)\nprint(\"Successfully wrote networks.py to /kaggle/working/p2w/training_amed/networks.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:40:22.288636Z","iopub.execute_input":"2025-03-27T04:40:22.288904Z","iopub.status.idle":"2025-03-27T04:40:22.294885Z","shell.execute_reply.started":"2025-03-27T04:40:22.288869Z","shell.execute_reply":"2025-03-27T04:40:22.293963Z"}},"outputs":[{"name":"stdout","text":"Successfully wrote networks.py to /kaggle/working/p2w/training_amed/networks.py\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"losses='''\nimport torch\nfrom torch_utils import persistence\nfrom torch_utils import distributed as dist\nimport solvers_amed\nfrom solver_utils_amed import get_schedule\n\n#----------------------------------------------------------------------------\n\ndef get_solver_fn(solver_name):\n    if solver_name == 'amed':\n        solver_fn = solvers_amed.amed_sampler\n    elif solver_name == 'euler':\n        solver_fn = solvers_amed.euler_sampler\n    elif solver_name == 'ipndm':\n        solver_fn = solvers_amed.ipndm_sampler\n    elif solver_name == 'dpm':\n        solver_fn = solvers_amed.dpm_2_sampler\n    elif solver_name == 'dpmpp':\n        solver_fn = solvers_amed.dpm_pp_sampler\n    elif solver_name == 'heun':\n        solver_fn = solvers_amed.heun_sampler\n    else:\n        raise ValueError(\"Got wrong solver name {}\".format(solver_name))\n    return solver_fn\n\n#----------------------------------------------------------------------------\n\n@persistence.persistent_class\nclass AMED_loss:\n    def __init__(\n        self, num_steps=None, sampler_stu=None, sampler_tea=None, M=None,\n        schedule_type=None, schedule_rho=None, afs=False, max_order=None,\n        sigma_min=None, sigma_max=None, predict_x0=True, lower_order_final=True,\n    ):\n        self.num_steps = num_steps\n        self.solver_stu = get_solver_fn(sampler_stu)\n        self.solver_tea = get_solver_fn(sampler_tea)\n        self.M = M\n        self.schedule_type = schedule_type\n        self.schedule_rho = schedule_rho\n        self.afs = afs\n        self.max_order = max_order\n        self.sigma_min = sigma_min\n        self.sigma_max = sigma_max\n        self.predict_x0 = predict_x0\n        self.lower_order_final = lower_order_final\n        self.t_steps = None\n        self.buffer_model = []\n        self.buffer_t = []\n\n    def __call__(self, AMED_predictor, net, tensor_in, labels=None, step_idx=None, teacher_out=None, condition=None, unconditional_condition=None,diffusion=None):\n        step_idx = torch.tensor([step_idx]).reshape(1,)\n        t_cur = self.t_steps[step_idx].to(tensor_in.device)\n        t_next = self.t_steps[step_idx + 1].to(tensor_in.device)\n        if step_idx == 0:\n            self.buffer_model = []\n            self.buffer_t = []\n\n        student_out, buffer_model, buffer_t, r, scale_dir, scale_time = self.solver_stu(\n            net, tensor_in / t_cur, class_labels=labels, condition=condition, unconditional_condition=unconditional_condition,\n            num_steps=2, sigma_min=t_next, sigma_max=t_cur, schedule_type=self.schedule_type, schedule_rho=self.schedule_rho,\n            afs=self.afs, denoise_to_zero=False, return_inters=False, AMED_predictor=AMED_predictor, step_idx=step_idx,\n            train=True, predict_x0=self.predict_x0, lower_order_final=self.lower_order_final, max_order=self.max_order,\n            buffer_model=self.buffer_model, buffer_t=self.buffer_t,diffusion=diffusion\n        )\n        self.buffer_model = buffer_model\n        self.buffer_t = buffer_t\n\n        loss = (student_out - teacher_out) ** 2\n        dist.print0(\"Step: {} | Loss: {:8.4f} | r (mean std): {:5.4f} {:5.4f} | scale_dir (mean std): {:5.4f} {:5.4f} | scale_time (mean std): {:5.4f} {:5.4f}\".format(\n            step_idx.item(), torch.mean(torch.norm(loss, p=2, dim=(1, 2, 3))).item(),\n            r.mean().item(), r.std().item(), scale_dir.mean().item(), scale_dir.std().item(),\n            scale_time.mean().item(), scale_time.std().item()\n        ))\n        return loss, student_out.detach()\n\n    def get_teacher_traj(self, net, tensor_in, labels=None, condition=None, unconditional_condition=None,diffusion=None):\n        if self.t_steps is None:\n            self.t_steps = get_schedule(self.num_steps, self.sigma_min, self.sigma_max, schedule_type=self.schedule_type, schedule_rho=self.schedule_rho, device=tensor_in.device, net=net,diffusion=diffusion)\n            #print(f\"[AMED_loss] t_steps: {self.t_steps}\")\n        num_steps_teacher = (self.M + 1) * (self.num_steps - 1) + 1\n        tea_slice = [i * (self.M + 1) for i in range(1, self.num_steps)]\n\n        with torch.no_grad():\n            teacher_traj = self.solver_tea(\n                net, tensor_in / self.t_steps[0], class_labels=labels, condition=condition, unconditional_condition=unconditional_condition,\n                num_steps=num_steps_teacher, sigma_min=self.sigma_min, sigma_max=self.sigma_max,\n                schedule_type=self.schedule_type, schedule_rho=self.schedule_rho, afs=False,\n                denoise_to_zero=False, return_inters=True, AMED_predictor=None, train=False,\n                predict_x0=self.predict_x0, lower_order_final=self.lower_order_final, max_order=self.max_order,diffusion=diffusion\n            )\n        return teacher_traj[tea_slice]\n'''\nwith open('/kaggle/working/p2w/training_amed/loss.py', 'w') as f:\n    f.write(losses)\nprint(\"Successfully wrote loss.py to /kaggle/working/p2w/training_amed/loss.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T06:20:49.653168Z","iopub.execute_input":"2025-03-27T06:20:49.653555Z","iopub.status.idle":"2025-03-27T06:20:49.660715Z","shell.execute_reply.started":"2025-03-27T06:20:49.653503Z","shell.execute_reply":"2025-03-27T06:20:49.660013Z"}},"outputs":[{"name":"stdout","text":"Successfully wrote loss.py to /kaggle/working/p2w/training_amed/loss.py\n","output_type":"stream"}],"execution_count":100},{"cell_type":"markdown","source":"**OLD SOLVERS AMED NO INPAINTING INSIDE SOLVERS**","metadata":{}},{"cell_type":"code","source":"solvers_amed_code='''import torch\nfrom solver_utils_amed import *\n\n#----------------------------------------------------------------------------\n# Initialize the hook function to get the U-Net bottleneck outputs\n\ndef init_hook(net, class_labels=None):\n    unet_enc_out = []\n    def hook_fn(module, input, output):\n        unet_enc_out.append(output.detach())\n    if hasattr(net, 'guidance_type'):                                       # models from LDM and Stable Diffusion\n        hook = net.model.model.diffusion_model.middle_block.register_forward_hook(hook_fn)\n    elif net.image_size == 256 or net.image_size==64:                                       # models from CM and ADM with resolution of 256\n        hook = net.middle_block.register_forward_hook(hook_fn)\n    else:                                                                   # models from EDM\n        module_name = '8x8_block2' if class_labels is not None else '8x8_block3'\n        hook = net.model.enc[module_name].register_forward_hook(hook_fn)\n    return unet_enc_out, hook\n\n#----------------------------------------------------------------------------\n\ndef get_amed_prediction(AMED_predictor, t_cur, t_next, net, unet_enc_out, use_afs, batch_size):\n    if hasattr(net, 'guidance_type') and net.guidance_type == 'classifier-free':\n        unet_enc = torch.mean(unet_enc_out[-1], dim=1) if not use_afs else torch.zeros((2*batch_size, 8, 8), device=t_cur.device)\n        output = AMED_predictor(unet_enc[batch_size:], t_cur, t_next)\n    else:\n        unet_enc = torch.mean(unet_enc_out[-1], dim=1) if not use_afs else torch.zeros((batch_size, 8, 8), device=t_cur.device)\n        output = AMED_predictor(unet_enc, t_cur, t_next)\n    output_list = [*output]\n    \n    if len(output_list) == 2:\n        try:\n            use_scale_time = AMED_predictor.module.scale_time\n        except:\n            use_scale_time = AMED_predictor.scale_time\n        if use_scale_time:\n            r, scale_time = output_list\n            r = r.reshape(-1, 1, 1, 1)\n            scale_time = scale_time.reshape(-1, 1, 1, 1)\n            scale_dir = torch.ones_like(scale_time)\n        else:\n            r, scale_dir = output_list\n            r = r.reshape(-1, 1, 1, 1)\n            scale_dir = scale_dir.reshape(-1, 1, 1, 1)\n            scale_time = torch.ones_like(scale_dir)\n    elif len(output_list) == 3:\n        r, scale_dir, scale_time = output_list\n        r = r.reshape(-1, 1, 1, 1)\n        scale_dir = scale_dir.reshape(-1, 1, 1, 1)\n        scale_time = scale_time.reshape(-1, 1, 1, 1)\n    else:\n        r = output.reshape(-1, 1, 1, 1)\n        scale_dir = torch.ones_like(r)\n        scale_time = torch.ones_like(r)\n    return r, scale_dir, scale_time\n\n#----------------------------------------------------------------------------\n# Get the denoised output from the pre-trained diffusion models.\n\ndef get_denoised(net, x, t, class_labels=None, condition=None, unconditional_condition=None):\n    if hasattr(net, 'guidance_type'):     # models from LDM and Stable Diffusion\n        denoised = net(x, t, condition=condition, unconditional_condition=unconditional_condition)\n    else:\n        denoised = net(x, t, y=class_labels) #NR:was class_labels=class_labels\n    return denoised\n\n#----------------------------------------------------------------------------\n\ndef amed_sampler(\n    net, \n    latents, \n    class_labels=None,\n    condition=None, \n    unconditional_condition=None,\n    num_steps=None, \n    sigma_min=0.002, \n    sigma_max=80, \n    schedule_type='time_uniform',\n    schedule_rho=1, \n    afs=False, \n    denoise_to_zero=False, \n    return_inters=False,\n    AMED_predictor=None, \n    step_idx=None, \n    train=False, \n    **kwargs\n):\n    \"\"\"\n    AMED-Solver (https://arxiv.org/abs/2312.00094).\n\n    Args:\n        net: A wrapped diffusion model.\n        latents: A pytorch tensor. Input sample at time `sigma_max`.\n        class_labels: A pytorch tensor. The condition for conditional sampling or guided sampling.\n        condition: A pytorch tensor. The condition to the model used in LDM and Stable Diffusion\n        unconditional_condition: A pytorch tensor. The unconditional condition to the model used in LDM and Stable Diffusion\n        num_steps: A `int`. The total number of the time steps with `num_steps-1` spacings. \n        sigma_min: A `float`. The ending sigma during samping.\n        sigma_max: A `float`. The starting sigma during sampling.\n        schedule_type: A `str`. The type of time schedule. We support three types:\n            - 'polynomial': polynomial time schedule. (Recommended in EDM.)\n            - 'logsnr': uniform logSNR time schedule. (Recommended in DPM-Solver for small-resolution datasets.)\n            - 'time_uniform': uniform time schedule. (Recommended in DPM-Solver for high-resolution datasets.)\n            - 'discrete': time schedule used in LDM. (Recommended when using pre-trained diffusion models from the LDM and Stable Diffusion codebases.)\n        schedule_rho: A `float`. Time step exponent. Need to be specified when schedule_type in ['polynomial', 'time_uniform'].\n        afs: A `bool`. Whether to use analytical first step (AFS) at the beginning of sampling.\n        denoise_to_zero: A `bool`. Whether to denoise the sample to from `sigma_min` to `0` at the end of sampling.\n        return_inters: A `bool`. Whether to save intermediate results, i.e. the whole sampling trajectory.\n        AMED_predictor: A predictor network.\n        step_idx: A `int`. An index to specify the sampling step for training.\n        train: A `bool`. In the training loop?\n    Returns:\n        A pytorch tensor. A batch of generated samples or sampling trajectories if return_inters=True.\n    \"\"\"\n    assert AMED_predictor is not None\n    #print(f\"[amed_sampler] sigma_max: {sigma_max}, sigma_min: {sigma_min}\")\n    # Time step discretization.\n    t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho, net=net)\n    #print(f\"[amed_sampler] t_steps: {t_steps}\")\n    # Main sampling loop.\n    x_next = latents * t_steps[0]\n    inters = [x_next.unsqueeze(0)]\n    batch_size = latents.shape[0]\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):  # 0, ..., N-1\n        x_cur = x_next\n        unet_enc_out, hook = init_hook(net, class_labels)\n        \n        # Expand t_cur and t_next to [batch_size]\n        t_cur = t_cur.expand(batch_size)  # [8]\n        t_next = t_next.expand(batch_size)  # [8]\n\n        # Reshape for broadcasting in division\n        t_cur_broadcast = t_cur.view(batch_size, 1, 1, 1)  # [8, 1, 1, 1]\n        t_next_broadcast = t_next.view(batch_size, 1, 1, 1)  # [8, 1, 1, 1]\n\n        # Euler step.\n        use_afs = afs and (((not train) and i == 0) or (train and step_idx == 0))\n        if use_afs:\n            d_cur = x_cur / ((1 + t_cur_broadcast**2).sqrt())\n        else:\n            model_output = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            denoised = model_output[:, :3]  # Final denoised output\n            d_cur = (x_cur - denoised) / t_cur_broadcast\n\n        hook.remove()\n        r, scale_dir, scale_time = get_amed_prediction(AMED_predictor, t_cur_broadcast, t_next_broadcast, net, unet_enc_out, use_afs, batch_size=latents.shape[0])\n        t_mid = (t_next_broadcast ** r) * (t_cur_broadcast ** (1 - r))  # [8, 1, 1, 1]\n        # Compute a 1D version of t_mid for get_denoised\n        t_mid_1d = t_mid.view(batch_size)  # [8]\n        x_next = x_cur + (t_mid - t_cur_broadcast) * d_cur\n\n        # Apply 2nd order correction.\n        # Use t_mid_1d (1D) for get_denoised, ignoring scale_time as it should be t_mid directly\n        model_output = get_denoised(net, x_next, t_mid_1d, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        denoised = model_output[:, :3]  # Final denoised output\n        d_mid = (x_next - denoised) / t_mid\n        x_next = x_cur + scale_dir * (t_next_broadcast - t_cur_broadcast) * d_mid\n    \n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n        \n    if denoise_to_zero:\n        # Use t_next (1D) for get_denoised, matching heun_sampler\n        model_output = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        x_next = model_output[:, :3]  # Final denoised output\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n\n    if return_inters:\n        return torch.cat(inters, dim=0).to(latents.device)\n    if train:\n        return x_next, [], [], r, scale_dir, scale_time\n    return x_next\n    # assert AMED_predictor is not None\n\n    # # Time step discretization.\n    # t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho, net=net)\n    \n    # # Main sampling loop.\n    # x_next = latents * t_steps[0]\n    # inters = [x_next.unsqueeze(0)]\n    # batch_size = latents.shape[0]\n    # for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):                # 0, ..., N-1\n    #     x_cur = x_next\n    #     unet_enc_out, hook = init_hook(net, class_labels)\n\n    #     # Expand t_cur and t_next to [batch_size]\n    #     t_cur = t_cur.expand(batch_size)  # [8]\n    #     t_next = t_next.expand(batch_size)  # [8]\n\n    #     # Reshape for broadcasting in division\n    #     t_cur_broadcast = t_cur.view(batch_size, 1, 1, 1)  # [8, 1, 1, 1]\n    #     t_next_broadcast = t_next.view(batch_size, 1, 1, 1)  # [8, 1, 1, 1]\n        \n    #     # Euler step.\n    #     use_afs = afs and (((not train) and i == 0) or (train and step_idx == 0))\n    #     if use_afs:\n    #         d_cur = x_cur / ((1 + t_cur_broadcast**2).sqrt())\n    #     else:\n    #         model_output = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n    #         denoised = model_output[:, :3]  # Final denoised output\n    #         d_cur = (x_cur - denoised) / t_cur_broadcast\n\n    #     hook.remove()\n    #     r, scale_dir, scale_time = get_amed_prediction(AMED_predictor, t_cur_broadcast, t_next_broadcast, net, unet_enc_out, use_afs, batch_size=latents.shape[0])\n    #     t_mid = (t_next_broadcast ** r) * (t_cur_broadcast ** (1 - r))\n    #     x_next = x_cur + (t_mid - t_cur_broadcast) * d_cur\n        \n\n    #     # Apply 2nd order correction.\n    #     t_mid_flat = t_mid.view(-1)  # [8]\n    #     model_output = get_denoised(net, x_next, t_mid_flat, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n    #     denoised = model_output[:, :3]  # Final denoised output\n    #     d_mid = (x_next - denoised) / t_mid\n    #     x_next = x_cur + scale_dir * (t_next_broadcast - t_cur_broadcast) * d_mid\n    \n    #     if return_inters:\n    #         inters.append(x_next.unsqueeze(0))\n        \n    # if denoise_to_zero:\n    #     # Flatten t_next for get_denoised\n    #     t_next_flat = t_next.view(-1)  # [8]\n    #     model_output = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n    #     x_next = model_output[:, :3]  # Final denoised output\n    #     if return_inters:\n    #         inters.append(x_next.unsqueeze(0))\n\n    # if return_inters:\n    #     return torch.cat(inters, dim=0).to(latents.device)\n    # if train:\n    #     return x_next, [], [], r, scale_dir, scale_time\n    # return x_next\n\n#----------------------------------------------------------------------------\ndef euler_sampler(\n    net, \n    latents, \n    class_labels=None, \n    condition=None, \n    unconditional_condition=None,\n    num_steps=None, \n    sigma_min=0.002, \n    sigma_max=80, \n    schedule_type='polynomial',\n    schedule_rho=7, \n    afs=False, \n    denoise_to_zero=False, \n    return_inters=False, \n    AMED_predictor=None, \n    step_idx=None, \n    train=False, \n    **kwargs\n):  \n    \"\"\"\n    AMED-Plugin for Euler sampler.\n\n    Args:\n        net: A wrapped diffusion model.\n        latents: A pytorch tensor. Input sample at time `sigma_max`.\n        class_labels: A pytorch tensor. The condition for conditional sampling or guided sampling.\n        condition: A pytorch tensor. The condition to the model used in LDM and Stable Diffusion\n        unconditional_condition: A pytorch tensor. The unconditional condition to the model used in LDM and Stable Diffusion\n        num_steps: A `int`. The total number of the time steps with `num_steps-1` spacings. \n        sigma_min: A `float`. The ending sigma during samping.\n        sigma_max: A `float`. The starting sigma during sampling.\n        schedule_type: A `str`. The type of time schedule. We support three types:\n            - 'polynomial': polynomial time schedule. (Recommended in EDM.)\n            - 'logsnr': uniform logSNR time schedule. (Recommended in DPM-Solver for small-resolution datasets.)\n            - 'time_uniform': uniform time schedule. (Recommended in DPM-Solver for high-resolution datasets.)\n            - 'discrete': time schedule used in LDM. (Recommended when using pre-trained diffusion models from the LDM and Stable Diffusion codebases.)\n        schedule_rho: A `float`. Time step exponent. Need to be specified when schedule_type in ['polynomial', 'time_uniform'].\n        afs: A `bool`. Whether to use analytical first step (AFS) at the beginning of sampling.\n        denoise_to_zero: A `bool`. Whether to denoise the sample to from `sigma_min` to `0` at the end of sampling.\n        return_inters: A `bool`. Whether to save intermediate results, i.e. the whole sampling trajectory.\n        AMED_predictor: A predictor network.\n        step_idx: A `int`. An index to specify the sampling step for training.\n        train: A `bool`. In the training loop?\n    Returns:\n        A pytorch tensor. A batch of generated samples or sampling trajectories if return_inters=True.\n    \"\"\"\n\n    # Time step discretization.\n    t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho, net=net)\n\n    # Main sampling loop.\n    x_next = latents * t_steps[0]\n    inters = [x_next.unsqueeze(0)]\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):                # 0, ..., N-1\n        x_cur = x_next\n        if AMED_predictor is not None:\n            unet_enc_out, hook = init_hook(net, class_labels)\n\n        # Euler step.\n        use_afs = afs and (((not train) and i == 0) or (train and step_idx == 0))\n        if use_afs:\n            d_cur = x_cur / ((1 + t_cur**2).sqrt())\n        else:\n            denoised = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            d_cur = (x_cur - denoised) / t_cur\n            \n        if AMED_predictor is not None:\n            hook.remove()\n            t_cur = t_cur.reshape(-1, 1, 1, 1)\n            t_next = t_next.reshape(-1, 1, 1, 1)\n            r, scale_dir, scale_time = get_amed_prediction(AMED_predictor, t_cur, t_next, net, unet_enc_out, use_afs, batch_size=latents.shape[0])\n            t_mid = (t_next**r) * (t_cur**(1-r))\n            x_next = x_cur + (t_mid - t_cur) * d_cur\n        else:\n            x_next = x_cur + (t_next - t_cur) * d_cur\n        \n        # One more step for student\n        if AMED_predictor is not None:\n            denoised = get_denoised(net, x_next, scale_time * t_mid, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            d_mid = (x_next - denoised) / t_mid\n            x_next = x_next + scale_dir * (t_next - t_mid) * d_mid\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n    \n    if denoise_to_zero:\n        x_next = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n\n    if return_inters:\n        return torch.cat(inters, dim=0).to(latents.device)\n    if train:\n        return x_next, [], [], r, scale_dir, scale_time\n    return x_next\n\n\n#----------------------------------------------------------------------------\n\ndef ipndm_sampler(\n    net, \n    latents, \n    class_labels=None, \n    condition=None, \n    unconditional_condition=None,\n    num_steps=None, \n    sigma_min=0.002, \n    sigma_max=80, \n    schedule_type='polynomial',\n    schedule_rho=7, \n    afs=False,\n    denoise_to_zero=False, \n    return_inters=False, \n    AMED_predictor=None, \n    train=False, \n    max_order=4, \n    buffer_model=[], \n    **kwargs\n):\n    \"\"\"\n    AMED-Plugin for improved PNDM sampler.\n\n    Args:\n        net: A wrapped diffusion model.\n        latents: A pytorch tensor. Input sample at time `sigma_max`.\n        class_labels: A pytorch tensor. The condition for conditional sampling or guided sampling.\n        condition: A pytorch tensor. The condition to the model used in LDM and Stable Diffusion\n        unconditional_condition: A pytorch tensor. The unconditional condition to the model used in LDM and Stable Diffusion\n        num_steps: A `int`. The total number of the time steps with `num_steps-1` spacings. \n        sigma_min: A `float`. The ending sigma during samping.\n        sigma_max: A `float`. The starting sigma during sampling.\n        schedule_type: A `str`. The type of time schedule. We support three types:\n            - 'polynomial': polynomial time schedule. (Recommended in EDM.)\n            - 'logsnr': uniform logSNR time schedule. (Recommended in DPM-Solver for small-resolution datasets.)\n            - 'time_uniform': uniform time schedule. (Recommended in DPM-Solver for high-resolution datasets.)\n            - 'discrete': time schedule used in LDM. (Recommended when using pre-trained diffusion models from the LDM and Stable Diffusion codebases.)\n        schedule_rho: A `float`. Time step exponent. Need to be specified when schedule_type in ['polynomial', 'time_uniform'].\n        afs: A `bool`. Whether to use analytical first step (AFS) at the beginning of sampling.\n        denoise_to_zero: A `bool`. Whether to denoise the sample to from `sigma_min` to `0` at the end of sampling.\n        return_inters: A `bool`. Whether to save intermediate results, i.e. the whole sampling trajectory.\n        AMED_predictor: A predictor network.\n        step_idx: A `int`. An index to specify the sampling step for training.\n        train: A `bool`. In the training loop?\n        max_order: A `int`. Maximum order of the solver. 1 <= max_order <= 4\n        buffer_model: A `list`. History model outputs.\n    Returns:\n        A pytorch tensor. A batch of generated samples or sampling trajectories if return_inters=True.\n    \"\"\"\n\n    assert max_order >= 1 and max_order <= 4\n    # Time step discretization.\n    t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho, net=net)\n\n    # Main sampling loop.\n    x_next = latents * t_steps[0]\n    inters = [x_next.unsqueeze(0)]\n    buffer_model = buffer_model if train else []\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):\n        x_cur = x_next\n        if AMED_predictor is not None:\n            unet_enc_out, hook = init_hook(net, class_labels)\n        \n        use_afs = (afs and len(buffer_model) == 0)\n        if use_afs:\n            d_cur = x_cur / ((1 + t_cur**2).sqrt())\n        else:\n            denoised = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            d_cur = (x_cur - denoised) / t_cur\n        \n        order = min(max_order, len(buffer_model)+1)\n        if AMED_predictor is not None:\n            hook.remove()\n            t_cur = t_cur.reshape(-1, 1, 1, 1)\n            t_next = t_next.reshape(-1, 1, 1, 1)\n            r, scale_dir, scale_time = get_amed_prediction(AMED_predictor, t_cur, t_next, net, unet_enc_out, use_afs, batch_size=latents.shape[0])\n            t_mid = (t_next**r) * (t_cur**(1-r))\n            if order == 1:      # First Euler step.\n                x_next = x_cur + (t_mid - t_cur) * d_cur\n            elif order == 2:    # Use one history point.\n                x_next = x_cur + (t_mid - t_cur) * (3 * d_cur - buffer_model[-1]) / 2\n            elif order == 3:    # Use two history points.\n                x_next = x_cur + (t_mid - t_cur) * (23 * d_cur - 16 * buffer_model[-1] + 5 * buffer_model[-2]) / 12\n            elif order == 4:    # Use three history points.\n                x_next = x_cur + (t_mid - t_cur) * (55 * d_cur - 59 * buffer_model[-1] + 37 * buffer_model[-2] - 9 * buffer_model[-3]) / 24\n        else:\n            if order == 1:      # First Euler step.\n                x_next = x_cur + (t_next - t_cur) * d_cur\n            elif order == 2:    # Use one history point.\n                x_next = x_cur + (t_next - t_cur) * (3 * d_cur - buffer_model[-1]) / 2\n            elif order == 3:    # Use two history points.\n                x_next = x_cur + (t_next - t_cur) * (23 * d_cur - 16 * buffer_model[-1] + 5 * buffer_model[-2]) / 12\n            elif order == 4:    # Use three history points.\n                x_next = x_cur + (t_next - t_cur) * (55 * d_cur - 59 * buffer_model[-1] + 37 * buffer_model[-2] - 9 * buffer_model[-3]) / 24\n        \n        if len(buffer_model) == max_order - 1:\n            for k in range(max_order - 2):\n                buffer_model[k] = buffer_model[k+1]\n            buffer_model[-1] = d_cur.detach()\n        else:\n            buffer_model.append(d_cur.detach())\n        \n        if AMED_predictor is not None:\n            order = min(max_order, len(buffer_model)+1)\n            denoised = get_denoised(net, x_next, scale_time * t_mid, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            d_cur = (x_next - denoised) / t_mid\n            if order == 1:      # First Euler step.\n                x_next = x_next + scale_dir * (t_next - t_mid) * d_cur\n            elif order == 2:    # Use one history point.\n                x_next = x_next + scale_dir * (t_next - t_mid) * (3 * d_cur - buffer_model[-1]) / 2\n            elif order == 3:    # Use two history points.\n                x_next = x_next + scale_dir * (t_next - t_mid) * (23 * d_cur - 16 * buffer_model[-1] + 5 * buffer_model[-2]) / 12\n            elif order == 4:    # Use three history points.\n                x_next = x_next + scale_dir * (t_next - t_mid) * (55 * d_cur - 59 * buffer_model[-1] + 37 * buffer_model[-2] - 9 * buffer_model[-3]) / 24\n            \n            if len(buffer_model) == max_order - 1:\n                for k in range(max_order - 2):\n                    buffer_model[k] = buffer_model[k+1]\n                buffer_model[-1] = d_cur.detach()\n            else:\n                buffer_model.append(d_cur.detach())\n                \n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n    \n    if denoise_to_zero:\n        x_next = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n\n    if return_inters:\n        return torch.cat(inters, dim=0).to(latents.device)\n    if train:\n        return x_next, buffer_model, [], r, scale_dir, scale_time\n    return x_next\n\n#----------------------------------------------------------------------------\ndef dpm_2_sampler(\n    net, \n    latents, \n    class_labels=None, \n    condition=None, \n    unconditional_condition=None,\n    num_steps=None, \n    sigma_min=0.002, \n    sigma_max=80, \n    schedule_type='polynomial',\n    schedule_rho=7, \n    afs=False, \n    denoise_to_zero=False, \n    return_inters=False, \n    AMED_predictor=None, \n    step_idx=None, \n    train=False, \n    r=0.5, \n    **kwargs\n):\n    \"\"\"\n    AMED-Plugin for DPM-Solver-2.\n\n    Args:\n        net: A wrapped diffusion model.\n        latents: A pytorch tensor. Input sample at time `sigma_max`.\n        class_labels: A pytorch tensor. The condition for conditional sampling or guided sampling.\n        condition: A pytorch tensor. The condition to the model used in LDM and Stable Diffusion\n        unconditional_condition: A pytorch tensor. The unconditional condition to the model used in LDM and Stable Diffusion\n        num_steps: A `int`. The total number of the time steps with `num_steps-1` spacings. \n        sigma_min: A `float`. The ending sigma during samping.\n        sigma_max: A `float`. The starting sigma during sampling.\n        schedule_type: A `str`. The type of time schedule. We support three types:\n            - 'polynomial': polynomial time schedule. (Recommended in EDM.)\n            - 'logsnr': uniform logSNR time schedule. (Recommended in DPM-Solver for small-resolution datasets.)\n            - 'time_uniform': uniform time schedule. (Recommended in DPM-Solver for high-resolution datasets.)\n            - 'discrete': time schedule used in LDM. (Recommended when using pre-trained diffusion models from the LDM and Stable Diffusion codebases.)\n        schedule_rho: A `float`. Time step exponent. Need to be specified when schedule_type in ['polynomial', 'time_uniform'].\n        afs: A `bool`. Whether to use analytical first step (AFS) at the beginning of sampling.\n        denoise_to_zero: A `bool`. Whether to denoise the sample to from `sigma_min` to `0` at the end of sampling.\n        return_inters: A `bool`. Whether to save intermediate results, i.e. the whole sampling trajectory.\n        AMED_predictor: A predictor network.\n        step_idx: A `int`. An index to specify the sampling step for training.\n        train: A `bool`. In the training loop?\n        r: A `float`. The hyperparameter controlling the location of the intermediate time step. r=0.5 recovers the original DPM-Solver-2.\n    Returns:\n        A pytorch tensor. A batch of generated samples or sampling trajectories if return_inters=True.\n    \"\"\"\n\n    # Time step discretization.\n    t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho, net=net)\n    \n    # Main sampling loop.\n    x_next = latents * t_steps[0]\n    inters = [x_next.unsqueeze(0)]\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):                # 0, ..., N-1\n        x_cur = x_next\n        if AMED_predictor is not None:\n            unet_enc_out, hook = init_hook(net, class_labels)\n        \n        # Euler step.\n        use_afs = afs and (((not train) and i == 0) or (train and step_idx == 0))\n        if use_afs:\n            d_cur = x_cur / ((1 + t_cur**2).sqrt())\n        else:\n            denoised = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            d_cur = (x_cur - denoised) / t_cur\n\n        scale_time, scale_dir = 1, 1\n        if AMED_predictor is not None:\n            hook.remove()\n            t_cur = t_cur.reshape(-1, 1, 1, 1)\n            t_next = t_next.reshape(-1, 1, 1, 1)\n            r, scale_dir, scale_time = get_amed_prediction(AMED_predictor, t_cur, t_next, net, unet_enc_out, use_afs, batch_size=latents.shape[0])\n        t_mid = (t_next ** r) * (t_cur ** (1 - r))\n        x_next = x_cur + (t_mid - t_cur) * d_cur\n\n        # Apply 2nd order correction.\n        denoised = get_denoised(net, x_next, scale_time * t_mid, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        d_mid = (x_next - denoised) / t_mid\n        x_next = x_cur + scale_dir * (t_next - t_cur) * ((1 / (2 * r)) * d_mid + (1 - 1 / (2 * r)) * d_cur)\n    \n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n    if denoise_to_zero:\n        x_next = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n\n    if return_inters:\n        return torch.cat(inters, dim=0).to(latents.device)\n    if train:\n        return x_next, [], [], r, scale_dir, scale_time\n    return x_next\n\n#----------------------------------------------------------------------------\n\ndef dpm_pp_sampler(\n    net, \n    latents, \n    class_labels=None, \n    condition=None, \n    unconditional_condition=None,\n    num_steps=None, \n    sigma_min=0.002, \n    sigma_max=80, \n    schedule_type='polynomial', \n    schedule_rho=7, \n    afs=False, \n    denoise_to_zero=False, \n    return_inters=False, \n    AMED_predictor=None, \n    step_idx=None, \n    train=False, \n    buffer_model=[], \n    buffer_t=[], \n    max_order=3, \n    predict_x0=True, \n    lower_order_final=True,\n    **kwargs\n):\n    \"\"\"\n    AMED-Plugin for multistep DPM-Solver++. \n\n    Args:\n        net: A wrapped diffusion model.\n        latents: A pytorch tensor. Input sample at time `sigma_max`.\n        class_labels: A pytorch tensor. The condition for conditional sampling or guided sampling.\n        condition: A pytorch tensor. The condition to the model used in LDM and Stable Diffusion\n        unconditional_condition: A pytorch tensor. The unconditional condition to the model used in LDM and Stable Diffusion\n        num_steps: A `int`. The total number of the time steps with `num_steps-1` spacings. \n        sigma_min: A `float`. The ending sigma during samping.\n        sigma_max: A `float`. The starting sigma during sampling.\n        schedule_type: A `str`. The type of time schedule. We support three types:\n            - 'polynomial': polynomial time schedule. (Recommended in EDM.)\n            - 'logsnr': uniform logSNR time schedule. (Recommended in DPM-Solver for small-resolution datasets.)\n            - 'time_uniform': uniform time schedule. (Recommended in DPM-Solver for high-resolution datasets.)\n            - 'discrete': time schedule used in LDM. (Recommended when using pre-trained diffusion models from the LDM and Stable Diffusion codebases.)\n        schedule_rho: A `float`. Time step exponent. Need to be specified when schedule_type in ['polynomial', 'time_uniform'].\n        afs: A `bool`. Whether to use analytical first step (AFS) at the beginning of sampling.\n        denoise_to_zero: A `bool`. Whether to denoise the sample to from `sigma_min` to `0` at the end of sampling.\n        return_inters: A `bool`. Whether to save intermediate results, i.e. the whole sampling trajectory.\n        AMED_predictor: A predictor network.\n        step_idx: A `int`. An index to specify the sampling step for training.\n        train: A `bool`. In the training loop?\n        buffer_model: A `list`. History model outputs.\n        buffer_t: A `list`. History time steps.\n        max_order: A `int`. Maximum order of the solver. 1 <= max_order <= 3\n        predict_x0: A `bool`. Whether to use the data prediction formulation. \n        lower_order_final: A `bool`. Whether to lower the order at the final stages of sampling. \n    Returns:\n        A pytorch tensor. The sample at time `sigma_min` or the whole sampling trajectory if return_inters=True.\n    \"\"\"\n\n    assert max_order >= 1 and max_order <= 3\n    latents = latents.to(dtype=torch.float32)\n    # Time step discretization.\n    t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho, net=net)\n\n    # Main sampling loop.\n    x_next = latents * t_steps[0]\n    inters = [x_next.unsqueeze(0)]\n    buffer_model = buffer_model if train else []\n    buffer_t = buffer_t if train else []\n    if AMED_predictor is not None:\n        num_steps = 2 * AMED_predictor.module.num_steps - 1 if train else 2 * num_steps - 1\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):                # 0, ..., N-1\n        x_cur = x_next\n        if AMED_predictor is not None:\n            step_cur = (2 * step_idx + 1 if train else 2 * i + 1)\n            unet_enc_out, hook = init_hook(net, class_labels)\n        else:\n            step_cur = i + 1\n        \n        use_afs = (afs and len(buffer_model) == 0)\n        if use_afs:\n            d_cur = x_cur / ((1 + t_cur**2).sqrt())\n            denoised = x_cur - t_cur * d_cur\n        else:\n            denoised = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            d_cur = (x_cur - denoised) / t_cur\n            \n        buffer_model.append(dynamic_thresholding_fn(denoised)) if predict_x0 else buffer_model.append(d_cur)\n        if AMED_predictor is not None:\n            hook.remove()\n            t_cur = t_cur.reshape(-1, 1, 1, 1)\n            t_next = t_next.reshape(-1, 1, 1, 1)\n            r, scale_dir, scale_time = get_amed_prediction(AMED_predictor, t_cur, t_next, net, unet_enc_out, use_afs, batch_size=latents.shape[0])\n            t_mid = (t_next**r) * (t_cur**(1-r))\n        buffer_t.append(t_cur)\n        \n        t_next_temp = t_mid if AMED_predictor is not None else t_next\n        if lower_order_final:\n            order = step_cur if step_cur < max_order else min(max_order, num_steps - step_cur)\n        else:\n            order = min(max_order, step_cur)\n        x_next = dpm_pp_update(x_cur, buffer_model, buffer_t, t_next_temp, order, predict_x0=predict_x0)\n            \n        # One more step for step instruction:\n        if AMED_predictor is not None:\n            step_cur = step_cur + 1\n            denoised = get_denoised(net, x_next, scale_time * t_mid, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            model_out = dynamic_thresholding_fn(denoised) if predict_x0 else ((x_next - denoised) / t_mid)\n            buffer_model.append(model_out)\n            buffer_t.append(t_mid)\n            \n            if lower_order_final:\n                order = step_cur if step_cur < max_order else min(max_order, num_steps - step_cur)\n            else:\n                order = min(step_cur, max_order)\n            x_next = dpm_pp_update(x_next, buffer_model, buffer_t, t_next, order, predict_x0=predict_x0, scale=scale_dir)\n            \n        if len(buffer_model) >= 3:\n            buffer_model = [a.detach() for a in buffer_model[-3:]]\n            buffer_t = [a.detach() for a in buffer_t[-3:]]\n        else:\n            buffer_model = [a.detach() for a in buffer_model]\n            buffer_t = [a.detach() for a in buffer_t]\n        \n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n            \n    if denoise_to_zero:\n        x_next = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n            \n    if return_inters:\n        return torch.cat(inters, dim=0).to(latents.device)\n    if train:\n        return x_next, buffer_model, buffer_t, r, scale_dir, scale_time\n    return x_next\n\n#----------------------------------------------------------------------------\ndef heun_sampler(\n    net, \n    latents, \n    class_labels=None, \n    condition=None, \n    unconditional_condition=None,\n    num_steps=None, \n    sigma_min=0.002, \n    sigma_max=80, \n    schedule_type='time_uniform', \n    schedule_rho=1, \n    afs=False, \n    denoise_to_zero=False, \n    return_inters=False, \n    **kwargs\n):\n    \"\"\"\n    Heun's second sampler. Introduced in EDM: https://arxiv.org/abs/2206.00364.\n\n    Args:\n        net: A wrapped diffusion model.\n        latents: A pytorch tensor. Input sample at time `sigma_max`.\n        class_labels: A pytorch tensor. The condition for conditional sampling or guided sampling.\n        condition: A pytorch tensor. The condition to the model used in LDM and Stable Diffusion\n        unconditional_condition: A pytorch tensor. The unconditional condition to the model used in LDM and Stable Diffusion\n        num_steps: A `int`. The total number of the time steps with `num_steps-1` spacings. \n        sigma_min: A `float`. The ending sigma during samping.\n        sigma_max: A `float`. The starting sigma during sampling.\n        schedule_type: A `str`. The type of time schedule. We support three types:\n            - 'polynomial': polynomial time schedule. (Recommended in EDM.)\n            - 'logsnr': uniform logSNR time schedule. (Recommended in DPM-Solver for small-resolution datasets.)\n            - 'time_uniform': uniform time schedule. (Recommended in DPM-Solver for high-resolution datasets.)\n            - 'discrete': time schedule used in LDM. (Recommended when using pre-trained diffusion models from the LDM and Stable Diffusion codebases.)\n        schedule_rho: A `float`. Time step exponent. Need to be specified when schedule_type in ['polynomial', 'time_uniform'].\n        afs: A `bool`. Whether to use analytical first step (AFS) at the beginning of sampling.\n        denoise_to_zero: A `bool`. Whether to denoise the sample to from `sigma_min` to `0` at the end of sampling.\n        return_inters: A `bool`. Whether to save intermediate results, i.e. the whole sampling trajectory.\n    Returns:\n        A pytorch tensor. A batch of generated samples or sampling trajectories if return_inters=True.\n    \"\"\"\n\n    # Time step discretization.\n    t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho)\n    assert t_steps.dim() == 1, f\"t_steps should be 1D, got {t_steps.shape}\"\n    # Main sampling loop.\n    x_next = latents * t_steps[0]\n    inters = [x_next.unsqueeze(0)]\n    batch_size = latents.shape[0]\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):                # 0, ..., N-1\n        x_cur = x_next\n\n        # Expand t_cur and t_next to [batch_size]\n        t_cur = t_cur.expand(batch_size)\n        t_next = t_next.expand(batch_size)\n\n        # Reshape t_cur and t_next for broadcasting in division\n        t_cur_broadcast = t_cur.view(batch_size, 1, 1, 1)  # [8, 1, 1, 1]\n        t_next_broadcast = t_next.view(batch_size, 1, 1, 1)  # [8, 1, 1, 1]\n        #print(f\"t_cur shape: {t_cur.shape}, t_cur_broadcast shape: {t_cur_broadcast.shape}\")\n\n        # Euler step.\n        use_afs = (afs and i == 0)\n        if use_afs:\n            d_cur = x_cur / ((1 + t_cur_broadcast**2).sqrt())\n        else:\n            model_output = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            denoised = model_output[:, :3]  # Extract noise prediction (first 3 channels)\n            #print(f\"x_cur shape: {x_cur.shape}, denoised shape: {denoised.shape}\")\n            d_cur = (x_cur - denoised) / t_cur_broadcast\n        x_next = x_cur + (t_next_broadcast - t_cur_broadcast) * d_cur\n\n        # Apply 2nd order correction.\n        model_output = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        denoised = model_output[:, :3]  # Extract noise prediction (first 3 channels)\n        d_prime = (x_next - denoised) / t_next_broadcast\n        x_next = x_cur + (t_next_broadcast - t_cur_broadcast) * (0.5 * d_cur + 0.5 * d_prime)\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n\n    if denoise_to_zero:\n        model_output = get_denoised(net, x_next, t_next.squeeze(1).squeeze(1).squeeze(1), class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        x_next = model_output[:, :3]  # Final denoised output\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n\n    if return_inters:\n        return torch.cat(inters, dim=0).to(latents.device)\n    return x_next\n'''\nwith open('/kaggle/working/p2w/solvers_amed.py', 'w') as f:\n    f.write(solvers_amed_code)\nprint(\"Successfully wrote solvers_amed.py to /kaggle/working/p2w/solvers_amed.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T06:11:15.096903Z","iopub.execute_input":"2025-03-27T06:11:15.097322Z","iopub.status.idle":"2025-03-27T06:11:15.112855Z","shell.execute_reply.started":"2025-03-27T06:11:15.097293Z","shell.execute_reply":"2025-03-27T06:11:15.111888Z"}},"outputs":[{"name":"stdout","text":"Successfully wrote solvers_amed.py to /kaggle/working/p2w/solvers_amed.py\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"import os\n\nold_filename = \"/kaggle/working/p2w/solvers_amed.py\"\nnew_filename = \"/kaggle/working/p2w/solvers_amed_old.py\"\n\nos.rename(old_filename, new_filename)\nprint(f\"Renamed {old_filename} to {new_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:40:22.337201Z","iopub.execute_input":"2025-03-27T04:40:22.337434Z","iopub.status.idle":"2025-03-27T04:40:22.355458Z","shell.execute_reply.started":"2025-03-27T04:40:22.337401Z","shell.execute_reply":"2025-03-27T04:40:22.354754Z"}},"outputs":[{"name":"stdout","text":"Renamed /kaggle/working/p2w/solvers_amed.py to /kaggle/working/p2w/solvers_amed_old.py\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"solver_utils='''\nimport torch\nimport numpy as np\n\n#----------------------------------------------------------------------------\n\ndef get_schedule(num_steps, sigma_min, sigma_max, device=None, schedule_type='polynomial', schedule_rho=7, net=None, diffusion=None):\n    \"\"\"\n    Get the time schedule for sampling.\n\n    Args:\n        num_steps: A `int`. The total number of the time steps with `num_steps-1` spacings. \n        sigma_min: A `float`. The ending sigma during samping.\n        sigma_max: A `float`. The starting sigma during sampling.\n        device: A torch device.\n        schedule_type: A `str`. The type of time schedule. We support three types:\n            - 'polynomial': polynomial time schedule. (Recommended in EDM.)\n            - 'logsnr': uniform logSNR time schedule. (Recommended in DPM-Solver for small-resolution datasets.)\n            - 'time_uniform': uniform time schedule. (Recommended in DPM-Solver for high-resolution datasets.)\n            - 'discrete': time schedule used in LDM. (Recommended when using pre-trained diffusion models from the LDM and Stable Diffusion codebases.)\n        schedule_type: A `float`. Time step exponent.\n        net: A pre-trained diffusion model. Required when schedule_type == 'discrete'.\n    Returns:\n        a PyTorch tensor with shape [num_steps].\n    \"\"\"\n    sigma_min = torch.tensor(sigma_min, device=device) if not isinstance(sigma_min, torch.Tensor) else sigma_min\n    sigma_max = torch.tensor(sigma_max, device=device) if not isinstance(sigma_max, torch.Tensor) else sigma_max\n    if schedule_type == 'polynomial':\n        step_indices = torch.arange(num_steps, device=device)\n        t_steps = (sigma_max ** (1 / schedule_rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / schedule_rho) - sigma_max ** (1 / schedule_rho))) ** schedule_rho\n    elif schedule_type == 'logsnr':\n        logsnr_max = -1 * torch.log(torch.tensor(sigma_min))\n        logsnr_min = -1 * torch.log(torch.tensor(sigma_max))\n        t_steps = torch.linspace(logsnr_min.item(), logsnr_max.item(), steps=num_steps, device=device)\n        t_steps = (-t_steps).exp()\n    elif schedule_type == 'time_uniform':\n        epsilon_s = 1e-3\n        # Use PyTorch operations for vp_sigma\n        vp_sigma = lambda beta_d, beta_min: lambda t: (torch.exp(torch.tensor(0.5 * beta_d * (t ** 2) + beta_min * t, device=device)) - 1) ** 0.5\n        vp_sigma_inv = lambda beta_d, beta_min: lambda sigma: ((beta_min ** 2 + 2 * beta_d * torch.log(sigma ** 2 + 1)).sqrt() - beta_min) / beta_d\n        step_indices = torch.arange(num_steps, device=device)\n        # Compute vp_beta_d and vp_beta_min using PyTorch\n        vp_beta_d = 2 * (torch.log(sigma_min ** 2 + 1) / epsilon_s - torch.log(sigma_max ** 2 + 1)) / (epsilon_s - 1)\n        vp_beta_min = torch.log(sigma_max ** 2 + 1) - 0.5 * vp_beta_d\n        t_steps_temp = (1 + step_indices / (num_steps - 1) * (epsilon_s ** (1 / schedule_rho) - 1)) ** schedule_rho\n        # Clamp t_steps_temp to avoid large values\n        t_steps_temp = torch.clamp(t_steps_temp, min=1e-6, max=1.0)\n        t_steps = vp_sigma(vp_beta_d.item(), vp_beta_min.item())(t_steps_temp)\n    elif schedule_type == 'discrete':\n        # assert net is not None\n        # t_steps_min = net.sigma_inv(torch.tensor(sigma_min, device=device))\n        # t_steps_max = net.sigma_inv(torch.tensor(sigma_max, device=device))\n        # step_indices = torch.arange(num_steps, device=device)\n        # t_steps_temp = (t_steps_max + step_indices / (num_steps - 1) * (t_steps_min ** (1 / schedule_rho) - t_steps_max)) ** schedule_rho\n        # t_steps = net.sigma(t_steps_temp)\n\n        # Check if the model has sigma and sigma_inv (e.g., for EDM or LDM models)\n        has_sigma_inv = net is not None and hasattr(net, 'sigma_inv') and callable(getattr(net, 'sigma_inv')) and hasattr(net, 'sigma') and callable(getattr(net, 'sigma'))\n    \n        if has_sigma_inv:\n            # Original discrete schedule for models with sigma_inv (e.g., EDM, LDM)\n            t_steps_min = net.sigma_inv(torch.tensor(sigma_min, device=device))\n            t_steps_max = net.sigma_inv(torch.tensor(sigma_max, device=device))\n            step_indices = torch.arange(num_steps, device=device)\n            t_steps_temp = (t_steps_max + step_indices / (num_steps - 1) * (t_steps_min ** (1 / schedule_rho) - t_steps_max)) ** schedule_rho\n            t_steps = net.sigma(t_steps_temp)\n        else:\n            # Modified discrete schedule for guided diffusion models\n            assert diffusion is not None, \"Diffusion object must be provided for discrete schedule with guided diffusion\"\n            assert hasattr(diffusion, 'alphas_cumprod'), \"Diffusion object must have alphas_cumprod for discrete schedule\"\n            alphas_cumprod = diffusion.alphas_cumprod  # Shape: [diffusion_steps]\n            print(f\"alphas_cumprod shape: {alphas_cumprod.shape}, device: {alphas_cumprod.device if isinstance(alphas_cumprod, torch.Tensor) else 'numpy'}\")\n            # Convert to torch tensor if it's a numpy array\n            if isinstance(alphas_cumprod, np.ndarray):\n                alphas_cumprod = torch.from_numpy(alphas_cumprod).to(device,dtype=torch.float32)\n            # Check for invalid values in alphas_cumprod\n            if (alphas_cumprod < 0).any() or (alphas_cumprod > 1).any():\n                raise ValueError(f\"alphas_cumprod contains invalid values: min {alphas_cumprod.min().item()}, max {alphas_cumprod.max().item()}\")\n            sigmas = torch.sqrt(1 - alphas_cumprod)  # Shape: [diffusion_steps]\n            # Check for NaN or Inf in sigmas\n            if torch.isnan(sigmas).any() or torch.isinf(sigmas).any():\n                raise ValueError(\"sigmas contains NaN or Inf values\")\n            print(f\"sigmas shape: {sigmas.shape}, device: {sigmas.device}, min: {sigmas.min().item()}, max: {sigmas.max().item()}\")\n    \n            # Ensure sigmas, sigma_min, and sigma_max are on the same device\n            sigmas = sigmas.to(device)\n            sigma_min = sigma_min.to(device)\n            sigma_max = sigma_max.to(device)\n            # Clamp sigma_min and sigma_max to the range of sigmas\n            sigma_min = torch.clamp(sigma_min, sigmas.min(), sigmas.max())\n            sigma_max = torch.clamp(sigma_max, sigmas.min(), sigmas.max())\n            print(f\"After clamping, sigma_min: {sigma_min.item()}, sigma_max: {sigma_max.item()}\")\n    \n            # Find the timesteps corresponding to sigma_min and sigma_max\n            timesteps = torch.arange(len(sigmas), device=device, dtype=torch.float32)  # [0, 1, ..., diffusion_steps-1]\n            sigma_indices = torch.arange(len(sigmas), device=device, dtype=torch.float32)\n            sigma_min_idx = torch.searchsorted(sigmas.flip(0), sigma_min, right=True).float()\n            sigma_max_idx = torch.searchsorted(sigmas.flip(0), sigma_max, right=True).float()\n            # Convert indices back to timesteps (since we flipped sigmas)\n            sigma_min_idx = (len(sigmas) - 1) - sigma_min_idx\n            sigma_max_idx = (len(sigmas) - 1) - sigma_max_idx\n            print(f\"sigma_min_idx: {sigma_min_idx.item()}, sigma_max_idx: {sigma_max_idx.item()}\")\n    \n            # Generate timesteps from sigma_max_idx to sigma_min_idx\n            step_indices = torch.arange(num_steps, device=device, dtype=torch.float32)\n            t_steps_temp = (sigma_max_idx + step_indices / (num_steps - 1) * (sigma_min_idx - sigma_max_idx)) ** schedule_rho\n            print(f\"t_steps_temp shape: {t_steps_temp.shape}, device: {t_steps_temp.device}, min: {t_steps_temp.min().item()}, max: {t_steps_temp.max().item()}\")\n    \n            # Map timesteps back to sigmas using interpolation\n            # Ensure t_steps_temp is within bounds\n            t_steps_temp = torch.clamp(t_steps_temp, 0, len(sigmas) - 1)\n            print(f\"After clamp, t_steps_temp min: {t_steps_temp.min().item()}, max: {t_steps_temp.max().item()}\")\n            # Interpolate sigmas at t_steps_temp\n            t_steps_temp_int = t_steps_temp.long()\n            t_steps_temp_frac = t_steps_temp - t_steps_temp_int.float()\n            print(f\"t_steps_temp_int shape: {t_steps_temp_int.shape}, device: {t_steps_temp_int.device}, min: {t_steps_temp_int.min().item()}, max: {t_steps_temp_int.max().item()}\")\n            print(f\"t_steps_temp_frac shape: {t_steps_temp_frac.shape}, device: {t_steps_temp_frac.device}, min: {t_steps_temp_frac.min().item()}, max: {t_steps_temp_frac.max().item()}\")\n            # Compute the next index, ensuring it doesn't exceed bounds\n            t_steps_temp_int_plus_1 = torch.clamp(t_steps_temp_int + 1, 0, len(sigmas) - 1)\n            # Compute the interpolated values\n            sigma_lower = sigmas[t_steps_temp_int]\n            sigma_upper = sigmas[t_steps_temp_int_plus_1]\n            # Only interpolate where t_steps_temp_int < len(sigmas) - 1\n            mask = t_steps_temp_int < torch.tensor(len(sigmas) - 1, device=t_steps_temp_int.device, dtype=torch.long)\n            print(f\"mask shape: {mask.shape}, device: {mask.device}\")\n            print(f\"sigma_lower shape: {sigma_lower.shape}, device: {sigma_lower.device}\")\n            print(f\"sigma_upper shape: {sigma_upper.shape}, device: {sigma_upper.device}\")\n            interpolated = sigma_lower + t_steps_temp_frac * (sigma_upper - sigma_lower)\n            t_steps = torch.where(mask, interpolated, sigma_lower)\n            print(f\"t_steps shape: {t_steps.shape}, device: {t_steps.device}, min: {t_steps.min().item()}, max: {t_steps.max().item()}\")\n        \n\n        \n    else:\n        raise ValueError(\"Got wrong schedule type {}\".format(schedule_type))\n    \n    return t_steps.to(device)\n\n\n# Copied from the DPM-Solver codebase (https://github.com/LuChengTHU/dpm-solver).\n# Different from the original codebase, we use the VE-SDE formulation for simplicity\n# while the official implementation uses the equivalent VP-SDE formulation. \n##############################\n### Utils for DPM-Solver++ ###\n##############################\n#----------------------------------------------------------------------------\n\ndef expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        v: a PyTorch tensor with shape [N].\n        dim: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]\n    \n#----------------------------------------------------------------------------\n\ndef dynamic_thresholding_fn(x0):\n    \"\"\"\n    The dynamic thresholding method\n    \"\"\"\n    dims = x0.dim()\n    p = 0.995\n    s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n    s = expand_dims(torch.maximum(s, 1. * torch.ones_like(s).to(s.device)), dims)\n    x0 = torch.clamp(x0, -s, s) / s\n    return x0\n\n#----------------------------------------------------------------------------\n\ndef dpm_pp_update(x, model_prev_list, t_prev_list, t, order, predict_x0=True, scale=1):\n    if order == 1:\n        return dpm_solver_first_update(x, t_prev_list[-1], t, model_s=model_prev_list[-1], predict_x0=predict_x0, scale=scale)\n    elif order == 2:\n        return multistep_dpm_solver_second_update(x, model_prev_list, t_prev_list, t, predict_x0=predict_x0, scale=scale)\n    elif order == 3:\n        return multistep_dpm_solver_third_update(x, model_prev_list, t_prev_list, t, predict_x0=predict_x0, scale=scale)\n    else:\n        raise ValueError(\"Solver order must be 1 or 2 or 3, got {}\".format(order))\n\n#----------------------------------------------------------------------------\n\ndef dpm_solver_first_update(x, s, t, model_s=None, predict_x0=True, scale=1):\n    s, t = s.reshape(-1, 1, 1, 1), t.reshape(-1, 1, 1, 1)\n    lambda_s, lambda_t = -1 * s.log(), -1 * t.log()\n    h = lambda_t - lambda_s\n\n    phi_1 = torch.expm1(-h) if predict_x0 else torch.expm1(h)\n    if predict_x0:\n        x_t = (t / s) * x - scale * phi_1 * model_s\n    else:\n        x_t = x - scale * t * phi_1 * model_s\n    return x_t\n\n#----------------------------------------------------------------------------\n\ndef multistep_dpm_solver_second_update(x, model_prev_list, t_prev_list, t, predict_x0=True, scale=1):\n    t = t.reshape(-1, 1, 1, 1)\n    model_prev_1, model_prev_0 = model_prev_list[-2], model_prev_list[-1]\n    t_prev_1, t_prev_0 = t_prev_list[-2].reshape(-1, 1, 1, 1), t_prev_list[-1].reshape(-1, 1, 1, 1)\n    lambda_prev_1, lambda_prev_0, lambda_t = -1 * t_prev_1.log(), -1 * t_prev_0.log(), -1 * t.log()\n\n    h_0 = lambda_prev_0 - lambda_prev_1\n    h = lambda_t - lambda_prev_0\n    r0 = h_0 / h\n    D1_0 = (1. / r0) * (model_prev_0 - model_prev_1)\n    phi_1 = torch.expm1(-h) if predict_x0 else torch.expm1(h)\n    if predict_x0:\n        x_t = (t / t_prev_0) * x - scale * (phi_1 * model_prev_0 + 0.5 * phi_1 * D1_0)\n    else:\n        x_t = x - scale * (t * phi_1 * model_prev_0 + 0.5 * t * phi_1 * D1_0)\n    return x_t\n\n#----------------------------------------------------------------------------\n\ndef multistep_dpm_solver_third_update(x, model_prev_list, t_prev_list, t, predict_x0=True, scale=1):\n    \n    t = t.reshape(-1, 1, 1, 1)\n    model_prev_2, model_prev_1, model_prev_0 = model_prev_list[-3], model_prev_list[-2], model_prev_list[-1]\n    \n    t_prev_2, t_prev_1, t_prev_0 = t_prev_list[-3], t_prev_list[-2], t_prev_list[-1]\n    t_prev_2, t_prev_1, t_prev_0 = t_prev_2.reshape(-1, 1, 1, 1), t_prev_1.reshape(-1, 1, 1, 1), t_prev_0.reshape(-1, 1, 1, 1)\n    lambda_prev_2, lambda_prev_1, lambda_prev_0, lambda_t = -1 * t_prev_2.log(), -1 * t_prev_1.log(), -1 * t_prev_0.log(), -1 * t.log()\n\n    h_1 = lambda_prev_1 - lambda_prev_2\n    h_0 = lambda_prev_0 - lambda_prev_1\n    h = lambda_t - lambda_prev_0\n    r0, r1 = h_0 / h, h_1 / h\n    D1_0 = (1. / r0) * (model_prev_0 - model_prev_1)\n    D1_1 = (1. / r1) * (model_prev_1 - model_prev_2)\n    D1 = D1_0 + (r0 / (r0 + r1)) * (D1_0 - D1_1)\n    D2 = (1. / (r0 + r1)) * (D1_0 - D1_1)\n    \n    phi_1 = torch.expm1(-h) if predict_x0 else torch.expm1(h)\n    phi_2 = phi_1 / h + 1. if predict_x0 else phi_1 / h - 1.\n    phi_3 = phi_2 / h - 0.5\n    if predict_x0:\n        x_t = (t / t_prev_0) * x - scale * (phi_1 * model_prev_0 - phi_2 * D1 + phi_3 * D2)\n    else:\n        x_t =  x - scale * (t * phi_1 * model_prev_0 + t * phi_2 * D1 + t * phi_3 * D2)\n    return x_t\n'''\n\nwith open('/kaggle/working/p2w/solver_utils_amed.py', 'w') as f:\n    f.write(solver_utils)\nprint(\"Successfully wrote solver_utils_amed.py to /kaggle/working/p2w/solver_utils_amed.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T06:17:02.862441Z","iopub.execute_input":"2025-03-27T06:17:02.862811Z","iopub.status.idle":"2025-03-27T06:17:02.871557Z","shell.execute_reply.started":"2025-03-27T06:17:02.862783Z","shell.execute_reply":"2025-03-27T06:17:02.870724Z"}},"outputs":[{"name":"stdout","text":"Successfully wrote solver_utils_amed.py to /kaggle/working/p2w/solver_utils_amed.py\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"#sigma_min=diffusion.sqrt_one_minus_alphas_cumprod[-1],\n#sigma_max=diffusion.betas[0],","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:40:22.375281Z","iopub.execute_input":"2025-03-27T04:40:22.375498Z","iopub.status.idle":"2025-03-27T04:40:22.391295Z","shell.execute_reply.started":"2025-03-27T04:40:22.375475Z","shell.execute_reply":"2025-03-27T04:40:22.390596Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"solvers_amedv2='''\nimport torch\nfrom solver_utils_amed import *\n\n#----------------------------------------------------------------------------\n# Initialize the hook function to get the U-Net bottleneck outputs\n\ndef init_hook(net, class_labels=None):\n    unet_enc_out = []\n    def hook_fn(module, input, output):\n        unet_enc_out.append(output.detach())\n    if hasattr(net, 'guidance_type'):                                       # models from LDM and Stable Diffusion\n        hook = net.model.model.diffusion_model.middle_block.register_forward_hook(hook_fn)\n    elif net.image_size == 256 or net.image_size==64:                                       # models from CM and ADM with resolution of 256\n        hook = net.middle_block.register_forward_hook(hook_fn)\n    else:                                                                   # models from EDM\n        module_name = '8x8_block2' if class_labels is not None else '8x8_block3'\n        hook = net.model.enc[module_name].register_forward_hook(hook_fn)\n    return unet_enc_out, hook\n\n#----------------------------------------------------------------------------\n\ndef get_amed_prediction(AMED_predictor, t_cur, t_next, net, unet_enc_out, use_afs, batch_size):\n    if hasattr(net, 'guidance_type') and net.guidance_type == 'classifier-free':\n        unet_enc = torch.mean(unet_enc_out[-1], dim=1) if not use_afs else torch.zeros((2*batch_size, 8, 8), device=t_cur.device)\n        output = AMED_predictor(unet_enc[batch_size:], t_cur, t_next)\n    else:\n        unet_enc = torch.mean(unet_enc_out[-1], dim=1) if not use_afs else torch.zeros((batch_size, 8, 8), device=t_cur.device)\n        output = AMED_predictor(unet_enc, t_cur, t_next)\n    output_list = [*output]\n    \n    if len(output_list) == 2:\n        try:\n            use_scale_time = AMED_predictor.module.scale_time\n        except:\n            use_scale_time = AMED_predictor.scale_time\n        if use_scale_time:\n            r, scale_time = output_list\n            r = r.reshape(-1, 1, 1, 1)\n            scale_time = scale_time.reshape(-1, 1, 1, 1)\n            scale_dir = torch.ones_like(scale_time)\n        else:\n            r, scale_dir = output_list\n            r = r.reshape(-1, 1, 1, 1)\n            scale_dir = scale_dir.reshape(-1, 1, 1, 1)\n            scale_time = torch.ones_like(scale_dir)\n    elif len(output_list) == 3:\n        r, scale_dir, scale_time = output_list\n        r = r.reshape(-1, 1, 1, 1)\n        scale_dir = scale_dir.reshape(-1, 1, 1, 1)\n        scale_time = scale_time.reshape(-1, 1, 1, 1)\n    else:\n        r = output.reshape(-1, 1, 1, 1)\n        scale_dir = torch.ones_like(r)\n        scale_time = torch.ones_like(r)\n    return r, scale_dir, scale_time\n\n#----------------------------------------------------------------------------\n# Get the denoised output from the pre-trained diffusion models.\n\ndef get_denoised(net, x, t, class_labels=None, condition=None, unconditional_condition=None):\n    if hasattr(net, 'guidance_type'):     # models from LDM and Stable Diffusion\n        denoised = net(x, t, condition=condition, unconditional_condition=unconditional_condition)\n    else:\n        denoised = net(x, t, y=class_labels) #NR:was class_labels=class_labels\n    return denoised\n\n#----------------------------------------------------------------------------\n\ndef amed_sampler(\n    net, \n    latents, \n    class_labels=None,\n    condition=None, \n    unconditional_condition=None,\n    num_steps=None, \n    sigma_min=0.01, \n    sigma_max=1, \n    schedule_type='time_uniform',\n    schedule_rho=1, \n    afs=False, \n    denoise_to_zero=False, \n    return_inters=False,\n    AMED_predictor=None, \n    step_idx=None, \n    train=False, \n    **kwargs\n):\n    \"\"\"\n    AMED-Solver (https://arxiv.org/abs/2312.00094).\n\n    Args:\n        net: A wrapped diffusion model.\n        latents: A pytorch tensor. Input sample at time `sigma_max`.\n        class_labels: A pytorch tensor. The condition for conditional sampling or guided sampling.\n        condition: A pytorch tensor. The condition to the model used in LDM and Stable Diffusion\n        unconditional_condition: A pytorch tensor. The unconditional condition to the model used in LDM and Stable Diffusion\n        num_steps: A `int`. The total number of the time steps with `num_steps-1` spacings. \n        sigma_min: A `float`. The ending sigma during samping.\n        sigma_max: A `float`. The starting sigma during sampling.\n        schedule_type: A `str`. The type of time schedule. We support three types:\n            - 'polynomial': polynomial time schedule. (Recommended in EDM.)\n            - 'logsnr': uniform logSNR time schedule. (Recommended in DPM-Solver for small-resolution datasets.)\n            - 'time_uniform': uniform time schedule. (Recommended in DPM-Solver for high-resolution datasets.)\n            - 'discrete': time schedule used in LDM. (Recommended when using pre-trained diffusion models from the LDM and Stable Diffusion codebases.)\n        schedule_rho: A `float`. Time step exponent. Need to be specified when schedule_type in ['polynomial', 'time_uniform'].\n        afs: A `bool`. Whether to use analytical first step (AFS) at the beginning of sampling.\n        denoise_to_zero: A `bool`. Whether to denoise the sample to from `sigma_min` to `0` at the end of sampling.\n        return_inters: A `bool`. Whether to save intermediate results, i.e. the whole sampling trajectory.\n        AMED_predictor: A predictor network.\n        step_idx: A `int`. An index to specify the sampling step for training.\n        train: A `bool`. In the training loop?\n    Returns:\n        A pytorch tensor. A batch of generated samples or sampling trajectories if return_inters=True.\n    \"\"\"\n    assert AMED_predictor is not None\n    #print(f\"[amed_sampler] sigma_max: {sigma_max}, sigma_min: {sigma_min}\")\n\n    # Extract mask and ground truth for inpainting\n    gt = kwargs.get('model_kwargs', {}).get('ref_img', None)\n    mask = kwargs.get('model_mask_kwargs', {}).get('ref_img', None)\n    is_inpainting = gt is not None and mask is not None\n\n    # Time step discretization.\n    t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho, net=net)\n    #print(f\"[amed_sampler] t_steps: {t_steps}\")\n    # Main sampling loop.\n    x_next = latents * t_steps[0]\n    inters = [x_next.unsqueeze(0)]\n    batch_size = latents.shape[0]\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):  # 0, ..., N-1\n        x_cur = x_next\n        unet_enc_out, hook = init_hook(net, class_labels)\n        \n        # Expand t_cur and t_next to [batch_size]\n        t_cur = t_cur.expand(batch_size)  # [8]\n        t_next = t_next.expand(batch_size)  # [8]\n\n        # Reshape for broadcasting in division\n        t_cur_broadcast = t_cur.view(batch_size, 1, 1, 1)  # [8, 1, 1, 1]\n        t_next_broadcast = t_next.view(batch_size, 1, 1, 1)  # [8, 1, 1, 1]\n\n        # Inpainting injection before denoising (mimics p_sample)\n        if is_inpainting:\n            if mask.shape[1] != x_cur.shape[1]:\n                mask = mask.expand(-1, x_cur.shape[1], -1, -1)\n            # Approximate alpha_cumprod based on sigma (t_cur / sigma_max)\n            alpha_cumprod = 1 - (t_cur_broadcast / sigma_max) ** 2  # Linear approximation\n            gt_weight = torch.sqrt(alpha_cumprod)\n            noise_weight = torch.sqrt(1 - alpha_cumprod)\n            weighed_gt = gt_weight * gt + noise_weight * torch.randn_like(gt)\n            x_cur = mask * weighed_gt + (1 - mask) * x_cur\n        \n        # Euler step.\n        use_afs = afs and (((not train) and i == 0) or (train and step_idx == 0))\n        if use_afs:\n            d_cur = x_cur / ((1 + t_cur_broadcast**2).sqrt())\n        else:\n            model_output = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            denoised = model_output[:, :3]  # Final denoised output\n            d_cur = (x_cur - denoised) / t_cur_broadcast\n\n        hook.remove()\n        r, scale_dir, scale_time = get_amed_prediction(AMED_predictor, t_cur_broadcast, t_next_broadcast, net, unet_enc_out, use_afs, batch_size=latents.shape[0])\n        t_mid = (t_next_broadcast ** r) * (t_cur_broadcast ** (1 - r))  # [8, 1, 1, 1]\n        # Compute a 1D version of t_mid for get_denoised\n        t_mid_1d = t_mid.view(batch_size)  # [8]\n        x_next = x_cur + (t_mid - t_cur_broadcast) * d_cur\n\n        # Apply 2nd order correction.\n        # Use t_mid_1d (1D) for get_denoised, ignoring scale_time as it should be t_mid directly\n        model_output = get_denoised(net, x_next, t_mid_1d, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        denoised = model_output[:, :3]  # Final denoised output\n        d_mid = (x_next - denoised) / t_mid\n        x_next = x_cur + scale_dir * (t_next_broadcast - t_cur_broadcast) * d_mid\n\n        # Blend with ground truth to preserve unmasked regions\n        if is_inpainting:\n            if mask.shape[1] != x_next.shape[1]:  # If mask is single-channel\n                mask = mask.expand(-1, x_next.shape[1], -1, -1)\n            x_next = mask * gt + (1 - mask) * x_next\n    \n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n        \n    if denoise_to_zero:\n        # Use t_next (1D) for get_denoised, matching heun_sampler\n        model_output = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        x_next = model_output[:, :3]  # Final denoised output\n\n        if is_inpainting:\n            if mask.shape[1] != x_next.shape[1]:  # If mask is single-channel\n                mask = mask.expand(-1, x_next.shape[1], -1, -1)\n            x_next = mask * gt + (1 - mask) * x_next\n\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n\n    if return_inters:\n        return torch.cat(inters, dim=0).to(latents.device)\n    if train:\n        return x_next, [], [], r, scale_dir, scale_time\n    return x_next\n    # assert AMED_predictor is not None\n\n    # # Time step discretization.\n    # t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho, net=net)\n    \n    # # Main sampling loop.\n    # x_next = latents * t_steps[0]\n    # inters = [x_next.unsqueeze(0)]\n    # batch_size = latents.shape[0]\n    # for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):                # 0, ..., N-1\n    #     x_cur = x_next\n    #     unet_enc_out, hook = init_hook(net, class_labels)\n\n    #     # Expand t_cur and t_next to [batch_size]\n    #     t_cur = t_cur.expand(batch_size)  # [8]\n    #     t_next = t_next.expand(batch_size)  # [8]\n\n    #     # Reshape for broadcasting in division\n    #     t_cur_broadcast = t_cur.view(batch_size, 1, 1, 1)  # [8, 1, 1, 1]\n    #     t_next_broadcast = t_next.view(batch_size, 1, 1, 1)  # [8, 1, 1, 1]\n        \n    #     # Euler step.\n    #     use_afs = afs and (((not train) and i == 0) or (train and step_idx == 0))\n    #     if use_afs:\n    #         d_cur = x_cur / ((1 + t_cur_broadcast**2).sqrt())\n    #     else:\n    #         model_output = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n    #         denoised = model_output[:, :3]  # Final denoised output\n    #         d_cur = (x_cur - denoised) / t_cur_broadcast\n\n    #     hook.remove()\n    #     r, scale_dir, scale_time = get_amed_prediction(AMED_predictor, t_cur_broadcast, t_next_broadcast, net, unet_enc_out, use_afs, batch_size=latents.shape[0])\n    #     t_mid = (t_next_broadcast ** r) * (t_cur_broadcast ** (1 - r))\n    #     x_next = x_cur + (t_mid - t_cur_broadcast) * d_cur\n        \n\n    #     # Apply 2nd order correction.\n    #     t_mid_flat = t_mid.view(-1)  # [8]\n    #     model_output = get_denoised(net, x_next, t_mid_flat, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n    #     denoised = model_output[:, :3]  # Final denoised output\n    #     d_mid = (x_next - denoised) / t_mid\n    #     x_next = x_cur + scale_dir * (t_next_broadcast - t_cur_broadcast) * d_mid\n    \n    #     if return_inters:\n    #         inters.append(x_next.unsqueeze(0))\n        \n    # if denoise_to_zero:\n    #     # Flatten t_next for get_denoised\n    #     t_next_flat = t_next.view(-1)  # [8]\n    #     model_output = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n    #     x_next = model_output[:, :3]  # Final denoised output\n    #     if return_inters:\n    #         inters.append(x_next.unsqueeze(0))\n\n    # if return_inters:\n    #     return torch.cat(inters, dim=0).to(latents.device)\n    # if train:\n    #     return x_next, [], [], r, scale_dir, scale_time\n    # return x_next\n\n#----------------------------------------------------------------------------\ndef euler_sampler(\n    net, \n    latents, \n    class_labels=None, \n    condition=None, \n    unconditional_condition=None,\n    num_steps=None, \n    sigma_min=0.002, \n    sigma_max=80, \n    schedule_type='polynomial',\n    schedule_rho=7, \n    afs=False, \n    denoise_to_zero=False, \n    return_inters=False, \n    AMED_predictor=None, \n    step_idx=None, \n    train=False, \n    **kwargs\n):  \n    \"\"\"\n    AMED-Plugin for Euler sampler.\n\n    Args:\n        net: A wrapped diffusion model.\n        latents: A pytorch tensor. Input sample at time `sigma_max`.\n        class_labels: A pytorch tensor. The condition for conditional sampling or guided sampling.\n        condition: A pytorch tensor. The condition to the model used in LDM and Stable Diffusion\n        unconditional_condition: A pytorch tensor. The unconditional condition to the model used in LDM and Stable Diffusion\n        num_steps: A `int`. The total number of the time steps with `num_steps-1` spacings. \n        sigma_min: A `float`. The ending sigma during samping.\n        sigma_max: A `float`. The starting sigma during sampling.\n        schedule_type: A `str`. The type of time schedule. We support three types:\n            - 'polynomial': polynomial time schedule. (Recommended in EDM.)\n            - 'logsnr': uniform logSNR time schedule. (Recommended in DPM-Solver for small-resolution datasets.)\n            - 'time_uniform': uniform time schedule. (Recommended in DPM-Solver for high-resolution datasets.)\n            - 'discrete': time schedule used in LDM. (Recommended when using pre-trained diffusion models from the LDM and Stable Diffusion codebases.)\n        schedule_rho: A `float`. Time step exponent. Need to be specified when schedule_type in ['polynomial', 'time_uniform'].\n        afs: A `bool`. Whether to use analytical first step (AFS) at the beginning of sampling.\n        denoise_to_zero: A `bool`. Whether to denoise the sample to from `sigma_min` to `0` at the end of sampling.\n        return_inters: A `bool`. Whether to save intermediate results, i.e. the whole sampling trajectory.\n        AMED_predictor: A predictor network.\n        step_idx: A `int`. An index to specify the sampling step for training.\n        train: A `bool`. In the training loop?\n    Returns:\n        A pytorch tensor. A batch of generated samples or sampling trajectories if return_inters=True.\n    \"\"\"\n\n    # Time step discretization.\n    t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho, net=net)\n\n    # Main sampling loop.\n    x_next = latents * t_steps[0]\n    inters = [x_next.unsqueeze(0)]\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):                # 0, ..., N-1\n        x_cur = x_next\n        if AMED_predictor is not None:\n            unet_enc_out, hook = init_hook(net, class_labels)\n\n        # Euler step.\n        use_afs = afs and (((not train) and i == 0) or (train and step_idx == 0))\n        if use_afs:\n            d_cur = x_cur / ((1 + t_cur**2).sqrt())\n        else:\n            denoised = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            d_cur = (x_cur - denoised) / t_cur\n            \n        if AMED_predictor is not None:\n            hook.remove()\n            t_cur = t_cur.reshape(-1, 1, 1, 1)\n            t_next = t_next.reshape(-1, 1, 1, 1)\n            r, scale_dir, scale_time = get_amed_prediction(AMED_predictor, t_cur, t_next, net, unet_enc_out, use_afs, batch_size=latents.shape[0])\n            t_mid = (t_next**r) * (t_cur**(1-r))\n            x_next = x_cur + (t_mid - t_cur) * d_cur\n        else:\n            x_next = x_cur + (t_next - t_cur) * d_cur\n        \n        # One more step for student\n        if AMED_predictor is not None:\n            denoised = get_denoised(net, x_next, scale_time * t_mid, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            d_mid = (x_next - denoised) / t_mid\n            x_next = x_next + scale_dir * (t_next - t_mid) * d_mid\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n    \n    if denoise_to_zero:\n        x_next = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n\n    if return_inters:\n        return torch.cat(inters, dim=0).to(latents.device)\n    if train:\n        return x_next, [], [], r, scale_dir, scale_time\n    return x_next\n\n\n#----------------------------------------------------------------------------\n\ndef ipndm_sampler(\n    net, \n    latents, \n    class_labels=None, \n    condition=None, \n    unconditional_condition=None,\n    num_steps=None, \n    sigma_min=0.002, \n    sigma_max=80, \n    schedule_type='polynomial',\n    schedule_rho=7, \n    afs=False,\n    denoise_to_zero=False, \n    return_inters=False, \n    AMED_predictor=None, \n    train=False, \n    max_order=4, \n    buffer_model=[], \n    **kwargs\n):\n    \"\"\"\n    AMED-Plugin for improved PNDM sampler.\n\n    Args:\n        net: A wrapped diffusion model.\n        latents: A pytorch tensor. Input sample at time `sigma_max`.\n        class_labels: A pytorch tensor. The condition for conditional sampling or guided sampling.\n        condition: A pytorch tensor. The condition to the model used in LDM and Stable Diffusion\n        unconditional_condition: A pytorch tensor. The unconditional condition to the model used in LDM and Stable Diffusion\n        num_steps: A `int`. The total number of the time steps with `num_steps-1` spacings. \n        sigma_min: A `float`. The ending sigma during samping.\n        sigma_max: A `float`. The starting sigma during sampling.\n        schedule_type: A `str`. The type of time schedule. We support three types:\n            - 'polynomial': polynomial time schedule. (Recommended in EDM.)\n            - 'logsnr': uniform logSNR time schedule. (Recommended in DPM-Solver for small-resolution datasets.)\n            - 'time_uniform': uniform time schedule. (Recommended in DPM-Solver for high-resolution datasets.)\n            - 'discrete': time schedule used in LDM. (Recommended when using pre-trained diffusion models from the LDM and Stable Diffusion codebases.)\n        schedule_rho: A `float`. Time step exponent. Need to be specified when schedule_type in ['polynomial', 'time_uniform'].\n        afs: A `bool`. Whether to use analytical first step (AFS) at the beginning of sampling.\n        denoise_to_zero: A `bool`. Whether to denoise the sample to from `sigma_min` to `0` at the end of sampling.\n        return_inters: A `bool`. Whether to save intermediate results, i.e. the whole sampling trajectory.\n        AMED_predictor: A predictor network.\n        step_idx: A `int`. An index to specify the sampling step for training.\n        train: A `bool`. In the training loop?\n        max_order: A `int`. Maximum order of the solver. 1 <= max_order <= 4\n        buffer_model: A `list`. History model outputs.\n    Returns:\n        A pytorch tensor. A batch of generated samples or sampling trajectories if return_inters=True.\n    \"\"\"\n\n    assert max_order >= 1 and max_order <= 4\n    # Time step discretization.\n    t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho, net=net)\n\n    # Main sampling loop.\n    x_next = latents * t_steps[0]\n    inters = [x_next.unsqueeze(0)]\n    buffer_model = buffer_model if train else []\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):\n        x_cur = x_next\n        if AMED_predictor is not None:\n            unet_enc_out, hook = init_hook(net, class_labels)\n        \n        use_afs = (afs and len(buffer_model) == 0)\n        if use_afs:\n            d_cur = x_cur / ((1 + t_cur**2).sqrt())\n        else:\n            denoised = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            d_cur = (x_cur - denoised) / t_cur\n        \n        order = min(max_order, len(buffer_model)+1)\n        if AMED_predictor is not None:\n            hook.remove()\n            t_cur = t_cur.reshape(-1, 1, 1, 1)\n            t_next = t_next.reshape(-1, 1, 1, 1)\n            r, scale_dir, scale_time = get_amed_prediction(AMED_predictor, t_cur, t_next, net, unet_enc_out, use_afs, batch_size=latents.shape[0])\n            t_mid = (t_next**r) * (t_cur**(1-r))\n            if order == 1:      # First Euler step.\n                x_next = x_cur + (t_mid - t_cur) * d_cur\n            elif order == 2:    # Use one history point.\n                x_next = x_cur + (t_mid - t_cur) * (3 * d_cur - buffer_model[-1]) / 2\n            elif order == 3:    # Use two history points.\n                x_next = x_cur + (t_mid - t_cur) * (23 * d_cur - 16 * buffer_model[-1] + 5 * buffer_model[-2]) / 12\n            elif order == 4:    # Use three history points.\n                x_next = x_cur + (t_mid - t_cur) * (55 * d_cur - 59 * buffer_model[-1] + 37 * buffer_model[-2] - 9 * buffer_model[-3]) / 24\n        else:\n            if order == 1:      # First Euler step.\n                x_next = x_cur + (t_next - t_cur) * d_cur\n            elif order == 2:    # Use one history point.\n                x_next = x_cur + (t_next - t_cur) * (3 * d_cur - buffer_model[-1]) / 2\n            elif order == 3:    # Use two history points.\n                x_next = x_cur + (t_next - t_cur) * (23 * d_cur - 16 * buffer_model[-1] + 5 * buffer_model[-2]) / 12\n            elif order == 4:    # Use three history points.\n                x_next = x_cur + (t_next - t_cur) * (55 * d_cur - 59 * buffer_model[-1] + 37 * buffer_model[-2] - 9 * buffer_model[-3]) / 24\n        \n        if len(buffer_model) == max_order - 1:\n            for k in range(max_order - 2):\n                buffer_model[k] = buffer_model[k+1]\n            buffer_model[-1] = d_cur.detach()\n        else:\n            buffer_model.append(d_cur.detach())\n        \n        if AMED_predictor is not None:\n            order = min(max_order, len(buffer_model)+1)\n            denoised = get_denoised(net, x_next, scale_time * t_mid, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            d_cur = (x_next - denoised) / t_mid\n            if order == 1:      # First Euler step.\n                x_next = x_next + scale_dir * (t_next - t_mid) * d_cur\n            elif order == 2:    # Use one history point.\n                x_next = x_next + scale_dir * (t_next - t_mid) * (3 * d_cur - buffer_model[-1]) / 2\n            elif order == 3:    # Use two history points.\n                x_next = x_next + scale_dir * (t_next - t_mid) * (23 * d_cur - 16 * buffer_model[-1] + 5 * buffer_model[-2]) / 12\n            elif order == 4:    # Use three history points.\n                x_next = x_next + scale_dir * (t_next - t_mid) * (55 * d_cur - 59 * buffer_model[-1] + 37 * buffer_model[-2] - 9 * buffer_model[-3]) / 24\n            \n            if len(buffer_model) == max_order - 1:\n                for k in range(max_order - 2):\n                    buffer_model[k] = buffer_model[k+1]\n                buffer_model[-1] = d_cur.detach()\n            else:\n                buffer_model.append(d_cur.detach())\n                \n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n    \n    if denoise_to_zero:\n        x_next = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n\n    if return_inters:\n        return torch.cat(inters, dim=0).to(latents.device)\n    if train:\n        return x_next, buffer_model, [], r, scale_dir, scale_time\n    return x_next\n\n#----------------------------------------------------------------------------\ndef dpm_2_sampler(\n    net, \n    latents, \n    class_labels=None, \n    condition=None, \n    unconditional_condition=None,\n    num_steps=None, \n    sigma_min=0.002, \n    sigma_max=80, \n    schedule_type='polynomial',\n    schedule_rho=7, \n    afs=False, \n    denoise_to_zero=False, \n    return_inters=False, \n    AMED_predictor=None, \n    step_idx=None, \n    train=False, \n    r=0.5, \n    **kwargs\n):\n    \"\"\"\n    AMED-Plugin for DPM-Solver-2.\n\n    Args:\n        net: A wrapped diffusion model.\n        latents: A pytorch tensor. Input sample at time `sigma_max`.\n        class_labels: A pytorch tensor. The condition for conditional sampling or guided sampling.\n        condition: A pytorch tensor. The condition to the model used in LDM and Stable Diffusion\n        unconditional_condition: A pytorch tensor. The unconditional condition to the model used in LDM and Stable Diffusion\n        num_steps: A `int`. The total number of the time steps with `num_steps-1` spacings. \n        sigma_min: A `float`. The ending sigma during samping.\n        sigma_max: A `float`. The starting sigma during sampling.\n        schedule_type: A `str`. The type of time schedule. We support three types:\n            - 'polynomial': polynomial time schedule. (Recommended in EDM.)\n            - 'logsnr': uniform logSNR time schedule. (Recommended in DPM-Solver for small-resolution datasets.)\n            - 'time_uniform': uniform time schedule. (Recommended in DPM-Solver for high-resolution datasets.)\n            - 'discrete': time schedule used in LDM. (Recommended when using pre-trained diffusion models from the LDM and Stable Diffusion codebases.)\n        schedule_rho: A `float`. Time step exponent. Need to be specified when schedule_type in ['polynomial', 'time_uniform'].\n        afs: A `bool`. Whether to use analytical first step (AFS) at the beginning of sampling.\n        denoise_to_zero: A `bool`. Whether to denoise the sample to from `sigma_min` to `0` at the end of sampling.\n        return_inters: A `bool`. Whether to save intermediate results, i.e. the whole sampling trajectory.\n        AMED_predictor: A predictor network.\n        step_idx: A `int`. An index to specify the sampling step for training.\n        train: A `bool`. In the training loop?\n        r: A `float`. The hyperparameter controlling the location of the intermediate time step. r=0.5 recovers the original DPM-Solver-2.\n    Returns:\n        A pytorch tensor. A batch of generated samples or sampling trajectories if return_inters=True.\n    \"\"\"\n\n    # Time step discretization.\n    t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho, net=net)\n    \n    # Main sampling loop.\n    x_next = latents * t_steps[0]\n    inters = [x_next.unsqueeze(0)]\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):                # 0, ..., N-1\n        x_cur = x_next\n        if AMED_predictor is not None:\n            unet_enc_out, hook = init_hook(net, class_labels)\n        \n        # Euler step.\n        use_afs = afs and (((not train) and i == 0) or (train and step_idx == 0))\n        if use_afs:\n            d_cur = x_cur / ((1 + t_cur**2).sqrt())\n        else:\n            denoised = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            d_cur = (x_cur - denoised) / t_cur\n\n        scale_time, scale_dir = 1, 1\n        if AMED_predictor is not None:\n            hook.remove()\n            t_cur = t_cur.reshape(-1, 1, 1, 1)\n            t_next = t_next.reshape(-1, 1, 1, 1)\n            r, scale_dir, scale_time = get_amed_prediction(AMED_predictor, t_cur, t_next, net, unet_enc_out, use_afs, batch_size=latents.shape[0])\n        t_mid = (t_next ** r) * (t_cur ** (1 - r))\n        x_next = x_cur + (t_mid - t_cur) * d_cur\n\n        # Apply 2nd order correction.\n        denoised = get_denoised(net, x_next, scale_time * t_mid, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        d_mid = (x_next - denoised) / t_mid\n        x_next = x_cur + scale_dir * (t_next - t_cur) * ((1 / (2 * r)) * d_mid + (1 - 1 / (2 * r)) * d_cur)\n    \n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n    if denoise_to_zero:\n        x_next = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n\n    if return_inters:\n        return torch.cat(inters, dim=0).to(latents.device)\n    if train:\n        return x_next, [], [], r, scale_dir, scale_time\n    return x_next\n\n#----------------------------------------------------------------------------\n\ndef dpm_pp_sampler(\n    net, \n    latents, \n    class_labels=None, \n    condition=None, \n    unconditional_condition=None,\n    num_steps=None, \n    sigma_min=0.002, \n    sigma_max=80, \n    schedule_type='polynomial', \n    schedule_rho=7, \n    afs=False, \n    denoise_to_zero=False, \n    return_inters=False, \n    AMED_predictor=None, \n    step_idx=None, \n    train=False, \n    buffer_model=[], \n    buffer_t=[], \n    max_order=3, \n    predict_x0=True, \n    lower_order_final=True,\n    diffusion=None,\n    **kwargs\n):\n    \"\"\"\n    AMED-Plugin for multistep DPM-Solver++. \n\n    Args:\n        net: A wrapped diffusion model.\n        latents: A pytorch tensor. Input sample at time `sigma_max`.\n        class_labels: A pytorch tensor. The condition for conditional sampling or guided sampling.\n        condition: A pytorch tensor. The condition to the model used in LDM and Stable Diffusion\n        unconditional_condition: A pytorch tensor. The unconditional condition to the model used in LDM and Stable Diffusion\n        num_steps: A `int`. The total number of the time steps with `num_steps-1` spacings. \n        sigma_min: A `float`. The ending sigma during samping.\n        sigma_max: A `float`. The starting sigma during sampling.\n        schedule_type: A `str`. The type of time schedule. We support three types:\n            - 'polynomial': polynomial time schedule. (Recommended in EDM.)\n            - 'logsnr': uniform logSNR time schedule. (Recommended in DPM-Solver for small-resolution datasets.)\n            - 'time_uniform': uniform time schedule. (Recommended in DPM-Solver for high-resolution datasets.)\n            - 'discrete': time schedule used in LDM. (Recommended when using pre-trained diffusion models from the LDM and Stable Diffusion codebases.)\n        schedule_rho: A `float`. Time step exponent. Need to be specified when schedule_type in ['polynomial', 'time_uniform'].\n        afs: A `bool`. Whether to use analytical first step (AFS) at the beginning of sampling.\n        denoise_to_zero: A `bool`. Whether to denoise the sample to from `sigma_min` to `0` at the end of sampling.\n        return_inters: A `bool`. Whether to save intermediate results, i.e. the whole sampling trajectory.\n        AMED_predictor: A predictor network.\n        step_idx: A `int`. An index to specify the sampling step for training.\n        train: A `bool`. In the training loop?\n        buffer_model: A `list`. History model outputs.\n        buffer_t: A `list`. History time steps.\n        max_order: A `int`. Maximum order of the solver. 1 <= max_order <= 3\n        predict_x0: A `bool`. Whether to use the data prediction formulation. \n        lower_order_final: A `bool`. Whether to lower the order at the final stages of sampling. \n    Returns:\n        A pytorch tensor. The sample at time `sigma_min` or the whole sampling trajectory if return_inters=True.\n    \"\"\"\n\n    assert max_order >= 1 and max_order <= 3\n    latents = latents.to(dtype=torch.float32)\n    # Time step discretization.\n    t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho, net=net, diffusion=diffusion)\n\n    # Main sampling loop.\n    x_next = latents * t_steps[0]\n    inters = [x_next.unsqueeze(0)]\n    buffer_model = buffer_model if train else []\n    buffer_t = buffer_t if train else []\n    batch_size = latents.shape[0]\n    if AMED_predictor is not None:\n        num_steps = 2 * AMED_predictor.module.num_steps - 1 if train else 2 * num_steps - 1\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):                # 0, ..., N-1\n        x_cur = x_next\n        if AMED_predictor is not None:\n            step_cur = (2 * step_idx + 1 if train else 2 * i + 1)\n            unet_enc_out, hook = init_hook(net, class_labels)\n        else:\n            step_cur = i + 1\n        \n        # Expand t_cur and t_next to [batch_size] for get_denoised\n        t_cur = t_cur.expand(batch_size)\n        t_next = t_next.expand(batch_size)\n        # Reshape for broadcasting in computations\n        t_cur_broadcast = t_cur.view(batch_size, 1, 1, 1)\n        t_next_broadcast = t_next.view(batch_size, 1, 1, 1)\n\n        use_afs = (afs and len(buffer_model) == 0)\n        if use_afs:\n            d_cur = x_cur / ((1 + t_cur_broadcast**2).sqrt())\n            denoised = x_cur - t_cur_broadcast * d_cur\n        else:\n            model_output = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            denoised = model_output[:, :3]  # Extract the first 3 channels (mean prediction)\n            d_cur = (x_cur - denoised) / t_cur_broadcast\n        \n        buffer_model.append(dynamic_thresholding_fn(denoised)) if predict_x0 else buffer_model.append(d_cur)\n        if AMED_predictor is not None:\n            hook.remove()\n            r, scale_dir, scale_time = get_amed_prediction(AMED_predictor, t_cur_broadcast, t_next_broadcast, net, unet_enc_out, use_afs, batch_size=latents.shape[0])\n            t_mid = (t_next_broadcast**r) * (t_cur_broadcast**(1-r))\n            # Create a broadcasted version of t_mid for calculations\n            t_mid_broadcast = t_mid  # Already [batch_size, 1, 1, 1]\n        # Store the 1D version in buffer_t to match the format expected by dpm_pp_update\n        buffer_t.append(t_cur)\n        \n        t_next_temp = t_mid_broadcast if AMED_predictor is not None else t_next_broadcast\n        if lower_order_final:\n            order = step_cur if step_cur < max_order else min(max_order, num_steps - step_cur)\n        else:\n            order = min(max_order, step_cur)\n        x_next = dpm_pp_update(x_cur, buffer_model, buffer_t, t_next_temp, order, predict_x0=predict_x0)\n            \n        # One more step for step instruction:\n        if AMED_predictor is not None:\n            step_cur = step_cur + 1\n            # Compute a 1D version of t_mid for get_denoised\n            t_mid_1d = (scale_time * t_mid).view(batch_size)\n            model_output = get_denoised(net, x_next, t_mid_1d, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            denoised = model_output[:, :3]  # Extract the first 3 channels (mean prediction)\n            model_out = dynamic_thresholding_fn(denoised) if predict_x0 else ((x_next - denoised) / t_mid_broadcast)\n            buffer_model.append(model_out)\n            # Store the 1D version of t_mid in buffer_t\n            buffer_t.append(t_mid_1d)\n            \n            if lower_order_final:\n                order = step_cur if step_cur < max_order else min(max_order, num_steps - step_cur)\n            else:\n                order = min(step_cur, max_order)\n            x_next = dpm_pp_update(x_next, buffer_model, buffer_t, t_next_broadcast, order, predict_x0=predict_x0, scale=scale_dir)\n            \n        if len(buffer_model) >= 3:\n            buffer_model = [a.detach() for a in buffer_model[-3:]]\n            buffer_t = [a.detach() for a in buffer_t[-3:]]\n        else:\n            buffer_model = [a.detach() for a in buffer_model]\n            buffer_t = [a.detach() for a in buffer_t]\n        \n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n            \n    if denoise_to_zero:\n        model_output = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        x_next = model_output[:, :3]  # Extract the first 3 channels (mean prediction)\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n            \n    if return_inters:\n        return torch.cat(inters, dim=0).to(latents.device)\n    if train:\n        return x_next, buffer_model, buffer_t, r, scale_dir, scale_time\n    return x_next\n            \n    \n\n#----------------------------------------------------------------------------\ndef heun_sampler(\n    net, \n    latents, \n    class_labels=None, \n    condition=None, \n    unconditional_condition=None,\n    num_steps=None, \n    sigma_min=0.002, \n    sigma_max=80, \n    schedule_type='time_uniform', \n    schedule_rho=1, \n    afs=False, \n    denoise_to_zero=False, \n    return_inters=False, \n    **kwargs\n):\n    \"\"\"\n    Heun's second sampler. Introduced in EDM: https://arxiv.org/abs/2206.00364.\n\n    Args:\n        net: A wrapped diffusion model.\n        latents: A pytorch tensor. Input sample at time `sigma_max`.\n        class_labels: A pytorch tensor. The condition for conditional sampling or guided sampling.\n        condition: A pytorch tensor. The condition to the model used in LDM and Stable Diffusion\n        unconditional_condition: A pytorch tensor. The unconditional condition to the model used in LDM and Stable Diffusion\n        num_steps: A `int`. The total number of the time steps with `num_steps-1` spacings. \n        sigma_min: A `float`. The ending sigma during samping.\n        sigma_max: A `float`. The starting sigma during sampling.\n        schedule_type: A `str`. The type of time schedule. We support three types:\n            - 'polynomial': polynomial time schedule. (Recommended in EDM.)\n            - 'logsnr': uniform logSNR time schedule. (Recommended in DPM-Solver for small-resolution datasets.)\n            - 'time_uniform': uniform time schedule. (Recommended in DPM-Solver for high-resolution datasets.)\n            - 'discrete': time schedule used in LDM. (Recommended when using pre-trained diffusion models from the LDM and Stable Diffusion codebases.)\n        schedule_rho: A `float`. Time step exponent. Need to be specified when schedule_type in ['polynomial', 'time_uniform'].\n        afs: A `bool`. Whether to use analytical first step (AFS) at the beginning of sampling.\n        denoise_to_zero: A `bool`. Whether to denoise the sample to from `sigma_min` to `0` at the end of sampling.\n        return_inters: A `bool`. Whether to save intermediate results, i.e. the whole sampling trajectory.\n    Returns:\n        A pytorch tensor. A batch of generated samples or sampling trajectories if return_inters=True.\n    \"\"\"\n\n    # Extract mask and ground truth for inpainting\n    gt = kwargs.get('model_kwargs', {}).get('ref_img', None)\n    mask = kwargs.get('model_mask_kwargs', {}).get('ref_img', None)\n    is_inpainting = gt is not None and mask is not None\n\n    # Time step discretization.\n    t_steps = get_schedule(num_steps, sigma_min, sigma_max, device=latents.device, schedule_type=schedule_type, schedule_rho=schedule_rho)\n    assert t_steps.dim() == 1, f\"t_steps should be 1D, got {t_steps.shape}\"\n    # Main sampling loop.\n    x_next = latents * t_steps[0]\n    inters = [x_next.unsqueeze(0)]\n    batch_size = latents.shape[0]\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):                # 0, ..., N-1\n        x_cur = x_next\n\n        # Expand t_cur and t_next to [batch_size]\n        t_cur = t_cur.expand(batch_size) #[8]\n        t_next = t_next.expand(batch_size)\n\n        # Reshape t_cur and t_next for broadcasting in division\n        t_cur_broadcast = t_cur.view(batch_size, 1, 1, 1)  # [8, 1, 1, 1]\n        t_next_broadcast = t_next.view(batch_size, 1, 1, 1)  # [8, 1, 1, 1]\n        #print(f\"t_cur shape: {t_cur.shape}, t_cur_broadcast shape: {t_cur_broadcast.shape}\")\n\n        # Euler step.\n        use_afs = (afs and i == 0)\n        if use_afs:\n            d_cur = x_cur / ((1 + t_cur_broadcast**2).sqrt())\n        else:\n            model_output = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n            denoised = model_output[:, :3]  # Extract noise prediction (first 3 channels)\n            #print(f\"x_cur shape: {x_cur.shape}, denoised shape: {denoised.shape}\")\n            d_cur = (x_cur - denoised) / t_cur_broadcast\n        x_next = x_cur + (t_next_broadcast - t_cur_broadcast) * d_cur\n\n        # Apply 2nd order correction.\n        model_output = get_denoised(net, x_next, t_next, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        denoised = model_output[:, :3]  # Extract noise prediction (first 3 channels)\n        d_prime = (x_next - denoised) / t_next_broadcast\n        x_next = x_cur + (t_next_broadcast - t_cur_broadcast) * (0.5 * d_cur + 0.5 * d_prime)\n\n        # Blend with ground truth to preserve unmasked regions\n        if is_inpainting:\n            x_next = mask * gt + (1 - mask) * x_next\n\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n\n    if denoise_to_zero:\n        model_output = get_denoised(net, x_next, t_next.squeeze(1).squeeze(1).squeeze(1), class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n        x_next = model_output[:, :3]  # Final denoised output\n\n        if is_inpainting:\n            if mask.shape[1] != x_next.shape[1]:  # If mask is single-channel\n                mask = mask.expand(-1, x_next.shape[1], -1, -1)\n            x_next = mask * gt + (1 - mask) * x_next\n\n        if return_inters:\n            inters.append(x_next.unsqueeze(0))\n\n    if return_inters:\n        return torch.cat(inters, dim=0).to(latents.device)\n    return x_next\n\n\n'''\nwith open('/kaggle/working/p2w/solvers_amed.py', 'w') as f:\n    f.write(solvers_amedv2)\nprint(\"Successfully wrote solvers_amed.py to /kaggle/working/p2w/solvers_amed.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T06:12:04.122805Z","iopub.execute_input":"2025-03-27T06:12:04.123151Z","iopub.status.idle":"2025-03-27T06:12:04.137443Z","shell.execute_reply.started":"2025-03-27T06:12:04.123128Z","shell.execute_reply":"2025-03-27T06:12:04.136741Z"}},"outputs":[{"name":"stdout","text":"Successfully wrote solvers_amed.py to /kaggle/working/p2w/solvers_amed.py\n","output_type":"stream"}],"execution_count":96},{"cell_type":"code","source":"training_loop_code = '''\"\"\"Main training loop.\"\"\"\nimport argparse\nimport os\nimport csv\nimport time\nimport copy\nimport json\nimport pickle\nimport numpy as np\nimport torch.nn.functional as F\nimport torch\nimport dnnlib\nimport random\nfrom torch import autocast\nfrom guided_diffusion import dist_util\nfrom guided_diffusion.script_util import create_model_and_diffusion, args_to_dict, model_and_diffusion_defaults\nfrom torch_utils import distributed as dist\nfrom torch_utils import training_stats\nfrom torch_utils import misc\nfrom ldm.util import instantiate_from_config\nfrom torch_utils.download_util import check_file_by_key\n\ndef training_loop(\n    run_dir             = '.',      # Output directory.\n    AMED_kwargs         = {},       # Options for AMED predictor.\n    loss_kwargs         = {},       # Options for loss function.\n    optimizer_kwargs    = {},       # Options for optimizer.\n    seed                = 0,        # Global random seed.\n    batch_size          = None,     # Total batch size for one training iteration.\n    batch_gpu           = None,     # Limit batch size per GPU, None = no limit.\n    total_kimg          = 20,       # Training duration, measured in thousands of training images.\n    kimg_per_tick       = 1,        # Interval of progress prints.\n    snapshot_ticks      = 1,        # How often to save network snapshots, None = disable.\n    state_dump_ticks    = 20,       # How often to dump training state, None = disable.\n    cudnn_benchmark     = True,     # Enable torch.backends.cudnn.benchmark?\n    dataset_name        = None,\n    prompt_path         = None,\n    guidance_type       = None,\n    guidance_rate       = 0.,\n    device              = torch.device('cuda'),\n    image_size          = 64,       # Add image_size as a parameter with default value\n    **kwargs,\n):\n    # Initialize.\n    start_time = time.time()\n    np.random.seed((seed * dist.get_world_size() + dist.get_rank()) % (1 << 31))\n    torch.manual_seed(np.random.randint(1 << 31))\n    torch.backends.cudnn.benchmark = cudnn_benchmark\n    torch.backends.cudnn.allow_tf32 = False\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n\n    # Select batch size per GPU.\n    batch_gpu_total = batch_size // dist.get_world_size()\n    if batch_gpu is None or batch_gpu > batch_gpu_total:\n        batch_gpu = batch_gpu_total\n    num_accumulation_rounds = batch_gpu_total // batch_gpu\n    assert batch_size == batch_gpu * num_accumulation_rounds * dist.get_world_size()\n\n    # Custom M2S LWDM defaults overriding model_and_diffusion_defaults()\n    custom_defaults = model_and_diffusion_defaults()\n    custom_defaults.update({\n        'attention_resolutions': \"16\",\n        'class_cond': False,\n        'diffusion_steps': 1000,\n        'dropout': 0.0,\n        'learn_sigma': True,\n        'noise_schedule': \"linear\",\n        'num_channels': 128,\n        'num_head_channels': 64,\n        'num_res_blocks': 1,\n        'resblock_updown': True,\n        'use_fp16': True,\n        'use_scale_shift_norm': True,\n        'rescale_learned_sigmas': True,\n        'p2_gamma': 1,\n        'p2_k': 1,\n        'image_size': image_size,  # Set image_size from args\n    })\n    # Load LWDM model based on args.image_size\n    #custom_defaults['image_size'] = args.image_size  # Set from command-line argument\n    args = argparse.Namespace(**custom_defaults)\n    dist.print0(f'Loading LWDM model ({args.image_size}x{args.image_size})...')\n    model, diffusion = create_model_and_diffusion(**args_to_dict(args, custom_defaults.keys()))\n    checkpoint = dist_util.load_state_dict(kwargs[\"checkpoint_path\"], map_location=\"cpu\")\n    model.load_state_dict(checkpoint if 'state_dict' not in checkpoint else checkpoint['state_dict'])\n    if args.use_fp16:\n        model.convert_to_fp16()\n    model.to(device)\n    \n    # Construct AMED predictor\n    dist.print0('Constructing AMED predictor...')\n    AMED_kwargs.update(img_resolution=args.image_size)  # Set resolution dynamically\n    predictor = dnnlib.util.construct_class_by_name(**AMED_kwargs).to(device)\n    predictor.train().requires_grad_(True)\n    \n    # Setup optimizer\n    dist.print0('Setting up optimizer...')\n    loss_kwargs.update(\n        num_steps=AMED_kwargs['num_steps'],\n        sampler_stu=AMED_kwargs['sampler_stu'],\n        sampler_tea=AMED_kwargs['sampler_tea'],\n        M=AMED_kwargs['M'],\n        schedule_type=AMED_kwargs['schedule_type'],\n        schedule_rho=AMED_kwargs['schedule_rho'],\n        afs=AMED_kwargs['afs'],\n        max_order=AMED_kwargs['max_order'],\n        sigma_min=0.002,\n        sigma_max=80,\n        predict_x0=AMED_kwargs['predict_x0'],\n        lower_order_final=AMED_kwargs['lower_order_final']\n    )\n    loss_fn = dnnlib.util.construct_class_by_name(**loss_kwargs)\n    \n    optimizer = dnnlib.util.construct_class_by_name(params=predictor.parameters(), **optimizer_kwargs)\n    ddp = torch.nn.parallel.DistributedDataParallel(predictor, device_ids=[device], broadcast_buffers=False)\n\n    # Train\n    dist.print0(f'Training for {total_kimg} kimg...')\n    cur_nimg = 0\n    cur_tick = 0\n    tick_start_nimg = cur_nimg\n    tick_start_time = time.time()\n    maintenance_time = tick_start_time - start_time\n    dist.update_progress(cur_nimg // 1000, total_kimg)\n    stats_jsonl = None\n    while True:\n        # Generate latents based on image size\n        latents = diffusion.q_sample(\n            torch.randn([batch_gpu, 3, args.image_size, args.image_size], device=device),\n            torch.tensor([diffusion.num_timesteps - 1], device=device)\n        )\n    \n        optimizer.zero_grad()\n        teacher_traj = loss_fn.get_teacher_traj(net=model, tensor_in=latents,diffusion=diffusion)\n        for step_idx in range(loss_fn.num_steps - 1):\n            loss, stu_out = loss_fn(\n                AMED_predictor=ddp,\n                net=model,\n                tensor_in=latents,\n                step_idx=step_idx,\n                teacher_out=teacher_traj[step_idx],\n                diffusion=diffusion\n            )\n            loss.sum().mul(1 / batch_gpu_total).backward()\n            torch.nn.utils.clip_grad_norm_(predictor.parameters(), max_norm=1.0)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            latents = stu_out if AMED_kwargs['sampler_stu'] not in ['euler', 'dpm', 'amed'] else teacher_traj[step_idx]\n    \n        cur_nimg += batch_size\n        if cur_nimg >= total_kimg * 1000 or (cur_tick % snapshot_ticks == 0 and cur_tick > 0):\n            if dist.get_rank() == 0:\n                with open(os.path.join(run_dir, f'predictor_{args.image_size}-{cur_nimg//1000:06d}.pkl'), 'wb') as f:\n                    pickle.dump({'model': predictor.cpu()}, f)\n            if cur_nimg >= total_kimg * 1000:\n                break\n            cur_tick += 1\n    \n    dist.print0('Exiting...')\n'''\nwith open('/kaggle/working/p2w/training_amed/training_loop.py', 'w') as f:\n    f.write(training_loop_code)\nprint(\"Successfully wrote training_loop.py to /kaggle/working/p2w/training_amed/training_loop.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:24:34.081082Z","iopub.execute_input":"2025-03-27T05:24:34.081385Z","iopub.status.idle":"2025-03-27T05:24:34.088234Z","shell.execute_reply.started":"2025-03-27T05:24:34.081363Z","shell.execute_reply":"2025-03-27T05:24:34.087114Z"}},"outputs":[{"name":"stdout","text":"Successfully wrote training_loop.py to /kaggle/working/p2w/training_amed/training_loop.py\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"train_amed = '''import argparse\nimport json\nimport os\nimport re\nimport sys\nimport torch\nfrom torch_utils import distributed as dist\nimport dnnlib\nfrom training_amed import training_loop\nfrom guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train AMED model')\n    parser.add_argument('--dataset_name', default='celebahq', help='Name of the dataset')\n    parser.add_argument('--batch', type=int, default=8, help='Batch size')\n    parser.add_argument('--total_kimg', type=int, default=10, help='Total training kilo-images')\n    parser.add_argument('--sampler_stu', default='amed', help='Student sampler')\n    parser.add_argument('--sampler_tea', default='heun', help='Teacher sampler')\n    parser.add_argument('--num_steps', type=int, default=4, help='Number of steps')\n    parser.add_argument('--M', type=int, default=1, help='M parameter')\n    parser.add_argument('--afs', type=bool, default=True, help='Use analytical first step')\n    parser.add_argument('--scale_dir', type=float, default=0.01, help='Scale direction')\n    parser.add_argument('--scale_time', type=float, default=0.0, help='Scale time')\n    parser.add_argument('--schedule_type', default='time_uniform', help='Schedule type')\n    parser.add_argument('--schedule_rho', type=float, default=1.0, help='Schedule rho')\n    parser.add_argument('--image_size', type=int, default=64, help='Image size')\n    parser.add_argument('--checkpoint_path', default='/kaggle/working/p2w/checkpoints/64x64.pt', help='Path to checkpoint')\n    parser.add_argument('--guidance_type', default=None, help='Guidance type')\n    parser.add_argument('--guidance_rate', type=float, default=0.0, help='Guidance rate')\n    parser.add_argument('--prompt_path', default=None, help='Prompt path')\n    parser.add_argument('--lr', type=float, default=0.005, help='Learning rate')\n    parser.add_argument('--batch_gpu', type=int, default=None, help='Batch size per GPU')\n    parser.add_argument('--bench', type=bool, default=True, help='Enable cudnn benchmark')\n    parser.add_argument('--max_order', type=int, default=3, help='Max order for solver')\n    parser.add_argument('--predict_x0', type=bool, default=True, help='Predict x0')\n    parser.add_argument('--lower_order_final', type=bool, default=True, help='Lower order at final steps')\n    parser.add_argument('--use_fp16', action='store_true', help='Use FP16 precision for training')\n    parser.add_argument('--seed', type=int, default=None, help='Random seed (optional)')\n    parser.add_argument('--nosubdir', action='store_true', help='Do not use subdirectory for run')\n    parser.add_argument('--outdir', default='./exps', help='Output directory')\n    parser.add_argument('--desc', default=None, help='Custom description string')\n    parser.add_argument('--dry_run', action='store_true', help='Perform a dry run without training')\n    \n    args = parser.parse_args()\n    return args\n\ndef main():\n    torch.multiprocessing.set_start_method('spawn')\n    dist.init()\n    args = parse_args()\n    \n    c = dnnlib.EasyDict()\n    c.loss_kwargs = dnnlib.EasyDict()\n    c.AMED_kwargs = dnnlib.EasyDict()\n    c.optimizer_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', lr=args.lr, betas=[0.9, 0.999], eps=1e-8)\n\n    c.AMED_kwargs.class_name = 'training_amed.networks.AMED_predictor'\n    c.AMED_kwargs.update(num_steps=args.num_steps, sampler_stu=args.sampler_stu, sampler_tea=args.sampler_tea,\n                         M=args.M, guidance_type=args.guidance_type, guidance_rate=args.guidance_rate,\n                         schedule_rho=args.schedule_rho, schedule_type=args.schedule_type, afs=args.afs,\n                         dataset_name=args.dataset_name, scale_dir=args.scale_dir, scale_time=args.scale_time,\n                         max_order=args.max_order, predict_x0=args.predict_x0, lower_order_final=args.lower_order_final)\n    c.loss_kwargs.class_name = 'training_amed.loss.AMED_loss'\n\n    c.total_kimg = args.total_kimg\n    c.kimg_per_tick = 1\n    c.snapshot_ticks = args.total_kimg\n    c.state_dump_ticks = args.total_kimg\n    c.update(dataset_name=args.dataset_name, batch_size=args.batch, batch_gpu=args.batch_gpu, gpus=dist.get_world_size(),\n             cudnn_benchmark=args.bench, use_fp16=args.use_fp16,image_size=args.image_size,checkpoint_path=args.checkpoint_path)\n    c.update(guidance_type=args.guidance_type, guidance_rate=args.guidance_rate, prompt_path=args.prompt_path)\n\n    if args.seed is not None:\n        c.seed = args.seed\n    else:\n        seed = torch.randint(1 << 31, size=[], device=torch.device('cuda'))\n        torch.distributed.broadcast(seed, src=0)\n        c.seed = int(seed)\n\n    if args.schedule_type == 'polynomial':\n        schedule_str = 'poly' + str(args.schedule_rho)\n    elif args.schedule_type == 'logsnr':\n        schedule_str = 'logsnr'\n    elif args.schedule_type == 'time_uniform':\n        schedule_str = 'uni' + str(args.schedule_rho)\n    elif args.schedule_type == 'discrete':\n        schedule_str = 'discrete'\n    else:\n        raise ValueError(f\"Got wrong schedule type: {args.schedule_type}\")\n    nfe = 2 * (args.num_steps - 1) - 1 if args.afs else 2 * (args.num_steps - 1)\n    nfe = 2 * nfe if args.dataset_name == 'ms_coco' else nfe\n    if args.afs:\n        desc_str = f'{args.dataset_name}-{args.num_steps}-{nfe}-{args.sampler_stu}-{args.sampler_tea}-{args.M}-{schedule_str}-afs'\n    else:\n        desc_str = f'{args.dataset_name}-{args.num_steps}-{nfe}-{args.sampler_stu}-{args.sampler_tea}-{args.M}-{schedule_str}'\n    if args.desc is not None:\n        desc_str += f'-{args.desc}'\n    c.desc = desc_str\n\n    if dist.get_rank() != 0:\n        c.run_dir = None\n    elif args.nosubdir:\n        c.run_dir = args.outdir\n    else:\n        prev_run_dirs = []\n        if os.path.isdir(args.outdir):\n            prev_run_dirs = [x for x in os.listdir(args.outdir) if os.path.isdir(os.path.join(args.outdir, x))]\n        prev_run_ids = [re.match(r'^\\d+', x) for x in prev_run_dirs]\n        prev_run_ids = [int(x.group()) for x in prev_run_ids if x is not None]\n        cur_run_id = max(prev_run_ids, default=-1) + 1\n        c.run_dir = os.path.join(args.outdir, f'{cur_run_id:05d}-{desc_str}')\n        assert not os.path.exists(c.run_dir)\n\n    dist.print0()\n    dist.print0('Training options:')\n    dist.print0(json.dumps(c, indent=2))\n    dist.print0()\n    dist.print0(f'Output directory:        {c.run_dir}')\n    dist.print0(f'Number of GPUs:          {dist.get_world_size()}')\n    dist.print0(f'Batch size:              {c.batch_size}')\n    dist.print0()\n\n    if args.dry_run:\n        dist.print0('Dry run; exiting.')\n        return\n\n    dist.print0('Creating output directory...')\n    if dist.get_rank() == 0:\n        os.makedirs(c.run_dir, exist_ok=True)\n        with open(os.path.join(c.run_dir, 'training_options.json'), 'wt') as f:\n            json.dump(c, f, indent=2)\n        dnnlib.util.Logger(file_name=os.path.join(c.run_dir, 'log.txt'), file_mode='a', should_flush=True)\n\n    training_loop.training_loop(**c)\n\nif __name__ == \"__main__\":\n    main()\n'''\n\nwith open('/kaggle/working/p2w/train_amed.py', 'w') as f:\n    f.write(train_amed)\nprint(\"Successfully wrote train_amed.py to /kaggle/working/p2w/train_amed.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:24:39.138059Z","iopub.execute_input":"2025-03-27T05:24:39.138370Z","iopub.status.idle":"2025-03-27T05:24:39.145140Z","shell.execute_reply.started":"2025-03-27T05:24:39.138346Z","shell.execute_reply":"2025-03-27T05:24:39.144072Z"}},"outputs":[{"name":"stdout","text":"Successfully wrote train_amed.py to /kaggle/working/p2w/train_amed.py\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"# !source /kaggle/working/miniconda/etc/profile.d/conda.sh && conda activate p2w && cd /kaggle/working/p2w && \\\n# CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 --master_port=11111 \\\n# train_amed.py \\\n#   --dataset_name celebahq \\\n#   --batch 64 \\\n#   --total_kimg 10 \\\n#   --sampler_stu dpmpp \\\n#   --sampler_tea dpmpp \\\n#   --num_steps 10 \\\n#   --afs True \\\n#   --scale_dir 0.01 \\\n#   --scale_time 0 \\\n#   --schedule_type discrete \\\n#   --max_order=3 --predict_x0=False --lower_order_final=True \\\n#   --schedule_rho 1 \\\n#   --image_size 64 \\\n#   --checkpoint_path '/kaggle/working/p2w/checkpoints/64x64.pt' \\\n#   --lr 0.005\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T06:22:11.163993Z","iopub.execute_input":"2025-03-27T06:22:11.164344Z","iopub.status.idle":"2025-03-27T06:26:34.802508Z","shell.execute_reply.started":"2025-03-27T06:22:11.164311Z","shell.execute_reply":"2025-03-27T06:26:34.801621Z"}},"outputs":[{"name":"stdout","text":"[2025-03-27 06:22:13,016] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\n\nTraining options:\n{\n  \"loss_kwargs\": {\n    \"class_name\": \"training_amed.loss.AMED_loss\"\n  },\n  \"AMED_kwargs\": {\n    \"class_name\": \"training_amed.networks.AMED_predictor\",\n    \"num_steps\": 10,\n    \"sampler_stu\": \"dpmpp\",\n    \"sampler_tea\": \"dpmpp\",\n    \"M\": 1,\n    \"guidance_type\": null,\n    \"guidance_rate\": 0.0,\n    \"schedule_rho\": 1.0,\n    \"schedule_type\": \"discrete\",\n    \"afs\": true,\n    \"dataset_name\": \"celebahq\",\n    \"scale_dir\": 0.01,\n    \"scale_time\": 0.0,\n    \"max_order\": 3,\n    \"predict_x0\": true,\n    \"lower_order_final\": true\n  },\n  \"optimizer_kwargs\": {\n    \"class_name\": \"torch.optim.Adam\",\n    \"lr\": 0.005,\n    \"betas\": [\n      0.9,\n      0.999\n    ],\n    \"eps\": 1e-08\n  },\n  \"total_kimg\": 10,\n  \"kimg_per_tick\": 1,\n  \"snapshot_ticks\": 10,\n  \"state_dump_ticks\": 10,\n  \"dataset_name\": \"celebahq\",\n  \"batch_size\": 64,\n  \"batch_gpu\": null,\n  \"gpus\": 1,\n  \"cudnn_benchmark\": true,\n  \"use_fp16\": false,\n  \"image_size\": 64,\n  \"checkpoint_path\": \"/kaggle/working/p2w/checkpoints/64x64.pt\",\n  \"guidance_type\": null,\n  \"guidance_rate\": 0.0,\n  \"prompt_path\": null,\n  \"seed\": 128690316,\n  \"desc\": \"celebahq-10-17-dpmpp-dpmpp-1-discrete-afs\",\n  \"run_dir\": \"./exps/00017-celebahq-10-17-dpmpp-dpmpp-1-discrete-afs\"\n}\n\nOutput directory:        ./exps/00017-celebahq-10-17-dpmpp-dpmpp-1-discrete-afs\nNumber of GPUs:          1\nBatch size:              64\n\nCreating output directory...\nLoading LWDM model (64x64)...\nConstructing AMED predictor...\nSetting up optimizer...\nTraining for 10 kimg...\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([10]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([10]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([10]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([10]), device: cuda:0\nsigma_lower shape: torch.Size([10]), device: cuda:0\nsigma_upper shape: torch.Size([10]), device: cuda:0\nt_steps shape: torch.Size([10]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([19]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([19]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([19]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([19]), device: cuda:0\nsigma_lower shape: torch.Size([19]), device: cuda:0\nsigma_upper shape: torch.Size([19]), device: cuda:0\nt_steps shape: torch.Size([19]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.35298073291778564, sigma_max: 0.010000829584896564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 0 | Loss:      nan | r (mean std): 0.5372 0.0000 | scale_dir (mean std): 0.9985 0.0000 | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.6333101391792297, sigma_max: 0.35298073291778564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 1 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.8245816230773926, sigma_max: 0.6333101391792297\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 2 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9308431148529053, sigma_max: 0.8245816230773926\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 3 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9780250787734985, sigma_max: 0.9308431148529053\nsigma_min_idx: -1.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nStep: 4 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9944677948951721, sigma_max: 0.9780250787734985\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 5 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9989079833030701, sigma_max: 0.9944677948951721\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 6 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9998317360877991, sigma_max: 0.9989079833030701\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 7 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9999797940254211, sigma_max: 0.9998317360877991\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 8 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([19]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([19]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([19]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([19]), device: cuda:0\nsigma_lower shape: torch.Size([19]), device: cuda:0\nsigma_upper shape: torch.Size([19]), device: cuda:0\nt_steps shape: torch.Size([19]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.35298073291778564, sigma_max: 0.010000829584896564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 0 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.6333101391792297, sigma_max: 0.35298073291778564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 1 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.8245816230773926, sigma_max: 0.6333101391792297\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 2 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9308431148529053, sigma_max: 0.8245816230773926\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 3 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9780250787734985, sigma_max: 0.9308431148529053\nsigma_min_idx: -1.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nStep: 4 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9944677948951721, sigma_max: 0.9780250787734985\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 5 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9989079833030701, sigma_max: 0.9944677948951721\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 6 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9998317360877991, sigma_max: 0.9989079833030701\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 7 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9999797940254211, sigma_max: 0.9998317360877991\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 8 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([19]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([19]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([19]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([19]), device: cuda:0\nsigma_lower shape: torch.Size([19]), device: cuda:0\nsigma_upper shape: torch.Size([19]), device: cuda:0\nt_steps shape: torch.Size([19]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.35298073291778564, sigma_max: 0.010000829584896564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 0 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.6333101391792297, sigma_max: 0.35298073291778564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 1 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.8245816230773926, sigma_max: 0.6333101391792297\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 2 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9308431148529053, sigma_max: 0.8245816230773926\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 3 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9780250787734985, sigma_max: 0.9308431148529053\nsigma_min_idx: -1.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nStep: 4 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9944677948951721, sigma_max: 0.9780250787734985\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 5 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9989079833030701, sigma_max: 0.9944677948951721\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 6 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9998317360877991, sigma_max: 0.9989079833030701\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 7 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9999797940254211, sigma_max: 0.9998317360877991\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 8 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([19]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([19]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([19]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([19]), device: cuda:0\nsigma_lower shape: torch.Size([19]), device: cuda:0\nsigma_upper shape: torch.Size([19]), device: cuda:0\nt_steps shape: torch.Size([19]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.35298073291778564, sigma_max: 0.010000829584896564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 0 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.6333101391792297, sigma_max: 0.35298073291778564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 1 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.8245816230773926, sigma_max: 0.6333101391792297\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 2 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9308431148529053, sigma_max: 0.8245816230773926\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 3 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9780250787734985, sigma_max: 0.9308431148529053\nsigma_min_idx: -1.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nStep: 4 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9944677948951721, sigma_max: 0.9780250787734985\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 5 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9989079833030701, sigma_max: 0.9944677948951721\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 6 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9998317360877991, sigma_max: 0.9989079833030701\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 7 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9999797940254211, sigma_max: 0.9998317360877991\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 8 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([19]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([19]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([19]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([19]), device: cuda:0\nsigma_lower shape: torch.Size([19]), device: cuda:0\nsigma_upper shape: torch.Size([19]), device: cuda:0\nt_steps shape: torch.Size([19]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.35298073291778564, sigma_max: 0.010000829584896564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 0 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.6333101391792297, sigma_max: 0.35298073291778564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 1 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.8245816230773926, sigma_max: 0.6333101391792297\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 2 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9308431148529053, sigma_max: 0.8245816230773926\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 3 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9780250787734985, sigma_max: 0.9308431148529053\nsigma_min_idx: -1.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nStep: 4 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9944677948951721, sigma_max: 0.9780250787734985\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 5 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9989079833030701, sigma_max: 0.9944677948951721\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 6 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9998317360877991, sigma_max: 0.9989079833030701\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 7 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9999797940254211, sigma_max: 0.9998317360877991\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 8 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([19]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([19]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([19]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([19]), device: cuda:0\nsigma_lower shape: torch.Size([19]), device: cuda:0\nsigma_upper shape: torch.Size([19]), device: cuda:0\nt_steps shape: torch.Size([19]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.35298073291778564, sigma_max: 0.010000829584896564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 0 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.6333101391792297, sigma_max: 0.35298073291778564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 1 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.8245816230773926, sigma_max: 0.6333101391792297\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 2 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9308431148529053, sigma_max: 0.8245816230773926\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 3 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9780250787734985, sigma_max: 0.9308431148529053\nsigma_min_idx: -1.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nStep: 4 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9944677948951721, sigma_max: 0.9780250787734985\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 5 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9989079833030701, sigma_max: 0.9944677948951721\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 6 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9998317360877991, sigma_max: 0.9989079833030701\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 7 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9999797940254211, sigma_max: 0.9998317360877991\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 8 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([19]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([19]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([19]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([19]), device: cuda:0\nsigma_lower shape: torch.Size([19]), device: cuda:0\nsigma_upper shape: torch.Size([19]), device: cuda:0\nt_steps shape: torch.Size([19]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.35298073291778564, sigma_max: 0.010000829584896564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 0 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.6333101391792297, sigma_max: 0.35298073291778564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 1 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.8245816230773926, sigma_max: 0.6333101391792297\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 2 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9308431148529053, sigma_max: 0.8245816230773926\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 3 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9780250787734985, sigma_max: 0.9308431148529053\nsigma_min_idx: -1.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nStep: 4 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9944677948951721, sigma_max: 0.9780250787734985\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 5 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9989079833030701, sigma_max: 0.9944677948951721\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 6 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9998317360877991, sigma_max: 0.9989079833030701\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 7 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9999797940254211, sigma_max: 0.9998317360877991\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 8 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([19]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([19]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([19]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([19]), device: cuda:0\nsigma_lower shape: torch.Size([19]), device: cuda:0\nsigma_upper shape: torch.Size([19]), device: cuda:0\nt_steps shape: torch.Size([19]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.35298073291778564, sigma_max: 0.010000829584896564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 0 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.6333101391792297, sigma_max: 0.35298073291778564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 1 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.8245816230773926, sigma_max: 0.6333101391792297\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 2 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9308431148529053, sigma_max: 0.8245816230773926\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 3 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9780250787734985, sigma_max: 0.9308431148529053\nsigma_min_idx: -1.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nStep: 4 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9944677948951721, sigma_max: 0.9780250787734985\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 5 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9989079833030701, sigma_max: 0.9944677948951721\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 6 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9998317360877991, sigma_max: 0.9989079833030701\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 7 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9999797940254211, sigma_max: 0.9998317360877991\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 8 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([19]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([19]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([19]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([19]), device: cuda:0\nsigma_lower shape: torch.Size([19]), device: cuda:0\nsigma_upper shape: torch.Size([19]), device: cuda:0\nt_steps shape: torch.Size([19]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.35298073291778564, sigma_max: 0.010000829584896564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 0 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.6333101391792297, sigma_max: 0.35298073291778564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 1 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.8245816230773926, sigma_max: 0.6333101391792297\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 2 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9308431148529053, sigma_max: 0.8245816230773926\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 3 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9780250787734985, sigma_max: 0.9308431148529053\nsigma_min_idx: -1.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nStep: 4 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9944677948951721, sigma_max: 0.9780250787734985\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 5 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9989079833030701, sigma_max: 0.9944677948951721\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 6 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9998317360877991, sigma_max: 0.9989079833030701\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 7 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9999797940254211, sigma_max: 0.9998317360877991\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 8 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([19]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([19]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([19]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([19]), device: cuda:0\nsigma_lower shape: torch.Size([19]), device: cuda:0\nsigma_upper shape: torch.Size([19]), device: cuda:0\nt_steps shape: torch.Size([19]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.35298073291778564, sigma_max: 0.010000829584896564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 0 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.6333101391792297, sigma_max: 0.35298073291778564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 1 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.8245816230773926, sigma_max: 0.6333101391792297\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 2 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9308431148529053, sigma_max: 0.8245816230773926\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 3 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9780250787734985, sigma_max: 0.9308431148529053\nsigma_min_idx: -1.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nStep: 4 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9944677948951721, sigma_max: 0.9780250787734985\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 5 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9989079833030701, sigma_max: 0.9944677948951721\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 6 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9998317360877991, sigma_max: 0.9989079833030701\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 7 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9999797940254211, sigma_max: 0.9998317360877991\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 8 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([19]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([19]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([19]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([19]), device: cuda:0\nsigma_lower shape: torch.Size([19]), device: cuda:0\nsigma_upper shape: torch.Size([19]), device: cuda:0\nt_steps shape: torch.Size([19]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.35298073291778564, sigma_max: 0.010000829584896564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 0 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.6333101391792297, sigma_max: 0.35298073291778564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 1 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.8245816230773926, sigma_max: 0.6333101391792297\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 2 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9308431148529053, sigma_max: 0.8245816230773926\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 3 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9780250787734985, sigma_max: 0.9308431148529053\nsigma_min_idx: -1.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nStep: 4 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9944677948951721, sigma_max: 0.9780250787734985\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 5 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9989079833030701, sigma_max: 0.9944677948951721\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 6 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9998317360877991, sigma_max: 0.9989079833030701\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 7 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9999797940254211, sigma_max: 0.9998317360877991\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 8 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([19]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([19]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([19]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([19]), device: cuda:0\nsigma_lower shape: torch.Size([19]), device: cuda:0\nsigma_upper shape: torch.Size([19]), device: cuda:0\nt_steps shape: torch.Size([19]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.35298073291778564, sigma_max: 0.010000829584896564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 0 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.6333101391792297, sigma_max: 0.35298073291778564\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 1 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.8245816230773926, sigma_max: 0.6333101391792297\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 2 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9308431148529053, sigma_max: 0.8245816230773926\nsigma_min_idx: 999.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: 999.0, max: 999.0\nAfter clamp, t_steps_temp min: 999.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 999, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.9999797940254211, max: 0.9999797940254211\nStep: 3 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9780250787734985, sigma_max: 0.9308431148529053\nsigma_min_idx: -1.0, sigma_max_idx: 999.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nStep: 4 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9944677948951721, sigma_max: 0.9780250787734985\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 5 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9989079833030701, sigma_max: 0.9944677948951721\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 6 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9998317360877991, sigma_max: 0.9989079833030701\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 7 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.9999797940254211, sigma_max: 0.9998317360877991\nsigma_min_idx: -1.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([2]), device: cuda:0, min: -1.0, max: -1.0\nAfter clamp, t_steps_temp min: 0.0, max: 0.0\nt_steps_temp_int shape: torch.Size([2]), device: cuda:0, min: 0, max: 0\nt_steps_temp_frac shape: torch.Size([2]), device: cuda:0, min: 0.0, max: 0.0\nmask shape: torch.Size([2]), device: cuda:0\nsigma_lower shape: torch.Size([2]), device: cuda:0\nsigma_upper shape: torch.Size([2]), device: cuda:0\nt_steps shape: torch.Size([2]), device: cuda:0, min: 0.010000829584896564, max: 0.010000829584896564\nStep: 8 | Loss:      nan | r (mean std):   nan   nan | scale_dir (mean std):   nan   nan | scale_time (mean std): 1.0000 0.0000\nalphas_cumprod shape: (1000,), device: numpy\nsigmas shape: torch.Size([1000]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\nAfter clamping, sigma_min: 0.010000829584896564, sigma_max: 0.9999797940254211\nsigma_min_idx: 999.0, sigma_max_idx: -1.0\nt_steps_temp shape: torch.Size([19]), device: cuda:0, min: -1.0, max: 999.0\nAfter clamp, t_steps_temp min: 0.0, max: 999.0\nt_steps_temp_int shape: torch.Size([19]), device: cuda:0, min: 0, max: 999\nt_steps_temp_frac shape: torch.Size([19]), device: cuda:0, min: 0.0, max: 0.888916015625\nmask shape: torch.Size([19]), device: cuda:0\nsigma_lower shape: torch.Size([19]), device: cuda:0\nsigma_upper shape: torch.Size([19]), device: cuda:0\nt_steps shape: torch.Size([19]), device: cuda:0, min: 0.010000829584896564, max: 0.9999797940254211\n^C\n[2025-03-27 06:26:34,097] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGINT death signal, shutting down workers\n[2025-03-27 06:26:34,097] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2961 closing signal SIGINT\nTraceback (most recent call last):\n  File \"/kaggle/working/p2w/train_amed.py\", line 138, in <module>\n    main()\n  File \"/kaggle/working/p2w/train_amed.py\", line 135, in main\n    training_loop.training_loop(**c)\n  File \"/kaggle/working/p2w/training_amed/training_loop.py\", line 135, in training_loop\n    teacher_traj = loss_fn.get_teacher_traj(net=model, tensor_in=latents,diffusion=diffusion)\n  File \"/kaggle/working/p2w/training_amed/loss.py\", line 86, in get_teacher_traj\n    teacher_traj = self.solver_tea(\n  File \"/kaggle/working/p2w/solvers_amed.py\", line 688, in dpm_pp_sampler\n    model_output = get_denoised(net, x_cur, t_cur, class_labels=class_labels, condition=condition, unconditional_condition=unconditional_condition)\n  File \"/kaggle/working/p2w/solvers_amed.py\", line 65, in get_denoised\n    denoised = net(x, t, y=class_labels) #NR:was class_labels=class_labels\n  File \"/kaggle/working/miniconda/envs/p2w/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/kaggle/working/miniconda/envs/p2w/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/kaggle/working/p2w/guided_diffusion/unet.py\", line 648, in forward\n    emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n  File \"/kaggle/working/p2w/guided_diffusion/nn.py\", line 114, in timestep_embedding\n    freqs = th.exp(\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":102},{"cell_type":"markdown","source":"bbbbbbbbbbbbbbbbbnnnnbbbbbbbbbbbbbbb","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"training for 256x256 ","metadata":{}},{"cell_type":"code","source":"# !source /kaggle/working/miniconda/etc/profile.d/conda.sh && conda activate p2w && cd /kaggle/working/p2w && \\\n# CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 --master_port=11111 \\\n# train_amed.py \\\n#   --dataset_name celebahq \\\n#   --batch 4 \\\n#   --total_kimg 10 \\\n#   --sampler_stu amed \\\n#   --sampler_tea heun \\\n#   --num_steps 50 \\\n#   --afs True \\\n#   --scale_dir 0.01 \\\n#   --scale_time 0 \\\n#   --schedule_type polynomial \\\n#   --schedule_rho 5 \\\n#   --image_size 256 \\\n#   --checkpoint_path '/kaggle/working/p2w/checkpoints/256x256.pt' \\\n#   --lr 0.0001\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:40:22.463770Z","iopub.execute_input":"2025-03-27T04:40:22.464010Z","iopub.status.idle":"2025-03-27T04:40:22.478701Z","shell.execute_reply.started":"2025-03-27T04:40:22.463987Z","shell.execute_reply":"2025-03-27T04:40:22.478066Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"For sampling only need olvers_amed, solvers_util and sample.py and pretrained weight and amed pretrained, after repo","metadata":{}},{"cell_type":"markdown","source":"not handled class_conditional(cg/cfg yet) line 349\nnot handled using fp_16","metadata":{}},{"cell_type":"code","source":"sample_amed='''\nimport os\nimport re\nimport csv\nimport argparse\nimport tqdm\nimport pickle\nimport numpy as np\nimport torch\nimport PIL.Image\nimport dnnlib\nfrom torch import autocast\nfrom torch_utils import distributed as dist\nfrom torchvision.utils import make_grid, save_image\nfrom torch_utils.download_util import check_file_by_key\nimport solvers_amed\nfrom guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n\n#----------------------------------------------------------------------------\n# Wrapper for torch.Generator that allows specifying a different random seed\n# for each sample in a minibatch.\n\nclass StackedRandomGenerator:\n    def __init__(self, device, seeds):\n        super().__init__()\n        self.generators = [torch.Generator(device).manual_seed(int(seed) % (1 << 32)) for seed in seeds]\n\n    def randn(self, size, **kwargs):\n        assert size[0] == len(self.generators)\n        return torch.stack([torch.randn(size[1:], generator=gen, **kwargs) for gen in self.generators])\n\n    def randn_like(self, input):\n        return self.randn(input.shape, dtype=input.dtype, layout=input.layout, device=input.device)\n\n    def randint(self, *args, size, **kwargs):\n        assert size[0] == len(self.generators)\n        return torch.stack([torch.randint(*args, size=size[1:], generator=gen, **kwargs) for gen in self.generators])\n\n#----------------------------------------------------------------------------\n# Parse a comma separated list of numbers or ranges and return a list of ints.\n# Example: '1,2,5-10' returns [1, 2, 5, 6, 7, 8, 9, 10]\n\ndef parse_int_list(s):\n    if isinstance(s, list): return s\n    ranges = []\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    for p in s.split(','):\n        m = range_re.match(p)\n        if m:\n            ranges.extend(range(int(m.group(1)), int(m.group(2))+1))\n        else:\n            ranges.append(int(p))\n    return ranges\n#----------------------------------------------------------------------------\n# Function to parse boolean arguments for argparse\n\ndef str2bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n#----------------------------------------------------------------------------\n# Load pre-trained models from the LDM codebase (https://github.com/CompVis/latent-diffusion) \n# and Stable Diffusion codebase (https://github.com/CompVis/stable-diffusion)\n\ndef load_ldm_model(config, ckpt, verbose=False):\n    from models.ldm.util import instantiate_from_config\n    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n    if \"global_step\" in pl_sd:\n        dist.print0(f\"Global Step: {pl_sd['global_step']}\")\n    sd = pl_sd[\"state_dict\"]\n    model = instantiate_from_config(config.model)\n    m, u = model.load_state_dict(sd, strict=False)\n    if len(m) > 0 and verbose:\n        print(\"missing keys:\")\n        print(m)\n    if len(u) > 0 and verbose:\n        print(\"unexpected keys:\")\n        print(u)\n    return model\n\n#----------------------------------------------------------------------------\n\ndef create_model(dataset_name=None, guidance_type=None, guidance_rate=None, device=None,image_size=64, model_path=None, classifier_path=None):\n    #model_path, classifier_path = check_file_by_key(dataset_name)\n    dist.print0(f'Loading the pre-trained diffusion model from \"{model_path}\"...')\n    if dataset_name == 'celebahq':\n        dist.print0(f'Loading pre-trained guided-diffusion model for CelebAHQ from \"{model_path}\"...')\n        # Use the same defaults as in training_loop.py\n        model_config = model_and_diffusion_defaults()\n        model_config.update({\n            'attention_resolutions': \"16\",\n            'class_cond': False,\n            'diffusion_steps': 1000,\n            'dropout': 0.0,\n            'learn_sigma': True,\n            'noise_schedule': \"linear\",\n            'num_channels': 128,\n            'num_head_channels': 64,\n            'num_res_blocks': 1,\n            'resblock_updown': True,\n            'use_fp16': False,\n            'use_scale_shift_norm': True,\n            'rescale_learned_sigmas': True,\n            'p2_gamma': 1,\n            'p2_k': 1,\n            'image_size': image_size,  # Use the command-line flag\n        })\n        # Create model and diffusion\n        model, diffusion = create_model_and_diffusion(**model_config)\n        # Load the pre-trained checkpoint\n        checkpoint = torch.load(model_path, map_location=\"cpu\")\n        model.load_state_dict(checkpoint if 'state_dict' not in checkpoint else checkpoint['state_dict'])\n        model = model.to(device)\n        model.eval()\n        \n        # Set sigma_min and sigma_max to match training_loop.py\n        net = model\n        #net.sigma_min = diffusion.sqrt_one_minus_alphas_cumprod[-1]  # Match training_loop.py\n        #net.sigma_max = diffusion.betas[0]  # Match training_loop.py\n        net.sigma_min = 0.002\n        net.sigma_max = 80\n        model_source = 'guided_diffusion'\n    elif dataset_name in ['cifar10', 'ffhq', 'afhqv2', 'imagenet64']:         # models from EDM\n        with dnnlib.util.open_url(model_path, verbose=(dist.get_rank() == 0)) as f:\n            net = pickle.load(f)['ema'].to(device)\n        net.sigma_min = 0.002\n        net.sigma_max = 80.0\n        model_source = 'edm'\n    elif dataset_name in ['lsun_bedroom']:                                  # models from Consistency Models\n        from models.cm.cm_model_loader import load_cm_model\n        from models.networks_edm import CMPrecond\n        net = load_cm_model(model_path)\n        net = CMPrecond(net).to(device)\n        model_source = 'cm'\n    else:\n        if guidance_type == 'cg':            # clssifier guidance           # models from ADM\n            if classifier_path is None:\n                raise ValueError(\"Classifier path must be provided for guidance_type='cg'\")\n            from models.guided_diffusion.cg_model_loader import load_cg_model\n            from models.networks_edm import CGPrecond\n            net, classifier = load_cg_model(model_path, classifier_path)\n            net = CGPrecond(net, classifier, guidance_rate=guidance_rate).to(device)\n            model_source = 'adm'\n        elif guidance_type in ['uncond', 'cfg']:                            # models from LDM\n            from omegaconf import OmegaConf\n            from models.networks_edm import CFGPrecond\n            if dataset_name in ['lsun_bedroom_ldm']:\n                config = OmegaConf.load('./models/ldm/configs/latent-diffusion/lsun_bedrooms-ldm-vq-4.yaml')\n                net = load_ldm_model(config, model_path)\n                net = CFGPrecond(net, img_resolution=64, img_channels=3, guidance_rate=1., guidance_type='uncond', label_dim=0).to(device)\n            elif dataset_name in ['ms_coco']:\n                assert guidance_type == 'cfg'\n                config = OmegaConf.load('./models/ldm/configs/stable-diffusion/v1-inference.yaml')\n                net = load_ldm_model(config, model_path)\n                net = CFGPrecond(net, img_resolution=64, img_channels=4, guidance_rate=guidance_rate, guidance_type='classifier-free', label_dim=True).to(device)\n            model_source = 'ldm'\n    if net is None:\n        raise ValueError(\"Got wrong settings: check dataset_name and guidance_type!\")\n    net.eval()\n\n    return net, model_source\n\n#----------------------------------------------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate samples using a pre-trained diffusion model and AMED predictor.\")\n\n    # General options\n    parser.add_argument('--predictor_path', type=str, required=True,\n                        help='Path to trained AMED instructor (directory or .pkl file)')\n    parser.add_argument('--model_path', type=str, required=True,\n                        help='Path to the pre-trained diffusion model (required)')\n    parser.add_argument('--classifier_path', type=str, default=None,\n                        help='Path to the classifier model (required for guidance_type=\"cg\")')\n    parser.add_argument('--batch', dest='max_batch_size', type=int, default=64,\n                        help='Maximum batch size (default: 64)')\n    parser.add_argument('--seeds', type=str, default='0-63',\n                        help='Random seeds (e.g., 1,2,5-10) (default: 0-63)')\n    parser.add_argument('--prompt', type=str,\n                        help='Prompt for Stable Diffusion sampling')\n    parser.add_argument('--image_size', type=int, default=64,\n                        help='Image size for sampling (default: 64)')\n\n    # Options for sampling\n    parser.add_argument('--return_inters', type=str2bool, default=False,\n                        help='Whether to save intermediate outputs (default: False)')\n\n    # Options for saving\n    parser.add_argument('--outdir', type=str,\n                        help='Where to save the output images (directory)')\n    parser.add_argument('--grid', type=str2bool, default=False,\n                        help='Whether to make a grid of images (default: False)')\n    parser.add_argument('--subdirs', type=str2bool, default=True,\n                        help='Create subdirectory for every 1000 seeds (default: True)')\n    \n\n    args = parser.parse_args()\n\n    # Convert seeds to a list using parse_int_list\n    seeds = parse_int_list(args.seeds)\n    max_batch_size = args.max_batch_size\n    device = torch.device('cuda')\n\n    # Pass arguments to solver_kwargs\n    solver_kwargs = {\n        'prompt': args.prompt,\n        'image_size': args.image_size,\n        'return_inters': args.return_inters,\n        'guidance_type': None,  # Will be set later\n        'guidance_rate': None,  # Will be set later\n    }\n    # Call the rest of the main function with the parsed arguments\n    dist.init()\n    num_batches = ((len(seeds) - 1) // (max_batch_size * dist.get_world_size()) + 1) * dist.get_world_size()\n    all_batches = torch.as_tensor(seeds).tensor_split(num_batches)\n    rank_batches = all_batches[dist.get_rank() :: dist.get_world_size()]\n\n    # Load models.\n    if dist.get_rank() != 0:\n        torch.distributed.barrier()     # rank 0 goes first\n\n    # Load AMED predictor\n    predictor_path = args.predictor_path\n    if not predictor_path.endswith('pkl'):      # load by experiment number\n        # find the directory with trained AMED predictor\n        predictor_path_str = '0' * (5 - len(predictor_path)) + predictor_path\n        for file_name in os.listdir(\"./exps\"):\n            if file_name.split('-')[0] == predictor_path_str:\n                file_list = [f for f in os.listdir(os.path.join('./exps', file_name)) if f.endswith(\"pkl\")]\n                max_index = -1\n                max_file = None\n                for ckpt_name in file_list:\n                    file_index = int(ckpt_name.split(\"-\")[-1].split(\".\")[0])\n                    if file_index > max_index:\n                        max_index = file_index\n                        max_file = ckpt_name\n                predictor_path = os.path.join('./exps', file_name, max_file)\n                break\n    dist.print0(f'Loading AMED predictor from \"{predictor_path}\"...')\n    with dnnlib.util.open_url(predictor_path, verbose=(dist.get_rank() == 0)) as f:\n        AMED_predictor = pickle.load(f)['model'].to(device)\n    \n    # Update settings\n    prompt = solver_kwargs['prompt']\n    solver_kwargs = {key: value for key, value in solver_kwargs.items() if value is not None}\n    solver_kwargs['AMED_predictor'] = AMED_predictor\n    solver_kwargs['solver'] = solver = AMED_predictor.sampler_stu\n    solver_kwargs['num_steps'] = AMED_predictor.num_steps\n    solver_kwargs['guidance_type'] = AMED_predictor.guidance_type\n    solver_kwargs['guidance_rate'] = AMED_predictor.guidance_rate\n    solver_kwargs['afs'] = AMED_predictor.afs\n    solver_kwargs['denoise_to_zero'] = False\n    solver_kwargs['max_order'] = AMED_predictor.max_order\n    solver_kwargs['predict_x0'] = AMED_predictor.predict_x0\n    solver_kwargs['lower_order_final'] = AMED_predictor.lower_order_final\n    solver_kwargs['schedule_type'] = AMED_predictor.schedule_type\n    solver_kwargs['schedule_rho'] = AMED_predictor.schedule_rho\n    solver_kwargs['prompt'] = prompt\n    solver_kwargs['dataset_name'] = dataset_name = AMED_predictor.dataset_name\n    \n    # Load pre-trained diffusion models.\n    net, solver_kwargs['model_source'] = create_model(\n        dataset_name,\n        solver_kwargs['guidance_type'],\n        solver_kwargs['guidance_rate'],\n        device,\n        image_size=solver_kwargs['image_size'],  # Pass image_size for UNet\n        model_path=args.model_path,\n        classifier_path=args.classifier_path  # Pass classifier_path from args\n    )\n    \n    # TODO: support mixed precision \n    # net.use_fp16 = solver_kwargs['use_fp16']\n\n    # Other ranks follow.\n    if dist.get_rank() == 0:\n        torch.distributed.barrier()\n    \n    # Update settings\n    solver_kwargs['sigma_min'] = net.sigma_min\n    solver_kwargs['sigma_max'] = net.sigma_max\n    nfe = 2 * (solver_kwargs['num_steps'] - 1) - 1 if solver_kwargs[\"afs\"] else 2 * (solver_kwargs['num_steps'] - 1)\n    nfe = 2 * nfe if dataset_name in ['ms_coco'] else nfe   # should double NFE due to the classifier-free-guidance\n    solver_kwargs['nfe'] = nfe\n\n    # Load the prompts\n    if dataset_name in ['ms_coco'] and solver_kwargs['prompt'] is None:\n        # Loading MS-COCO captions for FID-30k evaluaion\n        # We use the selected 30k captions from https://github.com/boomb0om/text2image-benchmark\n        prompt_path, _ = check_file_by_key('prompts')\n        sample_captions = []\n        with open(prompt_path, 'r') as file:\n            reader = csv.DictReader(file)\n            for row in reader:\n                text = row['text']\n                sample_captions.append(text)\n\n    # Construct solver, 5 solvers are provided\n    if solver == 'amed':\n        sampler_fn = solvers_amed.amed_sampler\n    elif solver == 'euler':\n        sampler_fn = solvers_amed.euler_sampler\n    elif solver == 'dpm':\n        sampler_fn = solvers_amed.dpm_2_sampler\n    elif solver == 'ipndm':\n        sampler_fn = solvers_amed.ipndm_sampler\n    elif solver == 'dpmpp':\n        sampler_fn = solvers_amed.dpm_pp_sampler\n    \n    # Print solver settings.\n    dist.print0(\"Solver settings:\")\n    for key, value in solver_kwargs.items():\n        if value is None:\n            continue\n        elif key == 'AMED_predictor':\n            continue\n        elif key == 'max_order' and solver in ['euler', 'dpm']:\n            continue\n        elif key in ['predict_x0', 'lower_order_final'] and solver not in ['dpmpp']:\n            continue\n        elif key in ['prompt'] and dataset_name not in ['ms_coco']:\n            continue\n        dist.print0(f\"\\t{key}: {value}\")\n\n    # Loop over batches.\n    outdir = args.outdir\n    if outdir is None:\n        if args.grid:\n            outdir = os.path.join(f\"./samples/grids/{dataset_name}\", f\"{solver}_nfe{nfe}\")\n        else:\n            outdir = os.path.join(f\"./samples/{dataset_name}\", f\"{solver}_nfe{nfe}\")\n    \n        \n    dist.print0(f'Generating {len(seeds)} images to \"{outdir}\"...')\n    for batch_seeds in tqdm.tqdm(rank_batches, unit='batch', disable=(dist.get_rank() != 0)):\n        torch.distributed.barrier()\n        batch_size = len(batch_seeds)\n        if batch_size == 0:\n            continue\n\n        # Pick latents and labels.\n        rnd = StackedRandomGenerator(device, batch_seeds)\n        latents = rnd.randn([batch_size, net.in_channels, net.image_size, net.image_size], device=device)\n        class_labels = c = uc = None\n        if net.num_classes:\n            if solver_kwargs['model_source'] == 'adm':\n                class_labels = rnd.randint(net.num_classes, size=(batch_size,), device=device)\n            elif solver_kwargs['model_source'] == 'ldm' and dataset_name == 'ms_coco':\n                if solver_kwargs['prompt'] is None:\n                    prompts = sample_captions[batch_seeds[0]:batch_seeds[-1]+1]\n                else:\n                    prompts = [solver_kwargs['prompt'] for i in range(batch_size)]\n                if solver_kwargs['guidance_rate'] != 1.0:\n                    uc = net.model.get_learned_conditioning(batch_size * [\"\"])\n                if isinstance(prompts, tuple):\n                    prompts = list(prompts)\n                c = net.model.get_learned_conditioning(prompts)\n            else:\n                class_labels = torch.eye(net.num_classes, device=device)[rnd.randint(net.num_classes, size=[batch_size], device=device)]\n\n        # Generate images.\n        with torch.no_grad():\n            if solver_kwargs['model_source'] == 'ldm':\n                with autocast(\"cuda\"):\n                    with net.model.ema_scope():\n                        images = sampler_fn(net, latents, condition=c, unconditional_condition=uc, **solver_kwargs)\n                        images = net.model.decode_first_stage(images)\n            elif solver_kwargs['model_source'] == 'guided_diffusion':\n                images = sampler_fn(net, latents, class_labels=class_labels, **solver_kwargs)\n            else:\n                images = sampler_fn(net, latents, class_labels=class_labels, **solver_kwargs)\n\n        # Save images.\n        if args.grid:\n            images = torch.clamp(images / 2 + 0.5, 0, 1)\n            os.makedirs(outdir, exist_ok=True)\n            nrows = int(images.shape[0] ** 0.5)\n            image_grid = make_grid(images, nrows, padding=0)\n            save_image(image_grid, os.path.join(outdir, \"grid.png\"))\n        else:\n            images_np = (images * 127.5 + 128).clip(0, 255).to(torch.uint8).permute(0, 2, 3, 1).cpu().numpy()\n            for seed, image_np in zip(batch_seeds, images_np):\n                image_dir = os.path.join(outdir, f'{seed-seed%1000:06d}') if subdirs else outdir\n                os.makedirs(image_dir, exist_ok=True)\n                image_path = os.path.join(image_dir, f'{seed:06d}.png')\n                PIL.Image.fromarray(image_np, 'RGB').save(image_path)\n        \n    # Done.\n    torch.distributed.barrier()\n    dist.print0('Done.')\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n\n'''\n\nwith open('/kaggle/working/p2w/sample_amed.py', 'w') as f:\n    f.write(sample_amed)\nprint(\"Successfully wrote sample_amed.py to /kaggle/working/p2w/sample_amed.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:40:22.479668Z","iopub.execute_input":"2025-03-27T04:40:22.479869Z","iopub.status.idle":"2025-03-27T04:40:22.501151Z","shell.execute_reply.started":"2025-03-27T04:40:22.479851Z","shell.execute_reply":"2025-03-27T04:40:22.500438Z"}},"outputs":[{"name":"stdout","text":"Successfully wrote sample_amed.py to /kaggle/working/p2w/sample_amed.py\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"my pkl from above code with amed and heun sigma_min=0.01 , sigma_max=10, lr=0.001","metadata":{}},{"cell_type":"code","source":"# import gdown\n\n# gdown.download(f'https://drive.google.com/uc?id=1Ba8Q7QemWnq5FR0CROtnKDWLCxlGolvC', '/kaggle/working/p2w/exps/00008-celebahq-20-37-amed-heun-1-poly7.0-afs-predictor_64-000010.pkl', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:40:22.501854Z","iopub.execute_input":"2025-03-27T04:40:22.502059Z","iopub.status.idle":"2025-03-27T04:40:22.519245Z","shell.execute_reply.started":"2025-03-27T04:40:22.502040Z","shell.execute_reply":"2025-03-27T04:40:22.518609Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"ifti pkl with training loop heun amed sigma min=0.002, sigma max =80","metadata":{}},{"cell_type":"code","source":"import gdown\n\ngdown.download(f'https://drive.google.com/uc?id=16uxpUAgycDj-tfV0nHJGn1nV0mlWNebh', '/kaggle/working/p2w/exps/00007-celebahq-4-5-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:40:22.520045Z","iopub.execute_input":"2025-03-27T04:40:22.520244Z","iopub.status.idle":"2025-03-27T04:40:26.289005Z","shell.execute_reply.started":"2025-03-27T04:40:22.520227Z","shell.execute_reply":"2025-03-27T04:40:26.288084Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?id=16uxpUAgycDj-tfV0nHJGn1nV0mlWNebh\nTo: /kaggle/working/p2w/exps/00007-celebahq-4-5-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl\n100%|██████████| 48.5k/48.5k [00:00<00:00, 38.6MB/s]\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/p2w/exps/00007-celebahq-4-5-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl'"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"import gdown\n\ngdown.download(f'https://drive.google.com/uc?id=1YCbKBKWMy2RC0zSdfptPDI5ANYydcfzB', '/kaggle/working/p2w/exps/00000-celebahq-20-37-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:41:19.636650Z","iopub.execute_input":"2025-03-27T04:41:19.636967Z","iopub.status.idle":"2025-03-27T04:41:23.172321Z","shell.execute_reply.started":"2025-03-27T04:41:19.636932Z","shell.execute_reply":"2025-03-27T04:41:23.171409Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1YCbKBKWMy2RC0zSdfptPDI5ANYydcfzB\nTo: /kaggle/working/p2w/exps/00000-celebahq-20-37-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl\n100%|██████████| 48.5k/48.5k [00:00<00:00, 33.3MB/s]\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/p2w/exps/00000-celebahq-20-37-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl'"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"import gdown\n\ngdown.download(f'https://drive.google.com/uc?id=1bJRkIjStFP8ohAQTrnzQ1oHd17P44p85', '/kaggle/working/p2w/exps/00000-celebahq-50-97-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:41:31.571506Z","iopub.execute_input":"2025-03-27T04:41:31.571868Z","iopub.status.idle":"2025-03-27T04:41:35.074397Z","shell.execute_reply.started":"2025-03-27T04:41:31.571839Z","shell.execute_reply":"2025-03-27T04:41:35.073727Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1bJRkIjStFP8ohAQTrnzQ1oHd17P44p85\nTo: /kaggle/working/p2w/exps/00000-celebahq-50-97-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl\n100%|██████████| 48.5k/48.5k [00:00<00:00, 29.4MB/s]\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/p2w/exps/00000-celebahq-50-97-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"import gdown\n\ngdown.download(f'https://drive.google.com/uc?id=15KKJssODiz8EskJZ4hS-m3678UzVEkR-', '/kaggle/working/p2w/exps/00000-celebahq-10-17-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:42:57.990596Z","iopub.execute_input":"2025-03-27T04:42:57.990942Z","iopub.status.idle":"2025-03-27T04:43:01.865600Z","shell.execute_reply.started":"2025-03-27T04:42:57.990918Z","shell.execute_reply":"2025-03-27T04:43:01.864715Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?id=15KKJssODiz8EskJZ4hS-m3678UzVEkR-\nTo: /kaggle/working/p2w/exps/00000-celebahq-10-17-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl\n100%|██████████| 48.5k/48.5k [00:00<00:00, 32.9MB/s]\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/p2w/exps/00000-celebahq-10-17-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl'"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"# !source /kaggle/working/miniconda/etc/profile.d/conda.sh && conda activate p2w && cd /kaggle/working/p2w && \\\n# CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 --master_port=22222 \\\n# sample_amed.py \\\n# --predictor_path /kaggle/working/p2w/exps/00001-celebahq-50-97-amed-heun-1-uni1.0-afs/predictor_64-000010.pkl \\\n# --model_path /kaggle/working/p2w/checkpoints/64x64.pt \\\n# --batch 64 \\\n# --seeds 0-4 \\\n# --outdir /kaggle/working/p2w/logs \\\n# --grid True \\\n# --subdirs True \\\n# --image_size 64","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:40:26.289817Z","iopub.execute_input":"2025-03-27T04:40:26.290103Z","iopub.status.idle":"2025-03-27T04:40:26.293296Z","shell.execute_reply.started":"2025-03-27T04:40:26.290081Z","shell.execute_reply":"2025-03-27T04:40:26.292634Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# from PIL import Image\n# import IPython.display as display\n\n# img = Image.open('/kaggle/working/p2w/logs/grid.png')\n# display.display(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:40:26.293993Z","iopub.execute_input":"2025-03-27T04:40:26.294193Z","iopub.status.idle":"2025-03-27T04:40:26.645844Z","shell.execute_reply.started":"2025-03-27T04:40:26.294175Z","shell.execute_reply":"2025-03-27T04:40:26.644868Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"rough suggestion for coarseIg inapinting from grok","metadata":{}},{"cell_type":"code","source":"import gdown\n!mkdir -p /kaggle/working/p2w/demo\n!mkdir -p /kaggle/working/p2w/demo/mask\n!mkdir -p /kaggle/working/p2w/demo/mask/thick\n!mkdir -p /kaggle/working/p2w/demo/image\ngdown.download(f'https://drive.google.com/uc?id=1RsdhJNRsss08_b931z6Lbk0dOHf_S2_O', '/kaggle/working/p2w/demo/image/27000.png', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=1QZKhF_Me569OlCc-UnsEkxxQht5ElHCY', '/kaggle/working/p2w/demo/image/27065.png', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=1Nza3W9lECZD5wc0jCWHOxClDrAFmCoo8', '/kaggle/working/p2w/demo/image/27054.png', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=19Ljx4oLcpwNHtgblfGy2KE4mcCI-MoLu', '/kaggle/working/p2w/demo/image/27078.png', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=1Q7Un8pVdz2q1_vB3MJMV9phV4MO3JPKj', '/kaggle/working/p2w/demo/image/27328.png', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=1ka7PoNUEGma0wedRT8dEhyRFUmP1stWi', '/kaggle/working/p2w/demo/mask/thick/000000.png', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=11KuSxK_znTAQvQwMvEnGer6ZhBa4FHU6', '/kaggle/working/p2w/demo/mask/thick/000001.png', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=1IaCiw6zPmtXEradXsaYh2EJu1-JzGDpj', '/kaggle/working/p2w/demo/mask/thick/000002.png', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=1so_ezrTm5Wv7DXttdZMjMiYzlgmvEE3Q', '/kaggle/working/p2w/demo/mask/thick/000003.png', quiet=False)\ngdown.download(f'https://drive.google.com/uc?id=1uUzTaDE-u9Ab04SZuB5C22NFyznEPJUn', '/kaggle/working/p2w/demo/mask/thick/000004.png', quiet=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:40:26.646785Z","iopub.execute_input":"2025-03-27T04:40:26.647042Z","iopub.status.idle":"2025-03-27T04:41:06.653139Z","shell.execute_reply.started":"2025-03-27T04:40:26.647020Z","shell.execute_reply":"2025-03-27T04:41:06.652316Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1RsdhJNRsss08_b931z6Lbk0dOHf_S2_O\nTo: /kaggle/working/p2w/demo/image/27000.png\n100%|██████████| 8.01k/8.01k [00:00<00:00, 6.72MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1QZKhF_Me569OlCc-UnsEkxxQht5ElHCY\nTo: /kaggle/working/p2w/demo/image/27065.png\n100%|██████████| 11.5k/11.5k [00:00<00:00, 14.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1Nza3W9lECZD5wc0jCWHOxClDrAFmCoo8\nTo: /kaggle/working/p2w/demo/image/27054.png\n100%|██████████| 8.21k/8.21k [00:00<00:00, 10.5MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=19Ljx4oLcpwNHtgblfGy2KE4mcCI-MoLu\nTo: /kaggle/working/p2w/demo/image/27078.png\n100%|██████████| 9.27k/9.27k [00:00<00:00, 12.3MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1Q7Un8pVdz2q1_vB3MJMV9phV4MO3JPKj\nTo: /kaggle/working/p2w/demo/image/27328.png\n100%|██████████| 8.73k/8.73k [00:00<00:00, 12.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1ka7PoNUEGma0wedRT8dEhyRFUmP1stWi\nTo: /kaggle/working/p2w/demo/mask/thick/000000.png\n100%|██████████| 1.15k/1.15k [00:00<00:00, 1.93MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=11KuSxK_znTAQvQwMvEnGer6ZhBa4FHU6\nTo: /kaggle/working/p2w/demo/mask/thick/000001.png\n100%|██████████| 1.45k/1.45k [00:00<00:00, 4.12MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1IaCiw6zPmtXEradXsaYh2EJu1-JzGDpj\nTo: /kaggle/working/p2w/demo/mask/thick/000002.png\n100%|██████████| 1.49k/1.49k [00:00<00:00, 4.43MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1so_ezrTm5Wv7DXttdZMjMiYzlgmvEE3Q\nTo: /kaggle/working/p2w/demo/mask/thick/000003.png\n100%|██████████| 1.85k/1.85k [00:00<00:00, 2.30MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1uUzTaDE-u9Ab04SZuB5C22NFyznEPJUn\nTo: /kaggle/working/p2w/demo/mask/thick/000004.png\n100%|██████████| 1.89k/1.89k [00:00<00:00, 2.83MB/s]\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/p2w/demo/mask/thick/000004.png'"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"**old coarse amed no inpainting in solvers**","metadata":{}},{"cell_type":"code","source":"coarse_amed='''import os\nimport re\nimport csv\nimport pickle\nimport numpy as np\nimport torch\nimport PIL.Image\nimport dnnlib\nfrom torch import autocast\nfrom torch_utils import distributed as dist\nfrom torchvision.utils import make_grid, save_image\nimport solvers_amed\nimport tqdm\nfrom guided_diffusion import dist_util, logger\nfrom guided_diffusion.script_util import (\n    model_and_diffusion_defaults,\n    create_model_and_diffusion,\n)\nfrom guided_diffusion.image_datasets import load_data\n\n#----------------------------------------------------------------------------\n# Wrapper for torch.Generator that allows specifying a different random seed\n# for each sample in a minibatch.\n\nclass StackedRandomGenerator:\n    def __init__(self, device, seeds):\n        super().__init__()\n        self.generators = [torch.Generator(device).manual_seed(int(seed) % (1 << 32)) for seed in seeds]\n\n    def randn(self, size, **kwargs):\n        assert size[0] == len(self.generators)\n        return torch.stack([torch.randn(size[1:], generator=gen, **kwargs) for gen in self.generators])\n\n    def randn_like(self, input):\n        return self.randn(input.shape, dtype=input.dtype, layout=input.layout, device=input.device)\n\n    def randint(self, *args, size, **kwargs):\n        assert size[0] == len(self.generators)\n        return torch.stack([torch.randint(*args, size=size[1:], generator=gen, **kwargs) for gen in self.generators])\n\n#----------------------------------------------------------------------------\n# Parse a comma separated list of numbers or ranges and return a list of ints.\n# Example: '1,2,5-10' returns [1, 2, 5, 6, 7, 8, 9, 10]\n\ndef parse_int_list(s):\n    if isinstance(s, list): return s\n    ranges = []\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    for p in s.split(','):\n        m = range_re.match(p)\n        if m:\n            ranges.extend(range(int(m.group(1)), int(m.group(2))+1))\n        else:\n            ranges.append(int(p))\n    return ranges\n\n#----------------------------------------------------------------------------\n# Function to parse boolean arguments for argparse\n\ndef str2bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\n#----------------------------------------------------------------------------\n# Load reference images and masks for inpainting\n\ndef load_reference(data_dir, batch_size, image_size, class_cond=False):\n    data = load_data(\n        data_dir=data_dir,\n        batch_size=batch_size,\n        image_size=image_size,\n        class_cond=class_cond,\n        deterministic=True,\n        random_flip=False,\n    )\n    for large_batch, model_kwargs in data:\n        model_kwargs[\"ref_img\"] = large_batch\n        yield model_kwargs\n\n#----------------------------------------------------------------------------\n\ndef create_model(dataset_name=None, guidance_type=None, guidance_rate=None, device=None, image_size=64, model_path=None, classifier_path=None):\n    from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n\n    if dataset_name == 'celebahq':\n        dist.print0(f'Loading pre-trained guided-diffusion model for CelebAHQ from \"{model_path}\"...')\n        model_config = model_and_diffusion_defaults()\n        model_config.update({\n            'attention_resolutions': \"16\",\n            'class_cond': False,\n            'diffusion_steps': 1000,\n            'dropout': 0.0,\n            'learn_sigma': True,\n            'noise_schedule': \"linear\",\n            'num_channels': 128,\n            'num_head_channels': 64,\n            'num_res_blocks': 1,\n            'resblock_updown': True,\n            'use_fp16': False,\n            'use_scale_shift_norm': True,\n            'rescale_learned_sigmas': True,\n            'p2_gamma': 1,\n            'p2_k': 1,\n            'image_size': image_size,\n        })\n        model, diffusion = create_model_and_diffusion(**model_config)\n        checkpoint = torch.load(model_path, map_location=\"cpu\")\n        model.load_state_dict(checkpoint if 'state_dict' not in checkpoint else checkpoint['state_dict'])\n        model = model.to(device)\n        model.eval()\n\n        net = model\n        net.sigma_min = 0.002\n        net.sigma_max = 80\n        net.img_resolution = image_size\n        model_source = 'guided_diffusion'\n    else:\n        dist.print0(f'Loading the pre-trained diffusion model from \"{model_path}\"...')\n        if dataset_name in ['cifar10', 'ffhq', 'afhqv2', 'imagenet64']:\n            with dnnlib.util.open_url(model_path, verbose=(dist.get_rank() == 0)) as f:\n                net = pickle.load(f)['ema'].to(device)\n            net.sigma_min = 0.002\n            net.sigma_max = 80.0\n            model_source = 'edm'\n            net.img_resolution = image_size\n        elif dataset_name in ['lsun_bedroom']:\n            from models.cm.cm_model_loader import load_cm_model\n            from models.networks_edm import CMPrecond\n            net = load_cm_model(model_path)\n            net = CMPrecond(net).to(device)\n            model_source = 'cm'\n            net.img_resolution = image_size\n        else:\n            if guidance_type == 'cg':\n                if classifier_path is None:\n                    raise ValueError(\"Classifier path must be provided for guidance_type='cg'\")\n                from models.guided_diffusion.cg_model_loader import load_cg_model\n                from models.networks_edm import CGPrecond\n                net, classifier = load_cg_model(model_path, classifier_path)\n                net = CGPrecond(net, classifier, guidance_rate=guidance_rate).to(device)\n                model_source = 'adm'\n                net.img_resolution = image_size\n            elif guidance_type in ['uncond', 'cfg']:\n                from omegaconf import OmegaConf\n                from models.networks_edm import CFGPrecond\n                if dataset_name in ['lsun_bedroom_ldm']:\n                    config = OmegaConf.load('./models/ldm/configs/latent-diffusion/lsun_bedrooms-ldm-vq-4.yaml')\n                    net = load_ldm_model(config, model_path)\n                    net = CFGPrecond(net, img_resolution=image_size, img_channels=3, guidance_rate=1., guidance_type='uncond', label_dim=0).to(device)\n                elif dataset_name in ['ms_coco']:\n                    assert guidance_type == 'cfg'\n                    config = OmegaConf.load('./models/ldm/configs/stable-diffusion/v1-inference.yaml')\n                    net = load_ldm_model(config, model_path)\n                    net = CFGPrecond(net, img_resolution=image_size, img_channels=4, guidance_rate=guidance_rate, guidance_type='classifier-free', label_dim=True).to(device)\n                model_source = 'ldm'\n                net.img_resolution = image_size\n        if net is None:\n            raise ValueError(\"Got wrong settings: check dataset_name and guidance_type!\")\n        net.eval()\n\n    return net, diffusion, model_source\n\n#----------------------------------------------------------------------------\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser(description=\"Generate samples using a pre-trained diffusion model and AMED predictor with coarse inpainting support at 64x64.\")\n\n    # General options\n    parser.add_argument('--predictor_path', type=str, required=True,\n                        help='Path to trained AMED instructor (directory or .pkl file)')\n    parser.add_argument('--model_path', type=str, required=True,\n                        help='Path to the pre-trained diffusion model (required)')\n    parser.add_argument('--classifier_path', type=str, default=None,\n                        help='Path to the classifier model (required for guidance_type=\"cg\")')\n    parser.add_argument('--batch', dest='max_batch_size', type=int, default=64,\n                        help='Maximum batch size (default: 64)')\n    parser.add_argument('--seeds', type=str, default='0-63',\n                        help='Random seeds (e.g., 1,2,5-10) (default: 0-63)')\n    parser.add_argument('--prompt', type=str,\n                        help='Prompt for Stable Diffusion sampling')\n    parser.add_argument('--image_size', type=int, default=64,\n                        help='Image size for sampling (default: 64, fixed for coarse inpainting)')\n\n    # Options for sampling\n    parser.add_argument('--return_inters', type=str2bool, default=False,\n                        help='Whether to save intermediate outputs (default: False)')\n\n    # Options for saving\n    parser.add_argument('--outdir', type=str,\n                        help='Where to save the output images (directory)')\n    parser.add_argument('--grid', type=str2bool, default=False,\n                        help='Whether to make a grid of images (default: False)')\n    parser.add_argument('--subdirs', type=str2bool, default=True,\n                        help='Create subdirectory for every 1000 seeds (default: True)')\n\n    # Inpainting-specific options (coarse stage only)\n    parser.add_argument('--base_samples', type=str, default='',\n                        help='Directory containing reference images for inpainting')\n    parser.add_argument('--mask_path', type=str, default='',\n                        help='Directory containing masks for inpainting')\n    parser.add_argument('--use_inpainting', type=str2bool, default=False,\n                        help='Whether to perform inpainting instead of unconditional generation')\n    parser.add_argument('--use_inverse_masks', type=str2bool, default=False,\n                        help='Whether to invert the masks (i.e., mask values become -1)')\n\n\n    args = parser.parse_args()\n\n    # Convert seeds to a list using parse_int_list\n    seeds = parse_int_list(args.seeds)\n    max_batch_size = args.max_batch_size\n    device = torch.device('cuda')\n\n    # Pass arguments to solver_kwargs\n    solver_kwargs = {\n        'prompt': args.prompt,\n        'image_size': args.image_size,\n        'return_inters': args.return_inters,\n        'guidance_type': None,\n        'guidance_rate': None,\n    }\n\n    # Call the rest of the main function with the parsed arguments\n    dist.init()\n    num_batches = ((len(seeds) - 1) // (max_batch_size * dist.get_world_size()) + 1) * dist.get_world_size()\n    all_batches = torch.as_tensor(seeds).tensor_split(num_batches)\n    rank_batches = all_batches[dist.get_rank() :: dist.get_world_size()]\n\n    # Setup distributed logging\n    dist_util.setup_dist()\n    logger.configure(dir=args.outdir)\n\n    # Load models\n    if dist.get_rank() != 0:\n        torch.distributed.barrier()\n\n    # Load AMED predictor\n    predictor_path = args.predictor_path\n    if not predictor_path.endswith('pkl'):\n        predictor_path_str = '0' * (5 - len(predictor_path)) + predictor_path\n        for file_name in os.listdir(\"./exps\"):\n            if file_name.split('-')[0] == predictor_path_str:\n                file_list = [f for f in os.listdir(os.path.join('./exps', file_name)) if f.endswith(\"pkl\")]\n                max_index = -1\n                max_file = None\n                for ckpt_name in file_list:\n                    file_index = int(ckpt_name.split(\"-\")[-1].split(\".\")[0])\n                    if file_index > max_index:\n                        max_index = file_index\n                        max_file = ckpt_name\n                predictor_path = os.path.join('./exps', file_name, max_file)\n                break\n    dist.print0(f'Loading AMED predictor from \"{predictor_path}\"...')\n    with dnnlib.util.open_url(predictor_path, verbose=(dist.get_rank() == 0)) as f:\n        AMED_predictor = pickle.load(f)['model'].to(device)\n\n    # Update solver settings\n    prompt = solver_kwargs['prompt']\n    solver_kwargs = {key: value for key, value in solver_kwargs.items() if value is not None}\n    solver_kwargs['AMED_predictor'] = AMED_predictor\n    solver_kwargs['solver'] = solver = AMED_predictor.sampler_stu\n    solver_kwargs['num_steps'] = AMED_predictor.num_steps\n    solver_kwargs['guidance_type'] = AMED_predictor.guidance_type\n    solver_kwargs['guidance_rate'] = AMED_predictor.guidance_rate\n    solver_kwargs['afs'] = AMED_predictor.afs\n    solver_kwargs['denoise_to_zero'] = False\n    solver_kwargs['max_order'] = AMED_predictor.max_order\n    solver_kwargs['predict_x0'] = AMED_predictor.predict_x0\n    solver_kwargs['lower_order_final'] = AMED_predictor.lower_order_final\n    solver_kwargs['schedule_type'] = AMED_predictor.schedule_type\n    solver_kwargs['schedule_rho'] = AMED_predictor.schedule_rho\n    solver_kwargs['prompt'] = prompt\n    solver_kwargs['dataset_name'] = dataset_name = AMED_predictor.dataset_name\n\n    # Load pre-trained diffusion model (64x64 only)\n    model_64, diffusion_64, model_source = create_model(\n        dataset_name,\n        solver_kwargs['guidance_type'],\n        solver_kwargs['guidance_rate'],\n        device,\n        image_size=64,  # Fixed for coarse inpainting\n        model_path=args.model_path,\n        classifier_path=args.classifier_path\n    )\n    solver_kwargs['model_source'] = model_source\n\n    # Other ranks follow\n    if dist.get_rank() == 0:\n        torch.distributed.barrier()\n\n    # Update solver settings with sigma values\n    solver_kwargs['sigma_min'] = model_64.sigma_min\n    solver_kwargs['sigma_max'] = model_64.sigma_max\n    nfe = 2 * (solver_kwargs['num_steps'] - 1) - 1 if solver_kwargs[\"afs\"] else 2 * (solver_kwargs['num_steps'] - 1)\n    nfe = 2 * nfe if dataset_name in ['ms_coco'] else nfe\n    solver_kwargs['nfe'] = nfe\n\n    # Load data for inpainting\n    if args.use_inpainting:\n        if not args.base_samples or not args.mask_path:\n            raise ValueError(\"Must provide --base_samples and --mask_path for inpainting\")\n\n        # Data for 64x64 (coarse stage)\n        data_64 = load_reference(\n            args.base_samples,\n            max_batch_size,\n            image_size=64,\n            class_cond=False,\n        )\n        data_mask_64 = load_reference(\n            args.mask_path,\n            max_batch_size,\n            image_size=64,\n            class_cond=False,\n        )\n\n        # Count total number of inputs\n        all_items = os.listdir(args.base_samples)\n        num_inputs = len(all_items)\n    else:\n        num_inputs = len(seeds)\n\n    # Construct solver\n    if solver == 'amed':\n        sampler_fn = solvers_amed.amed_sampler\n    elif solver == 'euler':\n        sampler_fn = solvers_amed.euler_sampler\n    elif solver == 'dpm':\n        sampler_fn = solvers_amed.dpm_2_sampler\n    elif solver == 'ipndm':\n        sampler_fn = solvers_amed.ipndm_sampler\n    elif solver == 'dpmpp':\n        sampler_fn = solvers_amed.dpm_pp_sampler\n    else:\n        sampler_fn = solvers_amed.euler_sampler\n        solver_kwargs['solver'] = 'euler'\n        solver_kwargs['num_steps'] = 50\n        nfe = 2 * (solver_kwargs['num_steps'] - 1)\n        solver_kwargs['nfe'] = nfe\n\n    # Print solver settings\n    dist.print0(\"Solver settings:\")\n    for key, value in solver_kwargs.items():\n        if value is None:\n            continue\n        elif key == 'AMED_predictor':\n            continue\n        elif key == 'max_order' and solver in ['euler', 'dpm']:\n            continue\n        elif key in ['predict_x0', 'lower_order_final'] and solver not in ['dpmpp']:\n            continue\n        elif key in ['prompt'] and dataset_name not in ['ms_coco']:\n            continue\n        dist.print0(f\"\\t{key}: {value}\")\n\n    # Loop over batches\n    if args.outdir is None:\n        if args.grid:\n            outdir = os.path.join(f\"./samples/grids/{dataset_name}\", f\"{solver}_nfe{nfe}\")\n        else:\n            outdir = os.path.join(f\"./samples/{dataset_name}\", f\"{solver}_nfe{nfe}\")\n    else:\n        outdir = args.outdir\n\n    dist.print0(f'Generating {num_inputs} images to \"{outdir}\"...')\n    count = 0\n    for batch_seeds in tqdm.tqdm(rank_batches, unit='batch', disable=(dist.get_rank() != 0)):\n        torch.distributed.barrier()\n        batch_size = len(batch_seeds)\n        if batch_size == 0:\n            continue\n\n        if args.use_inpainting:\n            # Load reference images and masks\n            model_kwargs_64 = next(data_64)\n            model_mask_kwargs_64 = next(data_mask_64)\n            model_kwargs_64 = {k: v.to(device) for k, v in model_kwargs_64.items()}\n            model_mask_kwargs_64 = {k: v.to(device) for k, v in model_mask_kwargs_64.items()}\n\n            gt = model_kwargs_64[\"ref_img\"]\n            mask = model_mask_kwargs_64['ref_img']\n\n            if args.use_inverse_masks:\n                model_mask_kwargs_64[\"ref_img\"] = model_mask_kwargs_64[\"ref_img\"] * (-1)\n\n            mask[mask < 0.] = 0\n            mask[mask > 0.] = 1\n\n            # Shape for sampling\n            shape = (batch_size, 3, 64, 64)  # Fixed for coarse inpainting\n\n            # Initial noise\n            latents = torch.randn(shape, device=device)\n        else:\n            # Unconditional generation\n            shape = (batch_size, 3, args.image_size, args.image_size)\n            latents = torch.randn(shape, device=device)\n            model_kwargs_64 = None\n            model_mask_kwargs_64 = None\n            gt = None\n            mask = None\n\n        # Pick latents and labels for unconditional generation\n        if not args.use_inpainting:\n            rnd = StackedRandomGenerator(device, batch_seeds)\n            latents = rnd.randn([batch_size, 3, args.image_size, args.image_size], device=device)\n            class_labels = c = uc = None\n            if model_64.num_classes:\n                if solver_kwargs['model_source'] == 'adm':\n                    class_labels = rnd.randint(model_64.num_classes, size=(batch_size,), device=device)\n                elif solver_kwargs['model_source'] == 'ldm' and dataset_name == 'ms_coco':\n                    if solver_kwargs['prompt'] is None:\n                        prompts = sample_captions[batch_seeds[0]:batch_seeds[-1]+1]\n                    else:\n                        prompts = [solver_kwargs['prompt'] for i in range(batch_size)]\n                    if solver_kwargs['guidance_rate'] != 1.0:\n                        uc = model_64.model.get_learned_conditioning(batch_size * [\"\"])\n                    if isinstance(prompts, tuple):\n                        prompts = list(prompts)\n                    c = model_64.model.get_learned_conditioning(prompts)\n                else:\n                    class_labels = torch.eye(model_64.num_classes, device=device)[rnd.randint(model_64.num_classes, size=[batch_size], device=device)]\n        else:\n            class_labels = c = uc = None\n\n        # Generate images\n        with torch.no_grad():\n            if solver_kwargs['model_source'] == 'ldm':\n                with autocast(\"cuda\"):\n                    with model_64.model.ema_scope():\n                        images = sampler_fn(model_64, latents, condition=c, unconditional_condition=uc, **solver_kwargs)\n                        images = model_64.model.decode_first_stage(images)\n            elif solver_kwargs['model_source'] == 'guided_diffusion':\n                # Add inpainting-specific kwargs to solver_kwargs\n                solver_kwargs['model_kwargs'] = model_kwargs_64 if args.use_inpainting else None\n                solver_kwargs['model_mask_kwargs'] = model_mask_kwargs_64 if args.use_inpainting else None\n                images = sampler_fn(model_64, latents, class_labels=class_labels, **solver_kwargs)\n            else:\n                images = sampler_fn(model_64, latents, class_labels=class_labels, **solver_kwargs)\n\n        # Save images\n        if args.use_inpainting:\n            for i in range(batch_size):\n                os.makedirs(os.path.join(outdir, \"gtImg\"), exist_ok=True)\n                os.makedirs(os.path.join(outdir, \"inputImg\"), exist_ok=True)\n                os.makedirs(os.path.join(outdir, \"sampledImg\"), exist_ok=True)\n                os.makedirs(os.path.join(outdir, \"coarseImg\"), exist_ok=True)\n\n                out_gtImg_path = os.path.join(outdir, \"gtImg\", f\"{str(count + i).zfill(4)}.png\")\n                out_inputImg_path = os.path.join(outdir, \"inputImg\", f\"{str(count + i).zfill(4)}.png\")\n                out_sampledImg_path = os.path.join(outdir, \"sampledImg\", f\"{str(count + i).zfill(4)}.png\")\n                out_coarseImg_path = os.path.join(outdir, \"coarseImg\", f\"{str(count + i).zfill(4)}.png\")\n\n                tmp_ones = torch.ones(gt[i].shape) * (-1)\n                inputImg = gt[i].to(mask.device) * mask[i] + (1 - mask[i]) * tmp_ones.to(mask.device)\n                sampledImg = images[i].unsqueeze(0)\n                coarseImg = mask[i] * inputImg + (1 - mask[i]) * sampledImg\n\n                gtImg = gt[i]\n                gtImg = gtImg.reshape(coarseImg.shape).to(coarseImg.device)\n\n                save_image(gtImg, out_gtImg_path, nrow=1, normalize=True, value_range=(-1, 1))\n                save_image(inputImg, out_inputImg_path, nrow=1, normalize=True, value_range=(-1, 1))\n                save_image(sampledImg, out_sampledImg_path, nrow=1, normalize=True, value_range=(-1, 1))\n                save_image(coarseImg, out_coarseImg_path, nrow=1, normalize=True, value_range=(-1, 1))\n        else:\n            if args.grid:\n                images = torch.clamp(images / 2 + 0.5, 0, 1)\n                os.makedirs(outdir, exist_ok=True)\n                nrows = int(images.shape[0] ** 0.5)\n                image_grid = make_grid(images, nrows, padding=0)\n                save_image(image_grid, os.path.join(outdir, \"grid.png\"))\n            else:\n                images_np = (images * 127.5 + 128).clip(0, 255).to(torch.uint8).permute(0, 2, 3, 1).cpu().numpy()\n                for seed, image_np in zip(batch_seeds, images_np):\n                    image_dir = os.path.join(outdir, f'{seed-seed%1000:06d}') if args.subdirs else outdir\n                    os.makedirs(image_dir, exist_ok=True)\n                    image_path = os.path.join(image_dir, f'{seed:06d}.png')\n                    PIL.Image.fromarray(image_np, 'RGB').save(image_path)\n\n        count += batch_size\n        dist.print0(f\"Created {count} samples\")\n\n    # Done\n    torch.distributed.barrier()\n    dist.print0('Done.')\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n'''\n\nwith open('/kaggle/working/p2w/coarse_amed.py', 'w') as f:\n    f.write(coarse_amed)\nprint(\"Successfully wrote coarse_amed.py to /kaggle/working/p2w/coarse_amed.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:41:06.654087Z","iopub.execute_input":"2025-03-27T04:41:06.654399Z","iopub.status.idle":"2025-03-27T04:41:06.664272Z","shell.execute_reply.started":"2025-03-27T04:41:06.654373Z","shell.execute_reply":"2025-03-27T04:41:06.663495Z"}},"outputs":[{"name":"stdout","text":"Successfully wrote coarse_amed.py to /kaggle/working/p2w/coarse_amed.py\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import os\n\nold_filename = \"/kaggle/working/p2w/coarse_amed.py\"\nnew_filename = \"/kaggle/working/p2w/coarse_amed_old.py\"\n\nos.rename(old_filename, new_filename)\nprint(f\"Renamed {old_filename} to {new_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:41:06.665205Z","iopub.execute_input":"2025-03-27T04:41:06.665561Z","iopub.status.idle":"2025-03-27T04:41:06.684443Z","shell.execute_reply.started":"2025-03-27T04:41:06.665507Z","shell.execute_reply":"2025-03-27T04:41:06.683424Z"}},"outputs":[{"name":"stdout","text":"Renamed /kaggle/working/p2w/coarse_amed.py to /kaggle/working/p2w/coarse_amed_old.py\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"coarse_amedv2='''\nimport os\nimport re\nimport csv\nimport pickle\nimport numpy as np\nimport torch\nimport PIL.Image\nimport dnnlib\nfrom torch import autocast\nfrom torch_utils import distributed as dist\nfrom torchvision.utils import make_grid, save_image\nimport solvers_amed\nimport tqdm\nfrom guided_diffusion import dist_util, logger\nfrom guided_diffusion.script_util import (\n    model_and_diffusion_defaults,\n    create_model_and_diffusion,\n)\nfrom guided_diffusion.image_datasets import load_data\n\n#----------------------------------------------------------------------------\n# Wrapper for torch.Generator that allows specifying a different random seed\n# for each sample in a minibatch.\n\nclass StackedRandomGenerator:\n    def __init__(self, device, seeds):\n        super().__init__()\n        self.generators = [torch.Generator(device).manual_seed(int(seed) % (1 << 32)) for seed in seeds]\n\n    def randn(self, size, **kwargs):\n        assert size[0] == len(self.generators)\n        return torch.stack([torch.randn(size[1:], generator=gen, **kwargs) for gen in self.generators])\n\n    def randn_like(self, input):\n        return self.randn(input.shape, dtype=input.dtype, layout=input.layout, device=input.device)\n\n    def randint(self, *args, size, **kwargs):\n        assert size[0] == len(self.generators)\n        return torch.stack([torch.randint(*args, size=size[1:], generator=gen, **kwargs) for gen in self.generators])\n\n#----------------------------------------------------------------------------\n# Parse a comma separated list of numbers or ranges and return a list of ints.\n# Example: '1,2,5-10' returns [1, 2, 5, 6, 7, 8, 9, 10]\n\ndef parse_int_list(s):\n    if isinstance(s, list): return s\n    ranges = []\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    for p in s.split(','):\n        m = range_re.match(p)\n        if m:\n            ranges.extend(range(int(m.group(1)), int(m.group(2))+1))\n        else:\n            ranges.append(int(p))\n    return ranges\n\n#----------------------------------------------------------------------------\n# Function to parse boolean arguments for argparse\n\ndef str2bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\n#----------------------------------------------------------------------------\n# Load reference images and masks for inpainting\n\ndef load_reference(data_dir, batch_size, image_size, class_cond=False):\n    data = load_data(\n        data_dir=data_dir,\n        batch_size=batch_size,\n        image_size=image_size,\n        class_cond=class_cond,\n        deterministic=True,\n        random_flip=False,\n    )\n    for large_batch, model_kwargs in data:\n        model_kwargs[\"ref_img\"] = large_batch\n        yield model_kwargs\n\n#----------------------------------------------------------------------------\n\ndef create_model(dataset_name=None, guidance_type=None, guidance_rate=None, device=None, image_size=64, model_path=None, classifier_path=None):\n    from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n\n    if dataset_name == 'celebahq':\n        dist.print0(f'Loading pre-trained guided-diffusion model for CelebAHQ from \"{model_path}\"...')\n        model_config = model_and_diffusion_defaults()\n        model_config.update({\n            'attention_resolutions': \"16\",\n            'class_cond': False,\n            'diffusion_steps': 1000,\n            'dropout': 0.0,\n            'learn_sigma': True,\n            'noise_schedule': \"linear\",\n            'num_channels': 128,\n            'num_head_channels': 64,\n            'num_res_blocks': 1,\n            'resblock_updown': True,\n            'use_fp16': False,\n            'use_scale_shift_norm': True,\n            'rescale_learned_sigmas': True,\n            'p2_gamma': 1,\n            'p2_k': 1,\n            'image_size': image_size,\n        })\n        model, diffusion = create_model_and_diffusion(**model_config)\n        checkpoint = torch.load(model_path, map_location=\"cpu\")\n        model.load_state_dict(checkpoint if 'state_dict' not in checkpoint else checkpoint['state_dict'])\n        model = model.to(device)\n        model.eval()\n\n        net = model\n        net.sigma_min = 0.01\n        net.sigma_max = 1\n        net.img_resolution = image_size\n        model_source = 'guided_diffusion'\n    else:\n        dist.print0(f'Loading the pre-trained diffusion model from \"{model_path}\"...')\n        if dataset_name in ['cifar10', 'ffhq', 'afhqv2', 'imagenet64']:\n            with dnnlib.util.open_url(model_path, verbose=(dist.get_rank() == 0)) as f:\n                net = pickle.load(f)['ema'].to(device)\n            net.sigma_min = 0.002\n            net.sigma_max = 80.0\n            model_source = 'edm'\n            net.img_resolution = image_size\n        elif dataset_name in ['lsun_bedroom']:\n            from models.cm.cm_model_loader import load_cm_model\n            from models.networks_edm import CMPrecond\n            net = load_cm_model(model_path)\n            net = CMPrecond(net).to(device)\n            model_source = 'cm'\n            net.img_resolution = image_size\n        else:\n            if guidance_type == 'cg':\n                if classifier_path is None:\n                    raise ValueError(\"Classifier path must be provided for guidance_type='cg'\")\n                from models.guided_diffusion.cg_model_loader import load_cg_model\n                from models.networks_edm import CGPrecond\n                net, classifier = load_cg_model(model_path, classifier_path)\n                net = CGPrecond(net, classifier, guidance_rate=guidance_rate).to(device)\n                model_source = 'adm'\n                net.img_resolution = image_size\n            elif guidance_type in ['uncond', 'cfg']:\n                from omegaconf import OmegaConf\n                from models.networks_edm import CFGPrecond\n                if dataset_name in ['lsun_bedroom_ldm']:\n                    config = OmegaConf.load('./models/ldm/configs/latent-diffusion/lsun_bedrooms-ldm-vq-4.yaml')\n                    net = load_ldm_model(config, model_path)\n                    net = CFGPrecond(net, img_resolution=image_size, img_channels=3, guidance_rate=1., guidance_type='uncond', label_dim=0).to(device)\n                elif dataset_name in ['ms_coco']:\n                    assert guidance_type == 'cfg'\n                    config = OmegaConf.load('./models/ldm/configs/stable-diffusion/v1-inference.yaml')\n                    net = load_ldm_model(config, model_path)\n                    net = CFGPrecond(net, img_resolution=image_size, img_channels=4, guidance_rate=guidance_rate, guidance_type='classifier-free', label_dim=True).to(device)\n                model_source = 'ldm'\n                net.img_resolution = image_size\n        if net is None:\n            raise ValueError(\"Got wrong settings: check dataset_name and guidance_type!\")\n        net.eval()\n\n    return net, diffusion, model_source\n\n#----------------------------------------------------------------------------\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser(description=\"Generate samples using a pre-trained diffusion model and AMED predictor with coarse inpainting support at 64x64.\")\n\n    # General options\n    parser.add_argument('--predictor_path', type=str, required=True,\n                        help='Path to trained AMED instructor (directory or .pkl file)')\n    parser.add_argument('--model_path', type=str, required=True,\n                        help='Path to the pre-trained diffusion model (required)')\n    parser.add_argument('--classifier_path', type=str, default=None,\n                        help='Path to the classifier model (required for guidance_type=\"cg\")')\n    parser.add_argument('--batch', dest='max_batch_size', type=int, default=64,\n                        help='Maximum batch size (default: 64)')\n    parser.add_argument('--seeds', type=str, default='0-63',\n                        help='Random seeds (e.g., 1,2,5-10) (default: 0-63)')\n    parser.add_argument('--prompt', type=str,\n                        help='Prompt for Stable Diffusion sampling')\n    parser.add_argument('--image_size', type=int, default=64,\n                        help='Image size for sampling (default: 64, fixed for coarse inpainting)')\n\n    # Options for sampling\n    parser.add_argument('--return_inters', type=str2bool, default=False,\n                        help='Whether to save intermediate outputs (default: False)')\n\n    # Options for saving\n    parser.add_argument('--outdir', type=str,\n                        help='Where to save the output images (directory)')\n    parser.add_argument('--grid', type=str2bool, default=False,\n                        help='Whether to make a grid of images (default: False)')\n    parser.add_argument('--subdirs', type=str2bool, default=True,\n                        help='Create subdirectory for every 1000 seeds (default: True)')\n\n    # Inpainting-specific options (coarse stage only)\n    parser.add_argument('--base_samples', type=str, default='',\n                        help='Directory containing reference images for inpainting')\n    parser.add_argument('--mask_path', type=str, default='',\n                        help='Directory containing masks for inpainting')\n    parser.add_argument('--use_inpainting', type=str2bool, default=False,\n                        help='Whether to perform inpainting instead of unconditional generation')\n    parser.add_argument('--use_inverse_masks', type=str2bool, default=False,\n                        help='Whether to invert the masks (i.e., mask values become -1)')\n\n\n    args = parser.parse_args()\n\n    # Convert seeds to a list using parse_int_list\n    seeds = parse_int_list(args.seeds)\n    max_batch_size = args.max_batch_size\n    device = torch.device('cuda')\n\n    # Pass arguments to solver_kwargs\n    solver_kwargs = {\n        'prompt': args.prompt,\n        'image_size': args.image_size,\n        'return_inters': args.return_inters,\n        'guidance_type': None,\n        'guidance_rate': None,\n    }\n\n    # Call the rest of the main function with the parsed arguments\n    dist.init()\n    num_batches = ((len(seeds) - 1) // (max_batch_size * dist.get_world_size()) + 1) * dist.get_world_size()\n    all_batches = torch.as_tensor(seeds).tensor_split(num_batches)\n    rank_batches = all_batches[dist.get_rank() :: dist.get_world_size()]\n\n    # Setup distributed logging\n    dist_util.setup_dist()\n    logger.configure(dir=args.outdir)\n\n    # Load models\n    if dist.get_rank() != 0:\n        torch.distributed.barrier()\n\n    # Load AMED predictor\n    predictor_path = args.predictor_path\n    if not predictor_path.endswith('pkl'):\n        predictor_path_str = '0' * (5 - len(predictor_path)) + predictor_path\n        for file_name in os.listdir(\"./exps\"):\n            if file_name.split('-')[0] == predictor_path_str:\n                file_list = [f for f in os.listdir(os.path.join('./exps', file_name)) if f.endswith(\"pkl\")]\n                max_index = -1\n                max_file = None\n                for ckpt_name in file_list:\n                    file_index = int(ckpt_name.split(\"-\")[-1].split(\".\")[0])\n                    if file_index > max_index:\n                        max_index = file_index\n                        max_file = ckpt_name\n                predictor_path = os.path.join('./exps', file_name, max_file)\n                break\n    dist.print0(f'Loading AMED predictor from \"{predictor_path}\"...')\n    with dnnlib.util.open_url(predictor_path, verbose=(dist.get_rank() == 0)) as f:\n        AMED_predictor = pickle.load(f)['model'].to(device)\n\n    # Update solver settings\n    prompt = solver_kwargs['prompt']\n    solver_kwargs = {key: value for key, value in solver_kwargs.items() if value is not None}\n    solver_kwargs['AMED_predictor'] = AMED_predictor\n    solver_kwargs['solver'] = solver = AMED_predictor.sampler_stu\n    solver_kwargs['num_steps'] = AMED_predictor.num_steps\n    solver_kwargs['guidance_type'] = AMED_predictor.guidance_type\n    solver_kwargs['guidance_rate'] = AMED_predictor.guidance_rate\n    solver_kwargs['afs'] = AMED_predictor.afs\n    solver_kwargs['denoise_to_zero'] = False\n    solver_kwargs['max_order'] = AMED_predictor.max_order\n    solver_kwargs['predict_x0'] = AMED_predictor.predict_x0\n    solver_kwargs['lower_order_final'] = AMED_predictor.lower_order_final\n    solver_kwargs['schedule_type'] = AMED_predictor.schedule_type\n    solver_kwargs['schedule_rho'] = AMED_predictor.schedule_rho\n    solver_kwargs['prompt'] = prompt\n    solver_kwargs['dataset_name'] = dataset_name = AMED_predictor.dataset_name\n\n    # Load pre-trained diffusion model (64x64 only)\n    model_64, diffusion_64, model_source = create_model(\n        dataset_name,\n        solver_kwargs['guidance_type'],\n        solver_kwargs['guidance_rate'],\n        device,\n        image_size=64,  # Fixed for coarse inpainting\n        model_path=args.model_path,\n        classifier_path=args.classifier_path\n    )\n    solver_kwargs['model_source'] = model_source\n\n    # Other ranks follow\n    if dist.get_rank() == 0:\n        torch.distributed.barrier()\n\n    # Update solver settings with sigma values\n    solver_kwargs['sigma_min'] = model_64.sigma_min\n    solver_kwargs['sigma_max'] = model_64.sigma_max\n    nfe = 2 * (solver_kwargs['num_steps'] - 1) - 1 if solver_kwargs[\"afs\"] else 2 * (solver_kwargs['num_steps'] - 1)\n    nfe = 2 * nfe if dataset_name in ['ms_coco'] else nfe\n    solver_kwargs['nfe'] = nfe\n\n    # Load data for inpainting\n    if args.use_inpainting:\n        if not args.base_samples or not args.mask_path:\n            raise ValueError(\"Must provide --base_samples and --mask_path for inpainting\")\n\n        # Data for 64x64 (coarse stage)\n        data_64 = load_reference(\n            args.base_samples,\n            max_batch_size,\n            image_size=64,\n            class_cond=False,\n        )\n        data_mask_64 = load_reference(\n            args.mask_path,\n            max_batch_size,\n            image_size=64,\n            class_cond=False,\n        )\n\n        # Count total number of inputs\n        all_items = os.listdir(args.base_samples)\n        num_inputs = len(all_items)\n    else:\n        num_inputs = len(seeds)\n\n    # Construct solver\n    if solver == 'amed':\n        sampler_fn = solvers_amed.amed_sampler\n    elif solver == 'euler':\n        sampler_fn = solvers_amed.euler_sampler\n    elif solver == 'dpm':\n        sampler_fn = solvers_amed.dpm_2_sampler\n    elif solver == 'ipndm':\n        sampler_fn = solvers_amed.ipndm_sampler\n    elif solver == 'dpmpp':\n        sampler_fn = solvers_amed.dpm_pp_sampler\n    else:\n        sampler_fn = solvers_amed.euler_sampler\n        solver_kwargs['solver'] = 'euler'\n        solver_kwargs['num_steps'] = 50\n        nfe = 2 * (solver_kwargs['num_steps'] - 1)\n        solver_kwargs['nfe'] = nfe\n\n    # Print solver settings\n    dist.print0(\"Solver settings:\")\n    for key, value in solver_kwargs.items():\n        if value is None:\n            continue\n        elif key == 'AMED_predictor':\n            continue\n        elif key == 'max_order' and solver in ['euler', 'dpm']:\n            continue\n        elif key in ['predict_x0', 'lower_order_final'] and solver not in ['dpmpp']:\n            continue\n        elif key in ['prompt'] and dataset_name not in ['ms_coco']:\n            continue\n        dist.print0(f\"\t{key}: {value}\")\n\n    # Loop over batches\n    if args.outdir is None:\n        if args.grid:\n            outdir = os.path.join(f\"./samples/grids/{dataset_name}\", f\"{solver}_nfe{nfe}\")\n        else:\n            outdir = os.path.join(f\"./samples/{dataset_name}\", f\"{solver}_nfe{nfe}\")\n    else:\n        outdir = args.outdir\n\n    dist.print0(f'Generating {num_inputs} images to \"{outdir}\"...')\n    count = 0\n    for batch_seeds in tqdm.tqdm(rank_batches, unit='batch', disable=(dist.get_rank() != 0)):\n        torch.distributed.barrier()\n        batch_size = len(batch_seeds)\n        if batch_size == 0:\n            continue\n\n        if args.use_inpainting:\n            # Load reference images and masks\n            model_kwargs_64 = next(data_64)\n            model_mask_kwargs_64 = next(data_mask_64)\n            model_kwargs_64 = {k: v.to(device) for k, v in model_kwargs_64.items()}\n            model_mask_kwargs_64 = {k: v.to(device) for k, v in model_mask_kwargs_64.items()}\n\n            gt = model_kwargs_64[\"ref_img\"]\n            mask = model_mask_kwargs_64['ref_img']\n\n            if args.use_inverse_masks:\n                model_mask_kwargs_64[\"ref_img\"] = model_mask_kwargs_64[\"ref_img\"] * (-1)\n\n            mask[mask < 0.] = 0\n            mask[mask > 0.] = 1\n           \n            \n\n            # Shape for sampling\n            shape = (batch_size, 3, 64, 64)  # Fixed for coarse inpainting\n\n            # Initial noise\n            noise = torch.randn(shape, device=device)\n\n            # Blend noise with ground truth: unmasked regions (mask=0) keep gt, masked regions (mask=1) get noise\n            latents = (1 - mask) * noise + mask * gt\n        else:\n            # Unconditional generation\n            shape = (batch_size, 3, args.image_size, args.image_size)\n            latents = torch.randn(shape, device=device)\n            model_kwargs_64 = None\n            model_mask_kwargs_64 = None\n            gt = None\n            mask = None\n\n        # Pick latents and labels for unconditional generation\n        if not args.use_inpainting:\n            rnd = StackedRandomGenerator(device, batch_seeds)\n            latents = rnd.randn([batch_size, 3, args.image_size, args.image_size], device=device)\n            class_labels = c = uc = None\n            if model_64.num_classes:\n                if solver_kwargs['model_source'] == 'adm':\n                    class_labels = rnd.randint(model_64.num_classes, size=(batch_size,), device=device)\n                elif solver_kwargs['model_source'] == 'ldm' and dataset_name == 'ms_coco':\n                    if solver_kwargs['prompt'] is None:\n                        prompts = sample_captions[batch_seeds[0]:batch_seeds[-1]+1]\n                    else:\n                        prompts = [solver_kwargs['prompt'] for i in range(batch_size)]\n                    if solver_kwargs['guidance_rate'] != 1.0:\n                        uc = model_64.model.get_learned_conditioning(batch_size * [\"\"])\n                    if isinstance(prompts, tuple):\n                        prompts = list(prompts)\n                    c = model_64.model.get_learned_conditioning(prompts)\n                else:\n                    class_labels = torch.eye(model_64.num_classes, device=device)[rnd.randint(model_64.num_classes, size=[batch_size], device=device)]\n        else:\n            class_labels = c = uc = None\n\n        # Generate images\n        with torch.no_grad():\n            if solver_kwargs['model_source'] == 'ldm':\n                with autocast(\"cuda\"):\n                    with model_64.model.ema_scope():\n                        images = sampler_fn(model_64, latents, condition=c, unconditional_condition=uc, **solver_kwargs)\n                        images = model_64.model.decode_first_stage(images)\n            elif solver_kwargs['model_source'] == 'guided_diffusion':\n                # Add inpainting-specific kwargs to solver_kwargs\n                solver_kwargs['model_kwargs'] = model_kwargs_64 if args.use_inpainting else None\n                solver_kwargs['model_mask_kwargs'] = model_mask_kwargs_64 if args.use_inpainting else None\n                images = sampler_fn(model_64, latents, class_labels=class_labels, **solver_kwargs)\n            else:\n                images = sampler_fn(model_64, latents, class_labels=class_labels, **solver_kwargs)\n\n        # Save images\n        if args.use_inpainting:\n            for i in range(batch_size):\n                os.makedirs(os.path.join(outdir, \"gtImg\"), exist_ok=True)\n                os.makedirs(os.path.join(outdir, \"inputImg\"), exist_ok=True)\n                os.makedirs(os.path.join(outdir, \"sampledImg\"), exist_ok=True)\n                os.makedirs(os.path.join(outdir, \"coarseImg\"), exist_ok=True)\n\n                out_gtImg_path = os.path.join(outdir, \"gtImg\", f\"{str(count + i).zfill(4)}.png\")\n                out_inputImg_path = os.path.join(outdir, \"inputImg\", f\"{str(count + i).zfill(4)}.png\")\n                out_sampledImg_path = os.path.join(outdir, \"sampledImg\", f\"{str(count + i).zfill(4)}.png\")\n                out_coarseImg_path = os.path.join(outdir, \"coarseImg\", f\"{str(count + i).zfill(4)}.png\")\n\n                tmp_ones = torch.ones(gt[i].shape) * (-1)\n                inputImg = gt[i].to(mask.device) * mask[i] + (1 - mask[i]) * tmp_ones.to(mask.device)\n                sampledImg = images[i].unsqueeze(0)\n                coarseImg = sampledImg \n\n                gtImg = gt[i]\n                gtImg = gtImg.reshape(coarseImg.shape).to(coarseImg.device)\n\n                save_image(gtImg, out_gtImg_path, nrow=1, normalize=True, value_range=(-1, 1))\n                save_image(inputImg, out_inputImg_path, nrow=1, normalize=True, value_range=(-1, 1))\n                save_image(sampledImg, out_sampledImg_path, nrow=1, normalize=True, value_range=(-1, 1))\n                save_image(coarseImg, out_coarseImg_path, nrow=1, normalize=True, value_range=(-1, 1))\n        else:\n            if args.grid:\n                images = torch.clamp(images / 2 + 0.5, 0, 1)\n                os.makedirs(outdir, exist_ok=True)\n                nrows = int(images.shape[0] ** 0.5)\n                image_grid = make_grid(images, nrows, padding=0)\n                save_image(image_grid, os.path.join(outdir, \"grid.png\"))\n            else:\n                images_np = (images * 127.5 + 128).clip(0, 255).to(torch.uint8).permute(0, 2, 3, 1).cpu().numpy()\n                for seed, image_np in zip(batch_seeds, images_np):\n                    image_dir = os.path.join(outdir, f'{seed-seed%1000:06d}') if args.subdirs else outdir\n                    os.makedirs(image_dir, exist_ok=True)\n                    image_path = os.path.join(image_dir, f'{seed:06d}.png')\n                    PIL.Image.fromarray(image_np, 'RGB').save(image_path)\n\n        count += batch_size\n        dist.print0(f\"Created {count} samples\")\n\n    # Done\n    torch.distributed.barrier()\n    dist.print0('Done.')\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n'''\nwith open('/kaggle/working/p2w/coarse_amed.py', 'w') as f:\n    f.write(coarse_amedv2)\nprint(\"Successfully wrote coarse_amed.py to /kaggle/working/p2w/coarse_amed.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:51:04.363746Z","iopub.execute_input":"2025-03-27T04:51:04.364069Z","iopub.status.idle":"2025-03-27T04:51:04.374749Z","shell.execute_reply.started":"2025-03-27T04:51:04.364043Z","shell.execute_reply":"2025-03-27T04:51:04.373849Z"}},"outputs":[{"name":"stdout","text":"Successfully wrote coarse_amed.py to /kaggle/working/p2w/coarse_amed.py\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"before fp16:","metadata":{}},{"cell_type":"code","source":"!source /kaggle/working/miniconda/etc/profile.d/conda.sh && conda activate p2w && cd /kaggle/working/p2w && \\\nCUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 --master_port=22222 \\\ncoarse_amed.py \\\n--predictor_path /kaggle/working/p2w/exps/00000-celebahq-10-17-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl \\\n--model_path /kaggle/working/p2w/checkpoints/64x64.pt \\\n--batch 5 \\\n--seeds 0-4 \\\n--outdir /kaggle/working/p2w/logs \\\n--grid False \\\n--subdirs True \\\n--image_size 64 \\\n--use_inpainting True \\\n--base_samples /kaggle/working/p2w/demo/image \\\n--mask_path /kaggle/working/p2w/demo/mask \\\n--use_inverse_masks False ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:53:56.102601Z","iopub.execute_input":"2025-03-27T04:53:56.102929Z","iopub.status.idle":"2025-03-27T04:54:08.535318Z","shell.execute_reply.started":"2025-03-27T04:53:56.102904Z","shell.execute_reply":"2025-03-27T04:54:08.534154Z"}},"outputs":[{"name":"stdout","text":"[2025-03-27 04:53:57,984] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\nLogging to /kaggle/working/p2w/logs\nLoading AMED predictor from \"/kaggle/working/p2w/exps/00000-celebahq-50-97-amed-heun-1-uni1.0-afs-predictor_64-000010.pkl\"...\nLoading pre-trained guided-diffusion model for CelebAHQ from \"/kaggle/working/p2w/checkpoints/64x64.pt\"...\nSolver settings:\n\timage_size: 64\n\treturn_inters: False\n\tsolver: amed\n\tnum_steps: 50\n\tguidance_rate: 0.0\n\tafs: True\n\tdenoise_to_zero: False\n\tmax_order: 3\n\tschedule_type: time_uniform\n\tschedule_rho: 1.0\n\tdataset_name: celebahq\n\tmodel_source: guided_diffusion\n\tsigma_min: 0.01\n\tsigma_max: 1\n\tnfe: 97\nGenerating 5 images to \"/kaggle/working/p2w/logs\"...\n  0%|                                                  | 0/1 [00:00<?, ?batch/s]/kaggle/working/p2w/solver_utils_amed.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  vp_sigma = lambda beta_d, beta_min: lambda t: (torch.exp(torch.tensor(0.5 * beta_d * (t ** 2) + beta_min * t, device=device)) - 1) ** 0.5\nCreated 5 samples\n100%|██████████████████████████████████████████| 1/1 [00:03<00:00,  3.55s/batch]\nDone.\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport math\n\n# Define the path to the outImg folder\nsave_dir = \"/kaggle/working/p2w/logs/coarseImg\"  # Replace with your args.save_dir\nout_img_dir = save_dir\n\n# List all PNG files in the outImg directory\nimage_files = sorted([f for f in os.listdir(out_img_dir) if f.endswith(\".png\")])\nprint(f\"Found {len(image_files)} images in {out_img_dir}\")\n\n# Define the grid size (e.g., 2 rows x 3 columns for 6 images)\nnum_images = len(image_files)\ncols = 3  # Number of columns in the grid\nrows = math.ceil(num_images / cols)  # Calculate rows needed\n\n# Create a figure and axes for the grid\nfig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n\n# Flatten axes array for easy iteration (handles both 1D and 2D cases)\naxes = axes.flatten() if rows > 1 else [axes] if cols == 1 else axes\n\n# Load and plot each image\nfor idx, image_file in enumerate(image_files):\n    if idx >= len(axes):  # Stop if we run out of grid spaces\n        break\n    # Load the image\n    image_path = os.path.join(out_img_dir, image_file)\n    img = Image.open(image_path)\n    \n    # Display the image in the grid\n    axes[idx].imshow(img)\n    axes[idx].set_title(image_file, fontsize=10)\n    axes[idx].axis(\"off\")  # Hide axes for cleaner display\n\n# Hide any unused subplots\nfor idx in range(len(image_files), len(axes)):\n    axes[idx].axis(\"off\")\n\n# Adjust layout and display the grid\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:54:15.448151Z","iopub.execute_input":"2025-03-27T04:54:15.448457Z","iopub.status.idle":"2025-03-27T04:54:15.957932Z","shell.execute_reply.started":"2025-03-27T04:54:15.448432Z","shell.execute_reply":"2025-03-27T04:54:15.957039Z"}},"outputs":[{"name":"stdout","text":"Found 5 images in /kaggle/working/p2w/logs/coarseImg\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x800 with 6 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":63}]}